{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42437534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from collections import defaultdict\n",
    "nltk.download('wordnet',quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt',quiet=True)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32843d78",
   "metadata": {},
   "source": [
    "### 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "096f1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41fc71",
   "metadata": {},
   "source": [
    "* Read the complete dataset from \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz\"\n",
    "* Create three-class classification problem according to the rating.\n",
    "* Performed the below data cleaning on complete data\n",
    "    * convert the all reviews into the lower case\n",
    "    * remove html and url\n",
    "    * remove non-alphabetical chars\n",
    "    * remove extra spaces\n",
    "    * perform contractions\n",
    "* Selectd 20,000 random reviews from each rating class and created a balanced dataset. \n",
    "\n",
    "Followed the same code as HW1. To avoid computational load, I have saved the balanced data in \"data_balanced_60.csv\" file.\n",
    "\n",
    "* Iterate through each review\n",
    "* If word in review is in Word embedding, get the word vector\n",
    "* Compute the average vector of sentence.\n",
    "* Split the dataset into train and test set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d7625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data = pd.read_csv(\"data_balanced_60\", sep='\\t', names = ['star_rating', 'review_body'])\n",
    "# balanced_data.dropna(inplace=True)\n",
    "# balanced_data = balanced_data.groupby('star_rating').apply(lambda group: group.sample(20000)).reset_index(drop = True)\n",
    "balanced_data = shuffle(balanced_data)\n",
    "vocab = defaultdict(int)\n",
    "for l in balanced_data['review_body']:\n",
    "    for w in l.split():\n",
    "        vocab[w] += 1 \n",
    "        \n",
    "for key in list(vocab.keys()):\n",
    "    if vocab[key] < 3:\n",
    "        del vocab[key]\n",
    "\n",
    "test_reviews = balanced_data['review_body']\n",
    "avg_review_vectors = []\n",
    "for review in test_reviews:\n",
    "    words = review.split()\n",
    "    rv = np.zeros(300)\n",
    "    for word in words:\n",
    "        if word in wv and word in vocab:\n",
    "            rv += wv[word]\n",
    "    if(len(words)):\n",
    "        rv /= len(words)\n",
    "    avg_review_vectors.append(rv)\n",
    "\n",
    "avg_review_vectors = np.array(avg_review_vectors)\n",
    "review_ratings = balanced_data['star_rating']\n",
    "train_X = avg_review_vectors[:int(0.8 * len(avg_review_vectors))]\n",
    "train_Y = review_ratings[:int(0.8 * len(avg_review_vectors))]\n",
    "test_X = avg_review_vectors[int(0.8 * len(avg_review_vectors)):]\n",
    "test_Y =  review_ratings[int(0.8 * len(avg_review_vectors)):]\n",
    "train_X=np.nan_to_num(train_X, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "test_X=np.nan_to_num(test_X,copy=True, nan=0.0, posinf=None, neginf=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ebedf1",
   "metadata": {},
   "source": [
    "### 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31735994",
   "metadata": {},
   "source": [
    "To learn the semantic similarity, I am considering the below three examples:\n",
    "* Finding words similar to \"happy\" using its vector\n",
    "* Performing \"big - large + small = tiny\" using word vector\n",
    "* Finding cosine similarity between words \"love\" and \"like\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec984af5",
   "metadata": {},
   "source": [
    "#### (a)  Google news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "731c6578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to happy:\n",
      "\thappy: 1.0000\n",
      "\tglad: 0.7409\n",
      "\tpleased: 0.6632\n",
      "\tecstatic: 0.6627\n",
      "\toverjoyed: 0.6599\n",
      "\n",
      "Most similar words to 'big-large+small':\n",
      "\tbig: 0.7968\n",
      "\tsmall: 0.6329\n",
      "\tbigger: 0.5330\n",
      "\thuge: 0.4986\n",
      "\tlittle_bitty: 0.4698\n",
      "\tbiggest: 0.4613\n",
      "\ttiny: 0.4609\n",
      "\tSmall: 0.4602\n",
      "\tnice: 0.4599\n",
      "\tabig: 0.4512\n",
      "\n",
      "Cosine similarity between love and like: 0.36713877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Similar words to happy:\")\n",
    "res = wv.similar_by_vector(wv[\"happy\"], topn=5)\n",
    "for word, score in res:\n",
    "    print(\"\\t{}: {:.4f}\".format(word, score))\n",
    "print()\n",
    "\n",
    "big = wv['big']\n",
    "large = wv['large']\n",
    "small = wv['small']\n",
    "result = big-large+small\n",
    "similarity = wv.similar_by_vector(result)\n",
    "print(\"Most similar words to 'big-large+small':\")\n",
    "for word, score in similarity:\n",
    "    print(\"\\t{}: {:.4f}\".format(word, score))\n",
    "print()    \n",
    "cosine = wv.similarity(\"love\", \"like\")\n",
    "print(\"Cosine similarity between love and like:\", cosine)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e3242",
   "metadata": {},
   "source": [
    "#### (b) Training Word2Vec on Review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ba4634e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to happy:\n",
      "\tbother: 0.1829\n",
      "\tanymore: 0.1633\n",
      "\treapplying: 0.1566\n",
      "\tquite: 0.1527\n",
      "\trepeat: 0.1498\n",
      "\n",
      "Most similar words to 'big-large+small':\n",
      "\tbig: 0.8519\n",
      "\tsmall: 0.6697\n",
      "\thuge: 0.6246\n",
      "\ttiny: 0.5004\n",
      "\tfaced: 0.3808\n",
      "\tcute: 0.3769\n",
      "\tlarge: 0.3757\n",
      "\tbad: 0.3635\n",
      "\treal: 0.3610\n",
      "\tpricy: 0.3400\n",
      "\n",
      "Cosine similarity between love and like: 0.25278062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r_data =balanced_data['review_body']\n",
    "sentences = []\n",
    "for s in r_data:\n",
    "    sentences.append(list(s.split(\" \")))\n",
    "    \n",
    "my_model = Word2Vec(sentences, vector_size=300, window=13, min_count=9)\n",
    "my_model.train(sentences, total_examples=my_model.corpus_count, epochs=my_model.epochs)\n",
    "\n",
    "print(\"Similar words to happy:\")\n",
    "res = my_model.wv.similar_by_vector(wv[\"happy\"], topn=5)\n",
    "for word, score in res:\n",
    "    print(\"\\t{}: {:.4f}\".format(word, score))\n",
    "print()\n",
    "\n",
    "big_2 = my_model.wv['big']\n",
    "large_2 = my_model.wv['large']\n",
    "small_2 = my_model.wv['small']\n",
    "result_2 = big_2-large_2+small_2\n",
    "similarity_2 = my_model.wv.similar_by_vector(result_2)\n",
    "print(\"Most similar words to 'big-large+small':\")\n",
    "for word, score in similarity_2:\n",
    "    print(\"\\t{}: {:.4f}\".format(word, score))\n",
    "print()\n",
    "\n",
    "cosine = my_model.wv.similarity(\"love\", \"like\")\n",
    "print(\"Cosine similarity between love and like:\", cosine)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6a108",
   "metadata": {},
   "source": [
    "#### Conclusions:\n",
    "\n",
    "* For the first example, pretrained models gives much better result compared to our custom model. Custome model process the word in refrence to the dataset that it has i.e with respect to reviews. Hence the similar words suggested by custom model are not synonyms of \"happy\"\n",
    "\n",
    "* For the second example \"big-large+small=tiny\", both the models are performing considerably well. The score returned for \"tiny\" by the custom model is slightly more compared to  pretrained model.\n",
    "\n",
    "* For the thrid example, cosine similarity score given by pretrained model is higher but difference is negligible. \n",
    "\n",
    "Overall, pretrained model performs much better in comparision to the custom model. The main reason being, custom model has smaller dataset (only the reviews) where as pretrained model has very vast dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eae6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1f218f6",
   "metadata": {},
   "source": [
    "## 3. Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "428c78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAccuracy(y_test, y_pred):\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, digits=4)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    return 100*accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d152e20",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "adb6b60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Accuracy using Word2Vec  =  56.94166666666667\n",
      "\n",
      "Perceptron Accuracy using TF-IDF =  66.73333333333333\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "perceptron = Perceptron(max_iter=9000)\n",
    "perceptron.fit(train_X, train_Y)\n",
    "perceptron_predictions = perceptron.predict(test_X)\n",
    "print(\"Perceptron Accuracy using Word2Vec  = \", findAccuracy(test_Y, perceptron_predictions))\n",
    "\n",
    "###########Compute using TFIDF#####################\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopword(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw =  \" \".join([word for word in text_tokens if word not in stop_words])\n",
    "    return tokens_without_sw\n",
    "def lemmetize(text):    \n",
    "    text_tokens = word_tokenize(text)\n",
    "    lemmatized_string = \" \".join([lemmatizer.lemmatize(words) for words in text_tokens])\n",
    "    return lemmatized_string\n",
    "balanced_data['cleaned_data'] = balanced_data.apply(lambda row : remove_stopword(row['review_body']), axis = 1)\n",
    "balanced_data['cleaned_data'] = balanced_data.apply(lambda row : lemmetize(row['review_body']), axis = 1)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=9, ngram_range=(1,2))\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(balanced_data['cleaned_data'])\n",
    "X_tfidf = tfidf_vector\n",
    "y_tfidf = balanced_data['star_rating']\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, y_tfidf, test_size=0.2, random_state=42)\n",
    "perceptron_model = Perceptron(max_iter=10000)\n",
    "perceptron_model.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_perceptron = perceptron_model.predict(X_test_tfidf)\n",
    "print(\"\\nPerceptron Accuracy using TF-IDF = \", findAccuracy(y_test_tfidf, y_pred_perceptron))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "feb6a154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy using Word2Vec (%) =  66.34166666666667\n",
      "\n",
      "SVM Accuracy using TF-IDF (%)=  74.14166666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = SVC(C=0.1)\n",
    "svm.fit(train_X, train_Y)\n",
    "svm_predictions = svm.predict(test_X)\n",
    "print(\"SVM Accuracy using Word2Vec (%) = \", findAccuracy(test_Y, svm_predictions))\n",
    "\n",
    "######################TF-IDF####################################\n",
    "\n",
    "svm_model = LinearSVC(C=0.1).fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "print(\"\\nSVM Accuracy using TF-IDF (%)= \", findAccuracy(y_test_tfidf, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432caf29",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "From above values, we can conclude that TF-IDF model performing better than Word2Vec.It's also worth noting that TFIDF and Word2Vec capture different aspects of language. TFIDF is a statistical measure of how important a word is to a document in a corpus, while Word2Vec is a neural network-based model that captures the semantic relationships between words. \n",
    "\n",
    "There could be several reasons why the model using TFIDF is performing better than the Word2Vec model.\n",
    "* Word2Vec models are trained on general language data, and may not perform as well on domain-specific language.\n",
    "* Word2Vec requires a large amount of data to accurately capture the nuances of language. If you have a small dataset, the Word2Vec model may not have enough data to work with and may not be able to capture the meaning of words accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b728cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ff1466d",
   "metadata": {},
   "source": [
    "## 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5ab4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust the class labels to start from 0\n",
    "train_label_0 = [x-1 for x in train_Y]\n",
    "test_label_0 = [x-1 for x in test_Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f99e86",
   "metadata": {},
   "source": [
    "#### (a) Using average Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e8439bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward_MLP(torch.nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(Feedforward_MLP, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, 50)\n",
    "            self.dropout = torch.nn.Dropout(0.2)            \n",
    "#             self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(50, 10)\n",
    "            self.fc3 = torch.nn.Linear(10, 3)\n",
    "#             self.sigmoid = torch.nn.Sigmoid()\n",
    "            self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            x= self.softmax(x)\n",
    "            return x\n",
    "        \n",
    "mlp_model = Feedforward_MLP(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "024ae834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.939375 \tTest Loss: 0.895852\n",
      "Test loss decreased (inf --> 0.895852).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.904035 \tTest Loss: 0.886219\n",
      "Test loss decreased (0.895852 --> 0.886219).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.895410 \tTest Loss: 0.884505\n",
      "Test loss decreased (0.886219 --> 0.884505).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.892376 \tTest Loss: 0.879492\n",
      "Test loss decreased (0.884505 --> 0.879492).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.888288 \tTest Loss: 0.873104\n",
      "Test loss decreased (0.879492 --> 0.873104).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.886309 \tTest Loss: 0.874145\n",
      "Epoch: 7 \tTraining Loss: 0.884005 \tTest Loss: 0.878798\n",
      "Epoch: 8 \tTraining Loss: 0.882419 \tTest Loss: 0.872698\n",
      "Test loss decreased (0.873104 --> 0.872698).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.880684 \tTest Loss: 0.870728\n",
      "Test loss decreased (0.872698 --> 0.870728).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.880729 \tTest Loss: 0.876556\n",
      "Epoch: 11 \tTraining Loss: 0.877763 \tTest Loss: 0.870106\n",
      "Test loss decreased (0.870728 --> 0.870106).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.878140 \tTest Loss: 0.872879\n",
      "Epoch: 13 \tTraining Loss: 0.878406 \tTest Loss: 0.870911\n",
      "Epoch: 14 \tTraining Loss: 0.875730 \tTest Loss: 0.867770\n",
      "Test loss decreased (0.870106 --> 0.867770).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.872950 \tTest Loss: 0.873454\n",
      "Epoch: 16 \tTraining Loss: 0.875098 \tTest Loss: 0.872517\n",
      "Epoch: 17 \tTraining Loss: 0.874921 \tTest Loss: 0.869660\n",
      "Epoch: 18 \tTraining Loss: 0.873441 \tTest Loss: 0.872034\n",
      "Epoch: 19 \tTraining Loss: 0.872960 \tTest Loss: 0.875569\n",
      "Epoch: 20 \tTraining Loss: 0.873011 \tTest Loss: 0.876439\n",
      "Epoch: 21 \tTraining Loss: 0.872743 \tTest Loss: 0.873748\n",
      "Epoch: 22 \tTraining Loss: 0.872795 \tTest Loss: 0.869946\n",
      "Epoch: 23 \tTraining Loss: 0.870646 \tTest Loss: 0.872049\n",
      "Epoch: 24 \tTraining Loss: 0.871340 \tTest Loss: 0.871194\n",
      "Epoch: 25 \tTraining Loss: 0.869503 \tTest Loss: 0.874808\n",
      "Epoch: 26 \tTraining Loss: 0.868518 \tTest Loss: 0.875831\n",
      "Epoch: 27 \tTraining Loss: 0.869103 \tTest Loss: 0.879045\n",
      "Epoch: 28 \tTraining Loss: 0.867731 \tTest Loss: 0.867787\n",
      "Epoch: 29 \tTraining Loss: 0.867641 \tTest Loss: 0.867744\n",
      "Test loss decreased (0.867770 --> 0.867744).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.866411 \tTest Loss: 0.872678\n",
      "Epoch: 31 \tTraining Loss: 0.868786 \tTest Loss: 0.879483\n",
      "Epoch: 32 \tTraining Loss: 0.867504 \tTest Loss: 0.885109\n",
      "Epoch: 33 \tTraining Loss: 0.867591 \tTest Loss: 0.873166\n",
      "Epoch: 34 \tTraining Loss: 0.865351 \tTest Loss: 0.880041\n",
      "Epoch: 35 \tTraining Loss: 0.865908 \tTest Loss: 0.871055\n",
      "Epoch: 36 \tTraining Loss: 0.863468 \tTest Loss: 0.869634\n",
      "Epoch: 37 \tTraining Loss: 0.863577 \tTest Loss: 0.871554\n",
      "Epoch: 38 \tTraining Loss: 0.864296 \tTest Loss: 0.874200\n",
      "Epoch: 39 \tTraining Loss: 0.862276 \tTest Loss: 0.870103\n",
      "Epoch: 40 \tTraining Loss: 0.865018 \tTest Loss: 0.882879\n",
      "Epoch: 41 \tTraining Loss: 0.864991 \tTest Loss: 0.868956\n",
      "Epoch: 42 \tTraining Loss: 0.861901 \tTest Loss: 0.873845\n",
      "Epoch: 43 \tTraining Loss: 0.861847 \tTest Loss: 0.876821\n",
      "Epoch: 44 \tTraining Loss: 0.863983 \tTest Loss: 0.881152\n",
      "Epoch: 45 \tTraining Loss: 0.861981 \tTest Loss: 0.867683\n",
      "Test loss decreased (0.867744 --> 0.867683).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.862923 \tTest Loss: 0.874249\n",
      "Epoch: 47 \tTraining Loss: 0.860855 \tTest Loss: 0.879159\n",
      "Epoch: 48 \tTraining Loss: 0.860520 \tTest Loss: 0.873496\n",
      "Epoch: 49 \tTraining Loss: 0.858770 \tTest Loss: 0.878228\n",
      "Epoch: 50 \tTraining Loss: 0.860168 \tTest Loss: 0.869522\n",
      "Epoch: 51 \tTraining Loss: 0.862264 \tTest Loss: 0.867621\n",
      "Test loss decreased (0.867683 --> 0.867621).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 0.857776 \tTest Loss: 0.868633\n",
      "Epoch: 53 \tTraining Loss: 0.858589 \tTest Loss: 0.869148\n",
      "Epoch: 54 \tTraining Loss: 0.859692 \tTest Loss: 0.872756\n",
      "Epoch: 55 \tTraining Loss: 0.858515 \tTest Loss: 0.866168\n",
      "Test loss decreased (0.867621 --> 0.866168).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 0.858418 \tTest Loss: 0.882488\n",
      "Epoch: 57 \tTraining Loss: 0.857088 \tTest Loss: 0.868401\n",
      "Epoch: 58 \tTraining Loss: 0.857353 \tTest Loss: 0.873084\n",
      "Epoch: 59 \tTraining Loss: 0.859559 \tTest Loss: 0.871187\n",
      "Epoch: 60 \tTraining Loss: 0.857024 \tTest Loss: 0.896796\n",
      "Epoch: 61 \tTraining Loss: 0.856441 \tTest Loss: 0.869557\n",
      "Epoch: 62 \tTraining Loss: 0.857391 \tTest Loss: 0.871121\n",
      "Epoch: 63 \tTraining Loss: 0.854984 \tTest Loss: 0.869901\n",
      "Epoch: 64 \tTraining Loss: 0.859753 \tTest Loss: 0.868945\n",
      "Epoch: 65 \tTraining Loss: 0.856461 \tTest Loss: 0.873639\n",
      "Epoch: 66 \tTraining Loss: 0.854874 \tTest Loss: 0.868108\n",
      "Epoch: 67 \tTraining Loss: 0.854944 \tTest Loss: 0.869325\n",
      "Epoch: 68 \tTraining Loss: 0.855156 \tTest Loss: 0.868872\n",
      "Epoch: 69 \tTraining Loss: 0.855473 \tTest Loss: 0.874643\n",
      "Epoch: 70 \tTraining Loss: 0.855234 \tTest Loss: 0.866014\n",
      "Test loss decreased (0.866168 --> 0.866014).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 0.854435 \tTest Loss: 0.870764\n",
      "Epoch: 72 \tTraining Loss: 0.854519 \tTest Loss: 0.870048\n",
      "Epoch: 73 \tTraining Loss: 0.855389 \tTest Loss: 0.871418\n",
      "Epoch: 74 \tTraining Loss: 0.853852 \tTest Loss: 0.872904\n",
      "Epoch: 75 \tTraining Loss: 0.856515 \tTest Loss: 0.875714\n",
      "Epoch: 76 \tTraining Loss: 0.856625 \tTest Loss: 0.872586\n",
      "Epoch: 77 \tTraining Loss: 0.854158 \tTest Loss: 0.872722\n",
      "Epoch: 78 \tTraining Loss: 0.853670 \tTest Loss: 0.880660\n",
      "Epoch: 79 \tTraining Loss: 0.854473 \tTest Loss: 0.870289\n",
      "Epoch: 80 \tTraining Loss: 0.855123 \tTest Loss: 0.888243\n",
      "Epoch: 81 \tTraining Loss: 0.853650 \tTest Loss: 0.873344\n",
      "Epoch: 82 \tTraining Loss: 0.853407 \tTest Loss: 0.866079\n",
      "Epoch: 83 \tTraining Loss: 0.852432 \tTest Loss: 0.868551\n",
      "Epoch: 84 \tTraining Loss: 0.853600 \tTest Loss: 0.880342\n",
      "Epoch: 85 \tTraining Loss: 0.854142 \tTest Loss: 0.867870\n",
      "Epoch: 86 \tTraining Loss: 0.854577 \tTest Loss: 0.873459\n",
      "Epoch: 87 \tTraining Loss: 0.851843 \tTest Loss: 0.870317\n",
      "Epoch: 88 \tTraining Loss: 0.851063 \tTest Loss: 0.867804\n",
      "Epoch: 89 \tTraining Loss: 0.850748 \tTest Loss: 0.869387\n",
      "Epoch: 90 \tTraining Loss: 0.854097 \tTest Loss: 0.869913\n",
      "Epoch: 91 \tTraining Loss: 0.851405 \tTest Loss: 0.882079\n",
      "Epoch: 92 \tTraining Loss: 0.850426 \tTest Loss: 0.873501\n",
      "Epoch: 93 \tTraining Loss: 0.849814 \tTest Loss: 0.881794\n",
      "Epoch: 94 \tTraining Loss: 0.851866 \tTest Loss: 0.874737\n",
      "Epoch: 95 \tTraining Loss: 0.851669 \tTest Loss: 0.867906\n",
      "Epoch: 96 \tTraining Loss: 0.848664 \tTest Loss: 0.872177\n",
      "Epoch: 97 \tTraining Loss: 0.851035 \tTest Loss: 0.868981\n",
      "Epoch: 98 \tTraining Loss: 0.850403 \tTest Loss: 0.870213\n",
      "Epoch: 99 \tTraining Loss: 0.850114 \tTest Loss: 0.875738\n",
      "Epoch: 100 \tTraining Loss: 0.848560 \tTest Loss: 0.880036\n"
     ]
    }
   ],
   "source": [
    "tensor_trainX = torch.Tensor(train_X)\n",
    "tensor_trainY = torch.Tensor(train_label_0).type(torch.LongTensor)\n",
    "tensor_testX = torch.Tensor(test_X)\n",
    "tensor_testY = torch.Tensor(test_label_0).type(torch.LongTensor)\n",
    "\n",
    "# Create Torch datasets\n",
    "tensorset_train = torch.utils.data.TensorDataset(tensor_trainX, tensor_trainY)\n",
    "tensorset_test = torch.utils.data.TensorDataset(tensor_testX, tensor_testY)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(tensorset_train, batch_size=20,shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(tensorset_test, batch_size=20, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(),lr=0.005,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "test_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(100):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    mlp_model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = mlp_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    mlp_model.eval() # prep model for evaluation\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = mlp_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if test_loss <= test_loss_min:\n",
    "        print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        test_loss_min,\n",
    "        test_loss))\n",
    "        torch.save(mlp_model.state_dict(), 'mlp_model.pt')\n",
    "        test_loss_min = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b618bad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward Neural Network Accuracy (Avg Vectors) =  66.49166666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred_mlp=torch.max(mlp_model(tensor_testX).data,1).indices\n",
    "print(\"Feedforward Neural Network Accuracy (Avg Vectors) = \", findAccuracy(tensor_testY.tolist(), y_pred_mlp.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b79aad",
   "metadata": {},
   "source": [
    "#### (b) Input feature as concatenated vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac3c20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_feature_vector(reviews):\n",
    "    review_words = [review.split() for review in reviews]    \n",
    "    vector_size = wv.vector_size    \n",
    "    num_reviews = len(reviews)\n",
    "    input_features = np.zeros((num_reviews, 10*vector_size))\n",
    "    \n",
    "    for i, words in enumerate(review_words):\n",
    "        vectors = []\n",
    "        for j in range(min(len(words), 10)):\n",
    "            word = words[j]\n",
    "            if word in wv:\n",
    "                vectors.append(wv[word])\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        num_missing_vectors = max(0, 10 - len(vectors))\n",
    "        padded_vectors = vectors + [np.zeros(vector_size)]*num_missing_vectors        \n",
    "        feature_vector = np.concatenate(padded_vectors)        \n",
    "        input_features[i,:] = feature_vector\n",
    "    \n",
    "    return input_features\n",
    "\n",
    "input_feature = generate_input_feature_vector(balanced_data['review_body'])\n",
    "train_data_concat = input_feature[:int(0.8 * len(input_feature))]\n",
    "train_label_concat = balanced_data['star_rating'][:int(0.8 * len(input_feature))]\n",
    "test_data_concat = input_feature[int(0.8 * len(input_feature)):]\n",
    "test_label_concat =  balanced_data['star_rating'][int(0.8 * len(input_feature)):]\n",
    "\n",
    "tensor_trainX_concat = torch.Tensor(train_data_concat)\n",
    "tensor_trainY_concat = torch.Tensor(train_label_0).type(torch.LongTensor)\n",
    "tensor_testX_concat = torch.Tensor(test_data_concat)\n",
    "tensor_testY_concat = torch.Tensor(test_label_0).type(torch.LongTensor)\n",
    "\n",
    "# Create Torch datasets\n",
    "train_tensorset_concat = torch.utils.data.TensorDataset(tensor_trainX_concat, tensor_trainY_concat)\n",
    "test_tensorset_concat = torch.utils.data.TensorDataset(tensor_testX_concat, tensor_testY_concat)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_tensorset_concat, batch_size=20,shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_tensorset_concat, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45bd324d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.990597 \tTest Loss: 0.969299\n",
      "Test loss decreased (inf --> 0.969299).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.951372 \tTest Loss: 0.959068\n",
      "Test loss decreased (0.969299 --> 0.959068).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.934073 \tTest Loss: 0.963491\n",
      "Epoch: 4 \tTraining Loss: 0.922198 \tTest Loss: 0.954142\n",
      "Test loss decreased (0.959068 --> 0.954142).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.914145 \tTest Loss: 0.962931\n",
      "Epoch: 6 \tTraining Loss: 0.909673 \tTest Loss: 0.960413\n",
      "Epoch: 7 \tTraining Loss: 0.901737 \tTest Loss: 0.967661\n",
      "Epoch: 8 \tTraining Loss: 0.902199 \tTest Loss: 0.961720\n",
      "Epoch: 9 \tTraining Loss: 0.894601 \tTest Loss: 0.963299\n",
      "Epoch: 10 \tTraining Loss: 0.890628 \tTest Loss: 0.968833\n",
      "Epoch: 11 \tTraining Loss: 0.887203 \tTest Loss: 0.971266\n",
      "Epoch: 12 \tTraining Loss: 0.882938 \tTest Loss: 0.968967\n",
      "Epoch: 13 \tTraining Loss: 0.883671 \tTest Loss: 0.971529\n",
      "Epoch: 14 \tTraining Loss: 0.881803 \tTest Loss: 0.966223\n",
      "Epoch: 15 \tTraining Loss: 0.875856 \tTest Loss: 0.966439\n",
      "Epoch: 16 \tTraining Loss: 0.875350 \tTest Loss: 0.973730\n",
      "Epoch: 17 \tTraining Loss: 0.876572 \tTest Loss: 0.967962\n",
      "Epoch: 18 \tTraining Loss: 0.869137 \tTest Loss: 0.970998\n",
      "Epoch: 19 \tTraining Loss: 0.872395 \tTest Loss: 0.971962\n",
      "Epoch: 20 \tTraining Loss: 0.868720 \tTest Loss: 0.974651\n",
      "Epoch: 21 \tTraining Loss: 0.864029 \tTest Loss: 0.967997\n",
      "Epoch: 22 \tTraining Loss: 0.865537 \tTest Loss: 0.970005\n",
      "Epoch: 23 \tTraining Loss: 0.862556 \tTest Loss: 0.974031\n",
      "Epoch: 24 \tTraining Loss: 0.860214 \tTest Loss: 0.972440\n",
      "Epoch: 25 \tTraining Loss: 0.861823 \tTest Loss: 0.975514\n",
      "Epoch: 26 \tTraining Loss: 0.858173 \tTest Loss: 0.972323\n",
      "Epoch: 27 \tTraining Loss: 0.855631 \tTest Loss: 0.970860\n",
      "Epoch: 28 \tTraining Loss: 0.859377 \tTest Loss: 0.966932\n",
      "Epoch: 29 \tTraining Loss: 0.859348 \tTest Loss: 0.973435\n",
      "Epoch: 30 \tTraining Loss: 0.858047 \tTest Loss: 0.973542\n",
      "Epoch: 31 \tTraining Loss: 0.853194 \tTest Loss: 0.976296\n",
      "Epoch: 32 \tTraining Loss: 0.857086 \tTest Loss: 0.977034\n",
      "Epoch: 33 \tTraining Loss: 0.850846 \tTest Loss: 0.972182\n",
      "Epoch: 34 \tTraining Loss: 0.850298 \tTest Loss: 0.972195\n",
      "Epoch: 35 \tTraining Loss: 0.851793 \tTest Loss: 0.970575\n",
      "Epoch: 36 \tTraining Loss: 0.850807 \tTest Loss: 0.975208\n",
      "Epoch: 37 \tTraining Loss: 0.855739 \tTest Loss: 0.973447\n",
      "Epoch: 38 \tTraining Loss: 0.852915 \tTest Loss: 0.973762\n",
      "Epoch: 39 \tTraining Loss: 0.854999 \tTest Loss: 0.975224\n",
      "Epoch: 40 \tTraining Loss: 0.853660 \tTest Loss: 0.973124\n",
      "Epoch: 41 \tTraining Loss: 0.854332 \tTest Loss: 0.978351\n",
      "Epoch: 42 \tTraining Loss: 0.850878 \tTest Loss: 0.976594\n",
      "Epoch: 43 \tTraining Loss: 0.850348 \tTest Loss: 0.973558\n",
      "Epoch: 44 \tTraining Loss: 0.855970 \tTest Loss: 0.977649\n",
      "Epoch: 45 \tTraining Loss: 0.854517 \tTest Loss: 0.973697\n",
      "Epoch: 46 \tTraining Loss: 0.854397 \tTest Loss: 0.973077\n",
      "Epoch: 47 \tTraining Loss: 0.852670 \tTest Loss: 0.972716\n",
      "Epoch: 48 \tTraining Loss: 0.850848 \tTest Loss: 0.975488\n",
      "Epoch: 49 \tTraining Loss: 0.852828 \tTest Loss: 0.977888\n",
      "Epoch: 50 \tTraining Loss: 0.856999 \tTest Loss: 0.977587\n",
      "Epoch: 51 \tTraining Loss: 0.853554 \tTest Loss: 0.975607\n",
      "Epoch: 52 \tTraining Loss: 0.848462 \tTest Loss: 0.973462\n",
      "Epoch: 53 \tTraining Loss: 0.853492 \tTest Loss: 0.972412\n",
      "Epoch: 54 \tTraining Loss: 0.851558 \tTest Loss: 0.971521\n",
      "Epoch: 55 \tTraining Loss: 0.853862 \tTest Loss: 0.973040\n",
      "Epoch: 56 \tTraining Loss: 0.853736 \tTest Loss: 0.969741\n",
      "Epoch: 57 \tTraining Loss: 0.848256 \tTest Loss: 0.975233\n",
      "Epoch: 58 \tTraining Loss: 0.848208 \tTest Loss: 0.971623\n",
      "Epoch: 59 \tTraining Loss: 0.844565 \tTest Loss: 0.971311\n",
      "Epoch: 60 \tTraining Loss: 0.844475 \tTest Loss: 0.976832\n",
      "Epoch: 61 \tTraining Loss: 0.843729 \tTest Loss: 0.981030\n",
      "Epoch: 62 \tTraining Loss: 0.845256 \tTest Loss: 0.974916\n",
      "Epoch: 63 \tTraining Loss: 0.842707 \tTest Loss: 0.978608\n",
      "Epoch: 64 \tTraining Loss: 0.845476 \tTest Loss: 0.979593\n",
      "Epoch: 65 \tTraining Loss: 0.847384 \tTest Loss: 0.977210\n",
      "Epoch: 66 \tTraining Loss: 0.846564 \tTest Loss: 0.981818\n",
      "Epoch: 67 \tTraining Loss: 0.840713 \tTest Loss: 0.977440\n",
      "Epoch: 68 \tTraining Loss: 0.849155 \tTest Loss: 0.977293\n",
      "Epoch: 69 \tTraining Loss: 0.850811 \tTest Loss: 0.979399\n",
      "Epoch: 70 \tTraining Loss: 0.848376 \tTest Loss: 0.976134\n",
      "Epoch: 71 \tTraining Loss: 0.845419 \tTest Loss: 0.979453\n",
      "Epoch: 72 \tTraining Loss: 0.843816 \tTest Loss: 0.977313\n",
      "Epoch: 73 \tTraining Loss: 0.845123 \tTest Loss: 0.977885\n",
      "Epoch: 74 \tTraining Loss: 0.846971 \tTest Loss: 0.975743\n",
      "Epoch: 75 \tTraining Loss: 0.848191 \tTest Loss: 0.976208\n",
      "Epoch: 76 \tTraining Loss: 0.848787 \tTest Loss: 0.977426\n",
      "Epoch: 77 \tTraining Loss: 0.846439 \tTest Loss: 0.980418\n",
      "Epoch: 78 \tTraining Loss: 0.851188 \tTest Loss: 0.987048\n",
      "Epoch: 79 \tTraining Loss: 0.856623 \tTest Loss: 0.978681\n",
      "Epoch: 80 \tTraining Loss: 0.848364 \tTest Loss: 0.977997\n",
      "Epoch: 81 \tTraining Loss: 0.850395 \tTest Loss: 0.980339\n",
      "Epoch: 82 \tTraining Loss: 0.851004 \tTest Loss: 0.980145\n",
      "Epoch: 83 \tTraining Loss: 0.855738 \tTest Loss: 0.973347\n",
      "Epoch: 84 \tTraining Loss: 0.847897 \tTest Loss: 0.974595\n",
      "Epoch: 85 \tTraining Loss: 0.851185 \tTest Loss: 0.980010\n",
      "Epoch: 86 \tTraining Loss: 0.857450 \tTest Loss: 0.976648\n",
      "Epoch: 87 \tTraining Loss: 0.854792 \tTest Loss: 0.974983\n",
      "Epoch: 88 \tTraining Loss: 0.854021 \tTest Loss: 0.969726\n",
      "Epoch: 89 \tTraining Loss: 0.850909 \tTest Loss: 0.971197\n",
      "Epoch: 90 \tTraining Loss: 0.855111 \tTest Loss: 0.974325\n",
      "Epoch: 91 \tTraining Loss: 0.852400 \tTest Loss: 0.974364\n",
      "Epoch: 92 \tTraining Loss: 0.851254 \tTest Loss: 0.976380\n",
      "Epoch: 93 \tTraining Loss: 0.853087 \tTest Loss: 0.978226\n",
      "Epoch: 94 \tTraining Loss: 0.853719 \tTest Loss: 0.988200\n",
      "Epoch: 95 \tTraining Loss: 0.856616 \tTest Loss: 0.981616\n",
      "Epoch: 96 \tTraining Loss: 0.852564 \tTest Loss: 0.979764\n",
      "Epoch: 97 \tTraining Loss: 0.856866 \tTest Loss: 0.978466\n",
      "Epoch: 98 \tTraining Loss: 0.853489 \tTest Loss: 0.975000\n",
      "Epoch: 99 \tTraining Loss: 0.859319 \tTest Loss: 0.981883\n",
      "Epoch: 100 \tTraining Loss: 0.852702 \tTest Loss: 0.979760\n"
     ]
    }
   ],
   "source": [
    "concatenated_model = Feedforward_MLP(3000)\n",
    "optimizer_concat = torch.optim.Adam(concatenated_model.parameters(),lr=0.005,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "test_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(100):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    concatenated_model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer_concat.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = concatenated_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer_concat.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    concatenated_model.eval() # prep model for evaluation\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = concatenated_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if test_loss <= test_loss_min:\n",
    "        print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        test_loss_min,\n",
    "        test_loss))\n",
    "        torch.save(concatenated_model.state_dict(), 'concatenated_model.pt')\n",
    "        test_loss_min = test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "741ca8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN Accuracy (concatenated words) in % =  56.458333333333336\n"
     ]
    }
   ],
   "source": [
    "y_pred_concat=torch.max(concatenated_model(tensor_testX_concat).data,1).indices\n",
    "print(\"FNN Accuracy (concatenated words) in % = \", findAccuracy(tensor_testY_concat.tolist(), y_pred_concat.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9142d10",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "MLP  Accuracy (avg vectors)  = 66.5%\n",
    "\n",
    "MLP Accuracy (Concatenated) = 56.5%\n",
    "\n",
    "MLP with averaged vectors has higher accuray compared to MLP with first 10 concatenated vectors. Possible reasons could be,\n",
    "\n",
    "* **Information loss**: The concatenation of the first 10 words as input feature may result in information loss since the first 10 words do not necessarily capture the context of the entire sentence. As a result, the model that takes this input feature may not have enough information to accurately predict the target variable.\n",
    "\n",
    "* **Word importance**: The first 10 words of a sentence may not always be the most important for predicting the target variable. The model that takes the average of all vectors as input feature considers all words in the sentence to be equally important, which may be more accurate for certain tasks.\n",
    "\n",
    "* **Overfitting**: The model that takes the concatenation of the first 10 words as input feature may be overfitting to the training data, since the input feature is specific to the first 10 words of the sentence. On the other hand, the model that takes the average of all vectors as input feature may be more generalizable since it is based on the entire sentence.\n",
    "\n",
    "In comparision to Simple Models ( SVM and Perceptron), SVM and Feedforward MLP with averaged vectors have similar accuracy values. Where as MLP performs better than single perceptron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139dcfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ab1119c",
   "metadata": {},
   "source": [
    "## 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c630a5e1",
   "metadata": {},
   "source": [
    "##### Data generation:\n",
    "\n",
    "Generate inpyt feature by taking maximum review length of 20. Truncat longer reviews and pad shorter reviews with a null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5af2cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SINDHURA\\AppData\\Local\\Temp/ipykernel_19272/3650168203.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  x_train_rnn = torch.Tensor(train_data_rnn)\n"
     ]
    }
   ],
   "source": [
    "def generate_input_vec_20(reviews):\n",
    "    sequences =  []\n",
    "    for review in reviews:\n",
    "        text_tokens = word_tokenize(review)        \n",
    "        vectors=[]\n",
    "        for i,word in enumerate(text_tokens):\n",
    "            if word in wv and word in vocab:\n",
    "                vectors.append(wv[word].reshape((1,300)))\n",
    "                \n",
    "        if(len(vectors)>=20):\n",
    "            padded_vectors = vectors[:20]\n",
    "        else:\n",
    "            num_missing_vectors = 20-len(vectors)\n",
    "            padded_vectors = vectors + [np.zeros((1,300))]*num_missing_vectors\n",
    "            \n",
    "        sequences.append(padded_vectors)\n",
    "            \n",
    "    return sequences\n",
    "            \n",
    "input_feature_rnn = generate_input_vec_20(balanced_data['review_body'])\n",
    "train_data_rnn = input_feature_rnn[:int(0.8 * len(input_feature_rnn))]\n",
    "test_data_rnn = input_feature_rnn[int(0.8 * len(input_feature_rnn)):]\n",
    "\n",
    "x_train_rnn = torch.Tensor(train_data_rnn)\n",
    "y_train_rnn = torch.Tensor(train_label_0).type(torch.LongTensor)\n",
    "x_cv_rnn = torch.Tensor(test_data_rnn)\n",
    "y_cv_rnn = torch.Tensor(test_label_0).type(torch.LongTensor)\n",
    "\n",
    "# Create Torch datasets\n",
    "train_rnn = torch.utils.data.TensorDataset(x_train_rnn, y_train_rnn)\n",
    "test_rnn = torch.utils.data.TensorDataset(x_cv_rnn, y_cv_rnn)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader_rnn = torch.utils.data.DataLoader(train_rnn, batch_size=1,shuffle=False)\n",
    "test_loader_rnn = torch.utils.data.DataLoader(test_rnn, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9c6fce",
   "metadata": {},
   "source": [
    "### (a) Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "20dc1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "rnn_model = RNN(300,20,3)\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.0001, betas=(0.9,0.999),eps=1e-08,weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9439bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0% (0m 0s) 1.1192 / 2 ✗ (1)\n",
      "10000 20% (0m 41s) 0.4729 / 0 ✓\n",
      "20000 41% (1m 22s) 0.4034 / 1 ✓\n",
      "30000 62% (2m 3s) 0.2303 / 0 ✓\n",
      "40000 83% (2m 45s) 0.4781 / 1 ✓\n",
      "0 0% (3m 19s) 1.6544 / 0 ✗ (1)\n",
      "10000 20% (3m 55s) 0.0013 / 0 ✓\n",
      "20000 41% (4m 31s) 0.4038 / 1 ✓\n",
      "30000 62% (5m 8s) 0.1405 / 0 ✓\n",
      "40000 83% (5m 44s) 0.5038 / 1 ✓\n",
      "0 0% (6m 12s) 1.8883 / 0 ✗ (1)\n",
      "10000 20% (6m 49s) 0.0013 / 0 ✓\n",
      "20000 41% (7m 30s) 0.4309 / 1 ✓\n",
      "30000 62% (8m 6s) 0.1433 / 0 ✓\n",
      "40000 83% (8m 43s) 0.5307 / 1 ✓\n",
      "0 0% (9m 12s) 2.2324 / 0 ✗ (1)\n",
      "10000 20% (9m 48s) 0.0062 / 0 ✓\n",
      "20000 41% (10m 24s) 0.3707 / 1 ✓\n",
      "30000 62% (11m 1s) 0.1188 / 0 ✓\n",
      "40000 83% (11m 37s) 0.8881 / 2 ✗ (1)\n",
      "0 0% (12m 7s) 1.5565 / 0 ✗ (1)\n",
      "10000 20% (12m 43s) 0.0040 / 0 ✓\n",
      "20000 41% (13m 20s) 0.3755 / 1 ✓\n",
      "30000 62% (13m 56s) 0.1166 / 0 ✓\n",
      "40000 83% (14m 32s) 0.9676 / 2 ✗ (1)\n"
     ]
    }
   ],
   "source": [
    "def train(line_tensor, category):\n",
    "    hidden = rnn_model.initHidden()\n",
    "    rnn_model.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.shape[1]):\n",
    "        inp_tensor = line_tensor[0][i]\n",
    "        output, hidden = rnn_model(inp_tensor, hidden)        \n",
    "    \n",
    "    loss = criterion(output, category)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 5)\n",
    "    optimizer.step()\n",
    "\n",
    "    for p in rnn_model.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-0.005)\n",
    "    \n",
    "    return output, loss.item()\n",
    "\n",
    "print_every = 10000\n",
    "plot_every = 1000\n",
    "\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(5):\n",
    "    print_counter = 0\n",
    "    for i, (review, rating) in enumerate(train_loader_rnn):\n",
    "        output, loss = train(review, rating)\n",
    "        current_loss += loss\n",
    "        # Print iter number, loss, name and guess\n",
    "        if print_counter % print_every == 0:\n",
    "            guess = torch.argmax(output).item()\n",
    "            correct = '✓' if guess == rating.item() else '✗ (%s)' % rating.item()\n",
    "            print('%d %d%% (%s) %.4f / %s %s' % (print_counter, (print_counter / 48000) * 100, timeSince(start), loss, guess, correct))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if print_counter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "            \n",
    "        print_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5acc131f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple RNN Accuracy in % =  59.266666666666666\n"
     ]
    }
   ],
   "source": [
    "y_pred_rnn = []\n",
    "y_test_rnn = []\n",
    "rnn_model.eval()\n",
    "for i, (review_test_rnn, rating_test_rnn) in enumerate(test_loader_rnn):\n",
    "    hidden = rnn_model.initHidden()\n",
    "    for j in range(review_test_rnn.shape[1]):\n",
    "        inp_tensor = review_test_rnn[0][j]\n",
    "        guess, hidden = rnn_model(inp_tensor, hidden)\n",
    "    guess = torch.argmax(guess).item()\n",
    "    y_test_rnn.append(rating_test_rnn.item())\n",
    "    y_pred_rnn.append(guess)\n",
    "\n",
    "print(\"Simple RNN Accuracy in % = \", findAccuracy( y_test_rnn, y_pred_rnn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf016e",
   "metadata": {},
   "source": [
    "### (b) Gated RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af9b2607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SINDHURA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers=1, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim)\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0)\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "gru_model = GRUNet(300,20,3)\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=0.0005, betas=(0.9,0.999),eps=1e-08,weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41ad15d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0......Step: 1000/48000....... Average Loss for Epoch: 1.0113867344260217\n",
      "Epoch 0......Step: 2000/48000....... Average Loss for Epoch: 0.43523130851984027\n",
      "Epoch 1......Step: 1000/48000....... Average Loss for Epoch: 1.1547885800004005\n",
      "Epoch 1......Step: 2000/48000....... Average Loss for Epoch: 0.3897461163252592\n",
      "Epoch 2......Step: 1000/48000....... Average Loss for Epoch: 1.0905340039730071\n",
      "Epoch 2......Step: 2000/48000....... Average Loss for Epoch: 0.37599247312545775\n",
      "Epoch 3......Step: 1000/48000....... Average Loss for Epoch: 1.0641080539822578\n",
      "Epoch 3......Step: 2000/48000....... Average Loss for Epoch: 0.367721270442009\n",
      "Epoch 4......Step: 1000/48000....... Average Loss for Epoch: 1.0441818330287933\n",
      "Epoch 4......Step: 2000/48000....... Average Loss for Epoch: 0.36128701432049276\n",
      "Epoch 5......Step: 1000/48000....... Average Loss for Epoch: 1.0274079230725766\n",
      "Epoch 5......Step: 2000/48000....... Average Loss for Epoch: 0.3559266780465841\n",
      "Epoch 6......Step: 1000/48000....... Average Loss for Epoch: 1.0127081057727336\n",
      "Epoch 6......Step: 2000/48000....... Average Loss for Epoch: 0.3512973253726959\n",
      "Epoch 7......Step: 1000/48000....... Average Loss for Epoch: 0.9995491859018802\n",
      "Epoch 7......Step: 2000/48000....... Average Loss for Epoch: 0.34719072698056697\n",
      "Epoch 8......Step: 1000/48000....... Average Loss for Epoch: 0.987601480782032\n",
      "Epoch 8......Step: 2000/48000....... Average Loss for Epoch: 0.34347263905406\n",
      "Epoch 9......Step: 1000/48000....... Average Loss for Epoch: 0.9766237780749798\n",
      "Epoch 9......Step: 2000/48000....... Average Loss for Epoch: 0.34006228694319723\n",
      "Epoch 10......Step: 1000/48000....... Average Loss for Epoch: 0.9664845919013023\n",
      "Epoch 10......Step: 2000/48000....... Average Loss for Epoch: 0.33690439078211787\n",
      "Epoch 11......Step: 1000/48000....... Average Loss for Epoch: 0.95711850258708\n",
      "Epoch 11......Step: 2000/48000....... Average Loss for Epoch: 0.33394753132015464\n",
      "Epoch 12......Step: 1000/48000....... Average Loss for Epoch: 0.9484272119700908\n",
      "Epoch 12......Step: 2000/48000....... Average Loss for Epoch: 0.33115656752884387\n",
      "Epoch 13......Step: 1000/48000....... Average Loss for Epoch: 0.9403214182555676\n",
      "Epoch 13......Step: 2000/48000....... Average Loss for Epoch: 0.3285079066157341\n",
      "Epoch 14......Step: 1000/48000....... Average Loss for Epoch: 0.9327193158566952\n",
      "Epoch 14......Step: 2000/48000....... Average Loss for Epoch: 0.32598010558635\n",
      "Epoch 15......Step: 1000/48000....... Average Loss for Epoch: 0.9255446022748948\n",
      "Epoch 15......Step: 2000/48000....... Average Loss for Epoch: 0.3235530160963535\n",
      "Epoch 16......Step: 1000/48000....... Average Loss for Epoch: 0.9187254720926284\n",
      "Epoch 16......Step: 2000/48000....... Average Loss for Epoch: 0.3212091476097703\n",
      "Epoch 17......Step: 1000/48000....... Average Loss for Epoch: 0.9122006306052208\n",
      "Epoch 17......Step: 2000/48000....... Average Loss for Epoch: 0.31893876794725656\n",
      "Epoch 18......Step: 1000/48000....... Average Loss for Epoch: 0.9059160495698452\n",
      "Epoch 18......Step: 2000/48000....... Average Loss for Epoch: 0.3167386449798942\n",
      "Epoch 19......Step: 1000/48000....... Average Loss for Epoch: 0.899825601875782\n",
      "Epoch 19......Step: 2000/48000....... Average Loss for Epoch: 0.31460522804409263\n"
     ]
    }
   ],
   "source": [
    "def train_gru(line_tensor, category):\n",
    "    gru_model.zero_grad()\n",
    "    line_tensor = line_tensor.reshape(-1, 20, 300)\n",
    "    output = gru_model(line_tensor)        \n",
    "    \n",
    "    loss = criterion(output, category)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(gru_model.parameters(), 5)\n",
    "    optimizer.step()\n",
    "\n",
    "    for p in gru_model.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-0.005)\n",
    "    \n",
    "    return output, loss.item()\n",
    "# Keep track of losses for plotting\n",
    "train_loader_gru = torch.utils.data.DataLoader(train_rnn, batch_size=20,shuffle=False)\n",
    "test_loader_gru = torch.utils.data.DataLoader(test_rnn, batch_size=20, shuffle=False)\n",
    "\n",
    "current_loss = 0\n",
    "for epoch in range(20):\n",
    "    print_counter = 0\n",
    "    for i, (review, rating) in enumerate(train_loader_gru):\n",
    "        print_counter += 1\n",
    "        output, loss = train_gru(review, rating)        \n",
    "        current_loss += loss\n",
    "        if print_counter%1000 == 0:\n",
    "            print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, print_counter, len(train_loader_rnn), current_loss/print_counter))\n",
    "            current_loss = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1c5c67d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gated RNN Accuracy =  67.27499999999999\n"
     ]
    }
   ],
   "source": [
    "y_pred_gru = []\n",
    "y_test_gru = []\n",
    "gru_model.eval()\n",
    "for i, (review, rating) in enumerate(test_loader_rnn):\n",
    "    line_tensor = review.reshape(-1, 20, 300)\n",
    "    output = gru_model(line_tensor) \n",
    "    output = torch.argmax(output).item()\n",
    "    y_test_gru.append(rating.item())\n",
    "    y_pred_gru.append(output)\n",
    "\n",
    "print(\"Gated RNN Accuracy = \", findAccuracy(y_test_gru, y_pred_gru))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2d2f2",
   "metadata": {},
   "source": [
    "### (c) LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "965ae8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0......Step: 1000/48000....... Average Loss for Epoch: 1.0196112685799599\n",
      "Epoch 0......Step: 2000/48000....... Average Loss for Epoch: 0.4385915504544973\n",
      "Epoch 1......Step: 1000/48000....... Average Loss for Epoch: 1.1694887163043022\n",
      "Epoch 1......Step: 2000/48000....... Average Loss for Epoch: 0.3989686401337385\n",
      "Epoch 2......Step: 1000/48000....... Average Loss for Epoch: 1.1120957169234753\n",
      "Epoch 2......Step: 2000/48000....... Average Loss for Epoch: 0.38428275449573995\n",
      "Epoch 3......Step: 1000/48000....... Average Loss for Epoch: 1.080011067122221\n",
      "Epoch 3......Step: 2000/48000....... Average Loss for Epoch: 0.37453182561695575\n",
      "Epoch 4......Step: 1000/48000....... Average Loss for Epoch: 1.0560543293654918\n",
      "Epoch 4......Step: 2000/48000....... Average Loss for Epoch: 0.3667751659452915\n",
      "Epoch 5......Step: 1000/48000....... Average Loss for Epoch: 1.0360082157552242\n",
      "Epoch 5......Step: 2000/48000....... Average Loss for Epoch: 0.3602815056219697\n",
      "Epoch 6......Step: 1000/48000....... Average Loss for Epoch: 1.0185942147374154\n",
      "Epoch 6......Step: 2000/48000....... Average Loss for Epoch: 0.35473686560243367\n",
      "Epoch 7......Step: 1000/48000....... Average Loss for Epoch: 1.0033243864178658\n",
      "Epoch 7......Step: 2000/48000....... Average Loss for Epoch: 0.34996139781177044\n",
      "Epoch 8......Step: 1000/48000....... Average Loss for Epoch: 0.9898229793310166\n",
      "Epoch 8......Step: 2000/48000....... Average Loss for Epoch: 0.3457866867631674\n",
      "Epoch 9......Step: 1000/48000....... Average Loss for Epoch: 0.9778707551658153\n",
      "Epoch 9......Step: 2000/48000....... Average Loss for Epoch: 0.3420435303002596\n",
      "Epoch 10......Step: 1000/48000....... Average Loss for Epoch: 0.9670844424068927\n",
      "Epoch 10......Step: 2000/48000....... Average Loss for Epoch: 0.3386075641959906\n",
      "Epoch 11......Step: 1000/48000....... Average Loss for Epoch: 0.9571639757156372\n",
      "Epoch 11......Step: 2000/48000....... Average Loss for Epoch: 0.33541244065016507\n",
      "Epoch 12......Step: 1000/48000....... Average Loss for Epoch: 0.9479696825742722\n",
      "Epoch 12......Step: 2000/48000....... Average Loss for Epoch: 0.33239467191696165\n",
      "Epoch 13......Step: 1000/48000....... Average Loss for Epoch: 0.9393274251222611\n",
      "Epoch 13......Step: 2000/48000....... Average Loss for Epoch: 0.32951243632286786\n",
      "Epoch 14......Step: 1000/48000....... Average Loss for Epoch: 0.9310925214886665\n",
      "Epoch 14......Step: 2000/48000....... Average Loss for Epoch: 0.3267280769199133\n",
      "Epoch 15......Step: 1000/48000....... Average Loss for Epoch: 0.923180210351944\n",
      "Epoch 15......Step: 2000/48000....... Average Loss for Epoch: 0.3240097621679306\n",
      "Epoch 16......Step: 1000/48000....... Average Loss for Epoch: 0.9154965470135212\n",
      "Epoch 16......Step: 2000/48000....... Average Loss for Epoch: 0.3213762909621\n",
      "Epoch 17......Step: 1000/48000....... Average Loss for Epoch: 0.9079940627217293\n",
      "Epoch 17......Step: 2000/48000....... Average Loss for Epoch: 0.3188462066575885\n",
      "Epoch 18......Step: 1000/48000....... Average Loss for Epoch: 0.9007407888770104\n",
      "Epoch 18......Step: 2000/48000....... Average Loss for Epoch: 0.31640006927400827\n",
      "Epoch 19......Step: 1000/48000....... Average Loss for Epoch: 0.8938386622667313\n",
      "Epoch 19......Step: 2000/48000....... Average Loss for Epoch: 0.31404206673800944\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layer_dim=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "\n",
    "lstm_model = LSTMModel(300, 20, 3)\n",
    "optimizer_lstm = torch.optim.Adam(lstm_model.parameters(), lr=0.0005, betas=(0.9,0.999),eps=1e-08,weight_decay=5e-5)  \n",
    "\n",
    "\n",
    "def train_lstm(line_tensor, category):\n",
    "    lstm_model.zero_grad()\n",
    "    line_tensor = line_tensor.reshape(-1, 20, 300)\n",
    "    output = lstm_model(line_tensor)        \n",
    "    \n",
    "    loss = criterion(output, category)\n",
    "    optimizer_lstm.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), 5)\n",
    "    optimizer_lstm.step()\n",
    "\n",
    "    for p in lstm_model.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-0.005)\n",
    "    \n",
    "    return output, loss.item()\n",
    "\n",
    "current_loss = 0\n",
    "for epoch in range(20):\n",
    "    print_counter = 0\n",
    "    for i, (review, rating) in enumerate(train_loader_gru):\n",
    "        print_counter += 1\n",
    "        output, loss = train_lstm(review, rating)       \n",
    "        current_loss += loss\n",
    "        if print_counter%1000 == 0:\n",
    "            print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, print_counter, len(train_loader_rnn), current_loss/print_counter))\n",
    "            current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "45ad8d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Accuracy in % =  67.175\n"
     ]
    }
   ],
   "source": [
    "y_pred_lstm = []\n",
    "y_test_lstm = []\n",
    "lstm_model.eval()\n",
    "for i, (review, rating) in enumerate(test_loader_rnn):\n",
    "    line_tensor = review.reshape(-1, 20, 300)\n",
    "    output = lstm_model(line_tensor) \n",
    "    output = torch.argmax(output).item()\n",
    "    y_test_lstm.append(rating.item())\n",
    "    y_pred_lstm.append(output)\n",
    "\n",
    "print(\"LSTM Accuracy in % = \", findAccuracy(y_test_lstm, y_pred_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb845f",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "Simple RNN = 59.3%\n",
    "Gated RNN = 67.2%\n",
    "LSTM = 67.2%\n",
    "\n",
    "Gated RNN models are designed to address the vanishing gradient problem that can occur in simple RNN models, where the gradient signal becomes too small to propagate through the network during backpropagation. This can lead to difficulty in capturing long-term dependencies in the data. The vanishing gradient problem can occur when the recurrent weights in an RNN are repeatedly multiplied by small values causing the gradient signal to shrink exponentially over time.\n",
    "\n",
    "Gated RNN models, on the other hand, use gating mechanisms to selectively update the hidden state and control the flow of information through the network. The gating mechanisms allow the model to remember information over longer periods of time and avoid the vanishing gradient problem.\n",
    "\n",
    "Therefore, Gated RNN models outperform the simple RNN model, because they are better at capturing long-term dependencies in the data. \n",
    "\n",
    "LSTM has more parameters compared to a simple RNN model, which means that it has more capacity to learn complex patterns in the data. This can also contribute to the improved performance of LSTM over the simple RNN model.\n",
    "\n",
    "Also, RNN has better performance overall when compared to FFN because FFN considered the average of review vectors which might suffer probelm of outliers ( too high or too low values). In contrast, RNN considered each word seperatly allowing them to better capture patterns in sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528dff30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25ebdb7b",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "* https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "* https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook\n",
    "* https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/\n",
    "* https://blog.floydhub.com/gru-with-pytorch/\n",
    "* https://towardsdatascience.com/building-rnn-lstm-and-gru-for-time-series-using-pytorch-a46e5b094e7b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
