{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42437534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "\n",
    "from collections import defaultdict\n",
    "nltk.download('wordnet',quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt',quiet=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "102bbc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz\"\n",
    "data = pd.read_csv(reviews, sep='\\t', on_bad_lines='skip', low_memory=False)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c267c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "review_data = data[['star_rating', 'review_body']]\n",
    "review_data.dropna(inplace=True)\n",
    "review_data['star_rating'] = review_data['star_rating'].astype('int32')\n",
    "\n",
    "for index, row in review_data.iterrows():\n",
    "    if row['star_rating'] in {1,2}:\n",
    "        review_data.loc[index, 'star_rating'] = 1\n",
    "    elif row['star_rating'] in {3}:\n",
    "        review_data.loc[index, 'star_rating'] = 2\n",
    "    elif row['star_rating'] in {4,5}:\n",
    "        review_data.loc[index, 'star_rating'] = 3\n",
    "        \n",
    "bal_data = review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "293f7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_data.to_csv('data_full', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525418a",
   "metadata": {},
   "source": [
    "### 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "3675666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv(\"data_full\", sep='\\t', names = ['star_rating', 'review_body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32843d78",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "8078d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopword(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw =  \" \".join([word for word in text_tokens if word not in stop_words])\n",
    "    return tokens_without_sw\n",
    "\n",
    "def lemmetize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text_tokens = word_tokenize(text)\n",
    "    lemmatized_string = \" \".join([lemmatizer.lemmatize(words) for words in text_tokens])\n",
    "\n",
    "    return lemmatized_string\n",
    "\n",
    "def clean_data(data):\n",
    "    #covert to lower\n",
    "    data= data.lower()\n",
    "    #remove html and url\n",
    "    data = re.sub(r'http\\S+', '', data)\n",
    "    urls = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    data = re.sub(urls, '', data)\n",
    "    #remove non-alphabetical chars\n",
    "    non_alpha = re.compile('[^a-zA-Z]')\n",
    "    data = non_alpha.sub(' ', data)\n",
    "    #remove extra spaces\n",
    "    data = re.sub(' +', ' ', data)\n",
    "    #perform contractions\n",
    "    data = contractions.fix(data)\n",
    "    return data\n",
    "\n",
    "full_data['review_body'] = full_data.apply(lambda row : clean_data(row['review_body']), axis = 1)\n",
    "# bal_data['review_body'] = bal_data.apply(lambda row : remove_stopword(row['review_body']), axis = 1)\n",
    "# bal_data['review_body'] = bal_data.apply(lambda row : lemmetize(row['review_body']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "2eb34464",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.to_csv('data_cleaned', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f149c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bal_data = pd.read_csv(\"data_cleaned\", sep='\\t', names = ['star_rating', 'review_body'])\n",
    "# bal_data.dropna(inplace=True)\n",
    "# review_samples = []\n",
    "\n",
    "# for rating in [1,2,3]:\n",
    "#     rating_df = bal_data[ bal_data['star_rating'] == rating ]\n",
    "\n",
    "#     reviews = rating_df['review_body'].tolist()\n",
    "\n",
    "#     tfIdfVect = TfidfVectorizer(use_idf=True)\n",
    "#     # getting tf-idf vectors\n",
    "#     vect_review = tfIdfVect.fit_transform(rating_df['review_body'])\n",
    "\n",
    "#     # take sum of tf-idf vector values -> indicates \"total\" importance of the review in the pool of other reviews \n",
    "#     # having the same rating\n",
    "#     vals = list(np.squeeze(np.asarray(np.sum(vect_review, axis = 1).astype(np.float32))))\n",
    "    \n",
    "#     vals = [vals[i]/len(reviews[i]) for i in range(len(vals))]\n",
    "    \n",
    "#     # sort by tf-idf sum value and take top 20000 reviews for each class\n",
    "#     rating_df = pd.DataFrame(list(zip(reviews, vals, rating_df['star_rating'].tolist())), \n",
    "#                                     columns=['review_body','tfidf_score', 'star_rating'])\n",
    "\n",
    "#     rating_df = rating_df.sort_values(by=['tfidf_score'], ascending=False)\n",
    "#     review_samples.append(rating_df.head(20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ad8d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bal_data = pd.concat(review_samples)\n",
    "\n",
    "# bal_data.drop(columns=['tfidf_score'], inplace=True)\n",
    "\n",
    "# bal_data = shuffle(bal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "617506fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bal_data.to_csv('balanced_data', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "72d7625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SINDHURA\\AppData\\Local\\Temp/ipykernel_17916/1721227163.py:22: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rv /= len(words)\n"
     ]
    }
   ],
   "source": [
    "balanced_data = pd.read_csv(\"data_cleaned\", sep='\\t', names = ['star_rating', 'review_body'])\n",
    "balanced_data.dropna(inplace=True)\n",
    "balanced_data = balanced_data.groupby('star_rating').apply(lambda group: group.sample(20000)).reset_index(drop = True)\n",
    "balanced_data = shuffle(balanced_data)\n",
    "vocab = defaultdict(int)\n",
    "for l in balanced_data['review_body']:\n",
    "    for w in l.split():\n",
    "        vocab[w] += 1 \n",
    "        \n",
    "for key in list(vocab.keys()):\n",
    "    if vocab[key] < 3:\n",
    "        del vocab[key]\n",
    "\n",
    "test_reviews = balanced_data['review_body']\n",
    "avg_review_vectors = []\n",
    "for review in test_reviews:\n",
    "    words = review.split()\n",
    "    rv = np.zeros(300)\n",
    "    for word in words:\n",
    "        if word in wv and word in vocab:\n",
    "            rv += wv[word]\n",
    "    rv /= len(words)\n",
    "    avg_review_vectors.append(rv)\n",
    "\n",
    "avg_review_vectors = np.array(avg_review_vectors)\n",
    "review_ratings = balanced_data['star_rating']\n",
    "train_X = avg_review_vectors[:int(0.8 * len(avg_review_vectors))]\n",
    "train_Y = review_ratings[:int(0.8 * len(avg_review_vectors))]\n",
    "test_X = avg_review_vectors[int(0.8 * len(avg_review_vectors)):]\n",
    "test_Y =  review_ratings[int(0.8 * len(avg_review_vectors)):]\n",
    "train_X=np.nan_to_num(train_X, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "test_X=np.nan_to_num(test_X,copy=True, nan=0.0, posinf=None, neginf=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ebedf1",
   "metadata": {},
   "source": [
    "### 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab9dc86",
   "metadata": {},
   "source": [
    "#### (a)  Google news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "7092579a",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17916/1870825812.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word2vec-google-news-300'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\downloader.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/gensim-data\\word2vec-google-news-300\\__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'word2vec-google-news-300'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"word2vec-google-news-300.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m         \"\"\"\n\u001b[1;32m-> 1719\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2060\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2061\u001b[0m             \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2062\u001b[1;33m         \u001b[0mkv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2063\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2064\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vector_size, count, dtype, mapfile_path)\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_to_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# formerly known as syn0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4783c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generating word embedding\n",
    "words = [\"cat\", \"dog\", \"man\", \"woman\"]\n",
    "vectors = [wv[word] for word in words]\n",
    "\n",
    "# print(\"Words and their Word embedding\")\n",
    "# print(\"------------------------------\")\n",
    "# for word, vector in zip(words, vectors):\n",
    "#     print(f\"Word: {word}  Vector: {vector}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31735994",
   "metadata": {},
   "source": [
    "To learn the semantic similarity, I am considering the below three examples:\n",
    "* Finding words similar to \"happy\" using its vector\n",
    "* Performing \"happy - smile + cry = sad\" using word vector\n",
    "* Finding cosine similarity between words \"fight\" and \"battle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "731c6578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 similar words to happy:\n",
      "\thappy: 1.0000\n",
      "\tglad: 0.7409\n",
      "\tpleased: 0.6632\n",
      "\tecstatic: 0.6627\n",
      "\toverjoyed: 0.6599\n",
      "\n",
      "Cosine similarity between love and like: 0.36713877\n",
      "\n",
      "Most similar words to 'big-large+small':\n",
      "\tbig: 0.7968\n",
      "\tsmall: 0.6329\n",
      "\tbigger: 0.5330\n",
      "\thuge: 0.4986\n",
      "\tlittle_bitty: 0.4698\n",
      "\tbiggest: 0.4613\n",
      "\ttiny: 0.4609\n",
      "\tSmall: 0.4602\n",
      "\tnice: 0.4599\n",
      "\tabig: 0.4512\n"
     ]
    }
   ],
   "source": [
    "print(\"3 similar words to happy:\")\n",
    "res = wv.similar_by_vector(wv[\"happy\"], topn=5)\n",
    "for word, score in res:\n",
    "    print(\"\\t{}: {:.4f}\".format(word, score))\n",
    "print()\n",
    "\n",
    "cosine = wv.similarity(\"love\", \"like\")\n",
    "print(\"Cosine similarity between love and like:\", cosine)\n",
    "print()\n",
    "\n",
    "\n",
    "big = wv['big']\n",
    "large = wv['large']\n",
    "small = wv['small']\n",
    "result = big-large+small\n",
    "similarity = wv.similar_by_vector(result)\n",
    "print(\"Most similar words to 'big-large+small':\")\n",
    "for word, score in similarity:\n",
    "    print(\"\\t{}: {:.4f}\".format(word, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e3242",
   "metadata": {},
   "source": [
    "#### (b) Training Word2Vec on Review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ba4634e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10820456, 15495095)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "r_data =bal_data['review_body']\n",
    "sentences = []\n",
    "for s in r_data:\n",
    "    sentences.append(list(s.split(\" \")))\n",
    "    \n",
    "my_model = Word2Vec(sentences, vector_size=300, window=13, min_count=9)\n",
    "my_model.train(sentences, total_examples=my_model.corpus_count, epochs=my_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "912af4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 similar words to happy:\n",
      "\tmoved: 0.2399\n",
      "\thaving: 0.2251\n",
      "\towners: 0.2117\n",
      "\tbeing: 0.1966\n",
      "\tdropping: 0.1966\n",
      "\n",
      "Cosine similarity between love and like: 0.3219804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"3 similar words to happy:\")\n",
    "res = my_model.wv.similar_by_vector(wv[\"happy\"], topn=5)\n",
    "for word, score in res:\n",
    "    print(\"\\t{}: {:.4f}\".format(word, score))\n",
    "print()\n",
    "\n",
    "cosine = my_model.wv.similarity(\"love\", \"like\")\n",
    "print(\"Cosine similarity between love and like:\", cosine)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f218f6",
   "metadata": {},
   "source": [
    "### 3. Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "428c78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAccuracy(y_test, y_pred):\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, digits=4)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    for i in range(3):\n",
    "        print(f\"Class {i+1} = {df['precision'][i-1]} , {df['recall'][i-1]} , {df['f1-score'][i-1]}\")\n",
    "    print(f\"average = {df['precision'].mean()} , {df['recall'].mean()} , {df['f1-score'].mean()}\")\n",
    "    print(\"Accuracy = \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d152e20",
   "metadata": {},
   "source": [
    "#### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "adb6b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "reviews = bal_data['review_body']\n",
    "review_vectors = []\n",
    "for review in reviews:\n",
    "    words = review.split()\n",
    "    review_vector = np.zeros(300)\n",
    "    for word in words:\n",
    "        if word in wv:\n",
    "            review_vector += wv[word]\n",
    "    review_vector /= len(words)\n",
    "    review_vectors.append(review_vector)\n",
    "review_vectors = np.array(review_vectors)\n",
    "\n",
    "ratings = bal_data['star_rating']\n",
    "train_data = review_vectors[:int(0.8 * len(review_vectors))]\n",
    "train_labels = ratings[:int(0.8 * len(review_vectors))]\n",
    "test_data = review_vectors[int(0.8 * len(review_vectors)):]\n",
    "test_labels =  ratings[int(0.8 * len(review_vectors)):]\n",
    "train_data=np.nan_to_num(train_data, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "test_data=np.nan_to_num(test_data,copy=True, nan=0.0, posinf=None, neginf=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4f810b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 = 0.6291793242757951 , 0.6139166666666667 , 0.6170392014900615\n",
      "Class 2 = 0.6963562753036437 , 0.599601593625498 , 0.6443671394166444\n",
      "Class 3 = 0.5105409705648369 , 0.6412690482138396 , 0.5684863248809655\n",
      "average = 0.6266499972244798 , 0.6139110441353318 , 0.6165250133803749\n",
      "Accuracy =  0.6139166666666667\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(max_iter=9000)\n",
    "perceptron.fit(train_data, train_labels)\n",
    "perceptron_predictions = perceptron.predict(test_data)\n",
    "findAccuracy(test_labels, perceptron_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "feb6a154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 = 0.7115481186378178 , 0.7074166666666667 , 0.6977263581801021\n",
      "Class 2 = 0.7105393369619001 , 0.7269045811187042 , 0.7186288002001752\n",
      "Class 3 = 0.732420429311621 , 0.4978616352201258 , 0.5927811891568069\n",
      "average = 0.7109654877052217 , 0.7064157974889214 , 0.6989149343114422\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(C=0.1)\n",
    "svm.fit(train_data, train_labels)\n",
    "svm_predictions = svm.predict(test_data)\n",
    "findAccuracy(test_labels, svm_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff1466d",
   "metadata": {},
   "source": [
    "### 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e8439bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward_MLP(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Feedforward_MLP(torch.nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(Feedforward_MLP, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, 50)\n",
    "            self.dropout = torch.nn.Dropout(0.2)\n",
    "            \n",
    "#             self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(50, 10)\n",
    "            self.fc3 = torch.nn.Linear(10, 3)\n",
    "#             self.sigmoid = torch.nn.Sigmoid()\n",
    "            self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "        \n",
    "mlp_model = Feedforward_MLP(300)\n",
    "print(mlp_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e4f1fb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "024ae834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.825234 \tTest Loss: 1.462071\n",
      "Test loss decreased (inf --> 1.462071).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.830298 \tTest Loss: 1.334126\n",
      "Test loss decreased (1.462071 --> 1.334126).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.821584 \tTest Loss: 1.374535\n",
      "Epoch: 4 \tTraining Loss: 0.822289 \tTest Loss: 1.213783\n",
      "Test loss decreased (1.334126 --> 1.213783).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.820358 \tTest Loss: 1.153229\n",
      "Test loss decreased (1.213783 --> 1.153229).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.819128 \tTest Loss: 1.040245\n",
      "Test loss decreased (1.153229 --> 1.040245).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.816706 \tTest Loss: 1.105544\n",
      "Epoch: 8 \tTraining Loss: 0.813357 \tTest Loss: 1.209472\n",
      "Epoch: 9 \tTraining Loss: 0.816107 \tTest Loss: 1.178686\n",
      "Epoch: 10 \tTraining Loss: 0.814381 \tTest Loss: 1.147119\n",
      "Epoch: 11 \tTraining Loss: 0.816718 \tTest Loss: 1.093866\n",
      "Epoch: 12 \tTraining Loss: 0.818749 \tTest Loss: 1.155726\n",
      "Epoch: 13 \tTraining Loss: 0.819823 \tTest Loss: 1.114640\n",
      "Epoch: 14 \tTraining Loss: 0.819742 \tTest Loss: 1.131737\n",
      "Epoch: 15 \tTraining Loss: 0.820929 \tTest Loss: 1.122296\n",
      "Epoch: 16 \tTraining Loss: 0.819424 \tTest Loss: 1.118051\n",
      "Epoch: 17 \tTraining Loss: 0.820941 \tTest Loss: 1.105814\n",
      "Epoch: 18 \tTraining Loss: 0.817937 \tTest Loss: 1.131387\n",
      "Epoch: 19 \tTraining Loss: 0.822670 \tTest Loss: 1.218237\n",
      "Epoch: 20 \tTraining Loss: 0.814193 \tTest Loss: 1.185323\n",
      "Epoch: 21 \tTraining Loss: 0.817503 \tTest Loss: 1.090307\n",
      "Epoch: 22 \tTraining Loss: 0.821162 \tTest Loss: 1.184783\n",
      "Epoch: 23 \tTraining Loss: 0.820763 \tTest Loss: 1.102368\n",
      "Epoch: 24 \tTraining Loss: 0.818096 \tTest Loss: 1.226094\n",
      "Epoch: 25 \tTraining Loss: 0.813939 \tTest Loss: 1.175149\n",
      "Epoch: 26 \tTraining Loss: 0.817331 \tTest Loss: 1.242101\n",
      "Epoch: 27 \tTraining Loss: 0.815690 \tTest Loss: 1.166069\n",
      "Epoch: 28 \tTraining Loss: 0.812468 \tTest Loss: 1.313767\n",
      "Epoch: 29 \tTraining Loss: 0.814700 \tTest Loss: 1.287794\n",
      "Epoch: 30 \tTraining Loss: 0.813666 \tTest Loss: 1.161961\n",
      "Epoch: 31 \tTraining Loss: 0.820813 \tTest Loss: 1.185528\n",
      "Epoch: 32 \tTraining Loss: 0.815341 \tTest Loss: 1.223590\n",
      "Epoch: 33 \tTraining Loss: 0.822185 \tTest Loss: 1.275085\n",
      "Epoch: 34 \tTraining Loss: 0.816398 \tTest Loss: 1.338450\n",
      "Epoch: 35 \tTraining Loss: 0.818806 \tTest Loss: 1.308126\n",
      "Epoch: 36 \tTraining Loss: 0.818942 \tTest Loss: 1.244946\n",
      "Epoch: 37 \tTraining Loss: 0.816408 \tTest Loss: 1.185464\n",
      "Epoch: 38 \tTraining Loss: 0.820976 \tTest Loss: 1.205923\n",
      "Epoch: 39 \tTraining Loss: 0.818424 \tTest Loss: 1.267139\n",
      "Epoch: 40 \tTraining Loss: 0.815637 \tTest Loss: 1.450175\n",
      "Epoch: 41 \tTraining Loss: 0.813752 \tTest Loss: 1.219278\n",
      "Epoch: 42 \tTraining Loss: 0.813341 \tTest Loss: 1.261481\n",
      "Epoch: 43 \tTraining Loss: 0.819583 \tTest Loss: 1.334807\n",
      "Epoch: 44 \tTraining Loss: 0.817676 \tTest Loss: 1.345037\n",
      "Epoch: 45 \tTraining Loss: 0.820751 \tTest Loss: 1.258298\n",
      "Epoch: 46 \tTraining Loss: 0.813978 \tTest Loss: 1.284431\n",
      "Epoch: 47 \tTraining Loss: 0.821499 \tTest Loss: 1.286955\n",
      "Epoch: 48 \tTraining Loss: 0.820401 \tTest Loss: 1.254191\n",
      "Epoch: 49 \tTraining Loss: 0.831449 \tTest Loss: 1.202462\n",
      "Epoch: 50 \tTraining Loss: 0.811655 \tTest Loss: 1.201349\n",
      "Epoch: 51 \tTraining Loss: 0.816900 \tTest Loss: 1.266860\n",
      "Epoch: 52 \tTraining Loss: 0.818967 \tTest Loss: 1.308711\n",
      "Epoch: 53 \tTraining Loss: 0.817105 \tTest Loss: 1.195534\n",
      "Epoch: 54 \tTraining Loss: 0.814356 \tTest Loss: 1.300610\n",
      "Epoch: 55 \tTraining Loss: 0.816941 \tTest Loss: 1.204701\n",
      "Epoch: 56 \tTraining Loss: 0.812930 \tTest Loss: 1.309965\n",
      "Epoch: 57 \tTraining Loss: 0.821661 \tTest Loss: 1.213934\n",
      "Epoch: 58 \tTraining Loss: 0.813155 \tTest Loss: 1.251876\n",
      "Epoch: 59 \tTraining Loss: 0.820022 \tTest Loss: 1.227225\n",
      "Epoch: 60 \tTraining Loss: 0.814215 \tTest Loss: 1.278534\n",
      "Epoch: 61 \tTraining Loss: 0.815494 \tTest Loss: 1.298429\n",
      "Epoch: 62 \tTraining Loss: 0.814142 \tTest Loss: 1.177060\n",
      "Epoch: 63 \tTraining Loss: 0.808733 \tTest Loss: 1.211813\n",
      "Epoch: 64 \tTraining Loss: 0.816795 \tTest Loss: 1.245723\n",
      "Epoch: 65 \tTraining Loss: 0.817353 \tTest Loss: 1.183227\n",
      "Epoch: 66 \tTraining Loss: 0.820927 \tTest Loss: 1.312463\n",
      "Epoch: 67 \tTraining Loss: 0.818609 \tTest Loss: 1.406628\n",
      "Epoch: 68 \tTraining Loss: 0.815532 \tTest Loss: 1.340242\n",
      "Epoch: 69 \tTraining Loss: 0.813700 \tTest Loss: 1.463863\n",
      "Epoch: 70 \tTraining Loss: 0.815208 \tTest Loss: 1.415139\n",
      "Epoch: 71 \tTraining Loss: 0.817420 \tTest Loss: 1.328132\n",
      "Epoch: 72 \tTraining Loss: 0.806303 \tTest Loss: 1.379822\n",
      "Epoch: 73 \tTraining Loss: 0.812344 \tTest Loss: 1.385707\n",
      "Epoch: 74 \tTraining Loss: 0.812922 \tTest Loss: 1.396515\n",
      "Epoch: 75 \tTraining Loss: 0.814033 \tTest Loss: 1.446421\n",
      "Epoch: 76 \tTraining Loss: 0.817200 \tTest Loss: 1.402757\n",
      "Epoch: 77 \tTraining Loss: 0.818524 \tTest Loss: 1.387233\n",
      "Epoch: 78 \tTraining Loss: 0.812696 \tTest Loss: 1.313026\n",
      "Epoch: 79 \tTraining Loss: 0.818854 \tTest Loss: 1.367596\n",
      "Epoch: 80 \tTraining Loss: 0.811374 \tTest Loss: 1.273305\n",
      "Epoch: 81 \tTraining Loss: 0.817471 \tTest Loss: 1.324132\n",
      "Epoch: 82 \tTraining Loss: 0.818448 \tTest Loss: 1.391223\n",
      "Epoch: 83 \tTraining Loss: 0.816854 \tTest Loss: 1.288296\n",
      "Epoch: 84 \tTraining Loss: 0.816018 \tTest Loss: 1.311742\n",
      "Epoch: 85 \tTraining Loss: 0.813834 \tTest Loss: 1.450492\n",
      "Epoch: 86 \tTraining Loss: 0.814121 \tTest Loss: 1.340977\n",
      "Epoch: 87 \tTraining Loss: 0.811904 \tTest Loss: 1.297805\n",
      "Epoch: 88 \tTraining Loss: 0.820402 \tTest Loss: 1.240226\n",
      "Epoch: 89 \tTraining Loss: 0.812399 \tTest Loss: 1.257751\n",
      "Epoch: 90 \tTraining Loss: 0.811986 \tTest Loss: 1.218605\n",
      "Epoch: 91 \tTraining Loss: 0.815713 \tTest Loss: 1.347570\n",
      "Epoch: 92 \tTraining Loss: 0.816024 \tTest Loss: 1.360696\n",
      "Epoch: 93 \tTraining Loss: 0.813410 \tTest Loss: 1.289533\n",
      "Epoch: 94 \tTraining Loss: 0.818714 \tTest Loss: 1.293643\n",
      "Epoch: 95 \tTraining Loss: 0.822663 \tTest Loss: 1.392459\n",
      "Epoch: 96 \tTraining Loss: 0.814709 \tTest Loss: 1.307855\n",
      "Epoch: 97 \tTraining Loss: 0.817133 \tTest Loss: 1.313517\n",
      "Epoch: 98 \tTraining Loss: 0.810213 \tTest Loss: 1.311612\n",
      "Epoch: 99 \tTraining Loss: 0.811550 \tTest Loss: 1.497221\n",
      "Epoch: 100 \tTraining Loss: 0.819064 \tTest Loss: 1.167414\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size= 15\n",
    "n_classes = 3\n",
    "input_size = 300\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_label_0 = [x-1 for x in tr_l]\n",
    "test_label_0 = [x-1 for x in ts_l]\n",
    "\n",
    "\n",
    "x_train = torch.Tensor(tr_d)\n",
    "y_train = torch.Tensor(train_label_0).type(torch.LongTensor)\n",
    "x_cv = torch.Tensor(ts_d)\n",
    "y_cv = torch.Tensor(test_label_0).type(torch.LongTensor)\n",
    "\n",
    "# Create Torch datasets\n",
    "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test = torch.utils.data.TensorDataset(x_cv, y_cv)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,  sampler = sampler,shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(),lr=0.005)\n",
    "test_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    mlp_model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = mlp_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    mlp_model.eval() # prep model for evaluation\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = mlp_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if test_loss <= test_loss_min:\n",
    "        print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        test_loss_min,\n",
    "        test_loss))\n",
    "        torch.save(mlp_model.state_dict(), 'mlp_model.pt')\n",
    "        test_loss_min = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b618bad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 = 0.6226853177888603 , 0.6250833333333333 , 0.6173578675732784\n",
      "Class 2 = 0.5997079682937004 , 0.7282168186423505 , 0.657744223289865\n",
      "Class 3 = 0.5912954777286638 , 0.43044554455445544 , 0.49820942558372733\n",
      "average = 0.6230522167545917 , 0.6257507976706318 , 0.6189733992899935\n",
      "Accuracy =  0.6250833333333333\n"
     ]
    }
   ],
   "source": [
    "y_pred_test=torch.max(mlp_model(x_cv).data,1).indices\n",
    "findAccuracy(y_cv.tolist(), y_pred_test.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b79aad",
   "metadata": {},
   "source": [
    "#### (b) Input feature as concatenated vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3f64fd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenate_MLP(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Concatenate_MLP(torch.nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(Concatenate_MLP, self).__init__()\n",
    "            self.input_size = input_size            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, 50)\n",
    "            self.dropout = torch.nn.Dropout(0.2)\n",
    "            self.fc2 = torch.nn.Linear(50, 10)\n",
    "            self.fc3 = torch.nn.Linear(10, 3)\n",
    "            self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            x= self.softmax(x)\n",
    "            return x\n",
    "        \n",
    "concatenated_model = Concatenate_MLP(3000)\n",
    "print(concatenated_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ac3c20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_feature_vector(reviews):\n",
    "    review_words = [review.split() for review in reviews]    \n",
    "    vector_size = wv.vector_size    \n",
    "    num_reviews = len(reviews)\n",
    "    input_features = np.zeros((num_reviews, 10*vector_size))\n",
    "    \n",
    "    for i, words in enumerate(review_words):\n",
    "        vectors = []\n",
    "        for j in range(min(len(words), 10)):\n",
    "            word = words[j]\n",
    "            if word in wv:\n",
    "                vectors.append(wv[word])\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        num_missing_vectors = max(0, 10 - len(vectors))\n",
    "        padded_vectors = vectors + [np.zeros(vector_size)]*num_missing_vectors        \n",
    "        feature_vector = np.concatenate(padded_vectors)        \n",
    "        input_features[i,:] = feature_vector\n",
    "    \n",
    "    return input_features\n",
    "\n",
    "input_feature = generate_input_feature_vector(test_60['review_body'])\n",
    "input_labels = test_60['star_rating']\n",
    "\n",
    "train_data_concat = input_feature[:int(0.8 * len(input_feature))]\n",
    "train_label_concat = input_labels[:int(0.8 * len(input_labels))]\n",
    "test_data_concat = input_feature[int(0.8 * len(input_feature)):]\n",
    "test_label_concat =  input_labels[int(0.8 * len(input_labels)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "45bd324d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.981438 \tTest Loss: 0.969641\n",
      "Test loss decreased (inf --> 0.969641).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.943694 \tTest Loss: 0.966551\n",
      "Test loss decreased (0.969641 --> 0.966551).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.933607 \tTest Loss: 0.969582\n",
      "Epoch: 4 \tTraining Loss: 0.919472 \tTest Loss: 0.978174\n",
      "Epoch: 5 \tTraining Loss: 0.909904 \tTest Loss: 0.975660\n",
      "Epoch: 6 \tTraining Loss: 0.902423 \tTest Loss: 0.975728\n",
      "Epoch: 7 \tTraining Loss: 0.894346 \tTest Loss: 0.980218\n",
      "Epoch: 8 \tTraining Loss: 0.894622 \tTest Loss: 0.981943\n",
      "Epoch: 9 \tTraining Loss: 0.891956 \tTest Loss: 0.983494\n",
      "Epoch: 10 \tTraining Loss: 0.884797 \tTest Loss: 0.980240\n",
      "Epoch: 11 \tTraining Loss: 0.880723 \tTest Loss: 0.984680\n",
      "Epoch: 12 \tTraining Loss: 0.880338 \tTest Loss: 0.984363\n",
      "Epoch: 13 \tTraining Loss: 0.876303 \tTest Loss: 0.984842\n",
      "Epoch: 14 \tTraining Loss: 0.875385 \tTest Loss: 0.982481\n",
      "Epoch: 15 \tTraining Loss: 0.874550 \tTest Loss: 0.986569\n",
      "Epoch: 16 \tTraining Loss: 0.872302 \tTest Loss: 0.987308\n",
      "Epoch: 17 \tTraining Loss: 0.864809 \tTest Loss: 0.988502\n",
      "Epoch: 18 \tTraining Loss: 0.861976 \tTest Loss: 0.986268\n",
      "Epoch: 19 \tTraining Loss: 0.865171 \tTest Loss: 0.985334\n",
      "Epoch: 20 \tTraining Loss: 0.868546 \tTest Loss: 0.989591\n",
      "Epoch: 21 \tTraining Loss: 0.861184 \tTest Loss: 0.984373\n",
      "Epoch: 22 \tTraining Loss: 0.860220 \tTest Loss: 0.987790\n",
      "Epoch: 23 \tTraining Loss: 0.864486 \tTest Loss: 0.984953\n",
      "Epoch: 24 \tTraining Loss: 0.858903 \tTest Loss: 0.987123\n",
      "Epoch: 25 \tTraining Loss: 0.860788 \tTest Loss: 0.986555\n",
      "Epoch: 26 \tTraining Loss: 0.861608 \tTest Loss: 0.983194\n",
      "Epoch: 27 \tTraining Loss: 0.860977 \tTest Loss: 0.986259\n",
      "Epoch: 28 \tTraining Loss: 0.869476 \tTest Loss: 0.991425\n",
      "Epoch: 29 \tTraining Loss: 0.864037 \tTest Loss: 0.991043\n",
      "Epoch: 30 \tTraining Loss: 0.858675 \tTest Loss: 0.985619\n",
      "Epoch: 31 \tTraining Loss: 0.862134 \tTest Loss: 0.983905\n",
      "Epoch: 32 \tTraining Loss: 0.859642 \tTest Loss: 0.986360\n",
      "Epoch: 33 \tTraining Loss: 0.862738 \tTest Loss: 0.985660\n",
      "Epoch: 34 \tTraining Loss: 0.861813 \tTest Loss: 0.984545\n",
      "Epoch: 35 \tTraining Loss: 0.861526 \tTest Loss: 0.984128\n",
      "Epoch: 36 \tTraining Loss: 0.866856 \tTest Loss: 0.983820\n",
      "Epoch: 37 \tTraining Loss: 0.858776 \tTest Loss: 0.985865\n",
      "Epoch: 38 \tTraining Loss: 0.855649 \tTest Loss: 0.984455\n",
      "Epoch: 39 \tTraining Loss: 0.856829 \tTest Loss: 0.981518\n",
      "Epoch: 40 \tTraining Loss: 0.854879 \tTest Loss: 0.985944\n",
      "Epoch: 41 \tTraining Loss: 0.859743 \tTest Loss: 0.984331\n",
      "Epoch: 42 \tTraining Loss: 0.864908 \tTest Loss: 0.983340\n",
      "Epoch: 43 \tTraining Loss: 0.861775 \tTest Loss: 0.984374\n",
      "Epoch: 44 \tTraining Loss: 0.870991 \tTest Loss: 0.984691\n",
      "Epoch: 45 \tTraining Loss: 0.860119 \tTest Loss: 0.987041\n",
      "Epoch: 46 \tTraining Loss: 0.860542 \tTest Loss: 0.989699\n",
      "Epoch: 47 \tTraining Loss: 0.860122 \tTest Loss: 0.984116\n",
      "Epoch: 48 \tTraining Loss: 0.863050 \tTest Loss: 0.985601\n",
      "Epoch: 49 \tTraining Loss: 0.866424 \tTest Loss: 0.990249\n",
      "Epoch: 50 \tTraining Loss: 0.875755 \tTest Loss: 0.991025\n",
      "Epoch: 51 \tTraining Loss: 0.886241 \tTest Loss: 0.991978\n",
      "Epoch: 52 \tTraining Loss: 0.871918 \tTest Loss: 0.990205\n",
      "Epoch: 53 \tTraining Loss: 0.874637 \tTest Loss: 0.992236\n",
      "Epoch: 54 \tTraining Loss: 0.872426 \tTest Loss: 1.000720\n",
      "Epoch: 55 \tTraining Loss: 0.887706 \tTest Loss: 1.001729\n",
      "Epoch: 56 \tTraining Loss: 0.878036 \tTest Loss: 0.995069\n",
      "Epoch: 57 \tTraining Loss: 0.886312 \tTest Loss: 0.993221\n",
      "Epoch: 58 \tTraining Loss: 0.881831 \tTest Loss: 0.998236\n",
      "Epoch: 59 \tTraining Loss: 0.886218 \tTest Loss: 0.991543\n",
      "Epoch: 60 \tTraining Loss: 0.888538 \tTest Loss: 0.993542\n",
      "Epoch: 61 \tTraining Loss: 0.890826 \tTest Loss: 0.994292\n",
      "Epoch: 62 \tTraining Loss: 0.890777 \tTest Loss: 0.993898\n",
      "Epoch: 63 \tTraining Loss: 0.885428 \tTest Loss: 0.987652\n",
      "Epoch: 64 \tTraining Loss: 0.885875 \tTest Loss: 0.990550\n",
      "Epoch: 65 \tTraining Loss: 0.886857 \tTest Loss: 0.989803\n",
      "Epoch: 66 \tTraining Loss: 0.887868 \tTest Loss: 0.991777\n",
      "Epoch: 67 \tTraining Loss: 0.888951 \tTest Loss: 0.997322\n",
      "Epoch: 68 \tTraining Loss: 0.888860 \tTest Loss: 0.992075\n",
      "Epoch: 69 \tTraining Loss: 0.890507 \tTest Loss: 0.988733\n",
      "Epoch: 70 \tTraining Loss: 0.892721 \tTest Loss: 0.990675\n",
      "Epoch: 71 \tTraining Loss: 0.894878 \tTest Loss: 0.991604\n",
      "Epoch: 72 \tTraining Loss: 0.894431 \tTest Loss: 0.992534\n",
      "Epoch: 73 \tTraining Loss: 0.893885 \tTest Loss: 0.991292\n",
      "Epoch: 74 \tTraining Loss: 0.889771 \tTest Loss: 0.991973\n",
      "Epoch: 75 \tTraining Loss: 0.894038 \tTest Loss: 0.990154\n",
      "Epoch: 76 \tTraining Loss: 0.898646 \tTest Loss: 0.985118\n",
      "Epoch: 77 \tTraining Loss: 0.901278 \tTest Loss: 0.988293\n",
      "Epoch: 78 \tTraining Loss: 0.898147 \tTest Loss: 0.993740\n",
      "Epoch: 79 \tTraining Loss: 0.898727 \tTest Loss: 0.993907\n",
      "Epoch: 80 \tTraining Loss: 0.907851 \tTest Loss: 0.996737\n",
      "Epoch: 81 \tTraining Loss: 0.905964 \tTest Loss: 0.996047\n",
      "Epoch: 82 \tTraining Loss: 0.904862 \tTest Loss: 0.996441\n",
      "Epoch: 83 \tTraining Loss: 0.903176 \tTest Loss: 1.000034\n",
      "Epoch: 84 \tTraining Loss: 0.901566 \tTest Loss: 0.993457\n",
      "Epoch: 85 \tTraining Loss: 0.901089 \tTest Loss: 0.992381\n",
      "Epoch: 86 \tTraining Loss: 0.896552 \tTest Loss: 0.990057\n",
      "Epoch: 87 \tTraining Loss: 0.902366 \tTest Loss: 0.991845\n",
      "Epoch: 88 \tTraining Loss: 0.899213 \tTest Loss: 0.992677\n",
      "Epoch: 89 \tTraining Loss: 0.900496 \tTest Loss: 0.997308\n",
      "Epoch: 90 \tTraining Loss: 0.897071 \tTest Loss: 0.995824\n",
      "Epoch: 91 \tTraining Loss: 0.898931 \tTest Loss: 0.993532\n",
      "Epoch: 92 \tTraining Loss: 0.906630 \tTest Loss: 0.995948\n",
      "Epoch: 93 \tTraining Loss: 0.914332 \tTest Loss: 1.005979\n",
      "Epoch: 94 \tTraining Loss: 0.907061 \tTest Loss: 0.993987\n",
      "Epoch: 95 \tTraining Loss: 0.901320 \tTest Loss: 0.991552\n",
      "Epoch: 96 \tTraining Loss: 0.907478 \tTest Loss: 0.991125\n",
      "Epoch: 97 \tTraining Loss: 0.923453 \tTest Loss: 0.996903\n",
      "Epoch: 98 \tTraining Loss: 0.907103 \tTest Loss: 0.989741\n",
      "Epoch: 99 \tTraining Loss: 0.902915 \tTest Loss: 0.991113\n",
      "Epoch: 100 \tTraining Loss: 0.904620 \tTest Loss: 0.991050\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size= 15\n",
    "n_classes = 3\n",
    "input_size = 3000\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_label_0 = [x-1 for x in train_label_concat]\n",
    "test_label_0 = [x-1 for x in test_label_concat]\n",
    "\n",
    "\n",
    "x_train = torch.Tensor(train_data_concat)\n",
    "y_train = torch.Tensor(train_label_0).type(torch.LongTensor)\n",
    "x_cv = torch.Tensor(test_data_concat)\n",
    "y_cv = torch.Tensor(test_label_0).type(torch.LongTensor)\n",
    "\n",
    "# Create Torch datasets\n",
    "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test = torch.utils.data.TensorDataset(x_cv, y_cv)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(concatenated_model.parameters(),lr=0.005,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "test_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    concatenated_model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = concatenated_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    concatenated_model.eval() # prep model for evaluation\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = concatenated_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if test_loss <= test_loss_min:\n",
    "        print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        test_loss_min,\n",
    "        test_loss))\n",
    "        torch.save(concatenated_model.state_dict(), 'concatenated_model.pt')\n",
    "        test_loss_min = test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "741ca8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 = 0.5539093719354284 , 0.5538333333333333 , 0.5520550539586373\n",
      "Class 2 = 0.5922244957614733 , 0.5131712259371833 , 0.5498710815578776\n",
      "Class 3 = 0.4859067099027189 , 0.48217821782178216 , 0.48403528388619704\n",
      "average = 0.5541379884363744 , 0.5538003195059803 , 0.5524491839003034\n",
      "Accuracy =  0.5538333333333333\n"
     ]
    }
   ],
   "source": [
    "y_pred_test=torch.max(concatenated_model(x_cv).data,1).indices\n",
    "findAccuracy(y_cv.tolist(), y_pred_test.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1119c",
   "metadata": {},
   "source": [
    "### 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5af2cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_vec_20(reviews):\n",
    "    sequences =  []\n",
    "    for review in reviews:\n",
    "        text_tokens = word_tokenize(review)        \n",
    "        vectors=[]\n",
    "        for i,word in enumerate(text_tokens):\n",
    "            if word in wv and word in vocab:\n",
    "                vectors.append(wv[word].reshape((1,300)))\n",
    "        \n",
    "        if(len(vectors)>=20):\n",
    "            padded_vectors = vectors[:20]\n",
    "        else:\n",
    "            num_missing_vectors = 20-len(vectors)\n",
    "            padded_vectors = vectors + [np.zeros((1,300))]*num_missing_vectors\n",
    "            \n",
    "        sequences.append(padded_vectors)\n",
    "            \n",
    "    return sequences\n",
    "            \n",
    "input_feature_rnn = generate_input_vec_20(test_60['review_body'])\n",
    "input_labels_rnn = test_60['star_rating']\n",
    "\n",
    "train_data_rnn = input_feature_rnn[:int(0.8 * len(input_feature))]\n",
    "train_label_rnn = input_labels_rnn[:int(0.8 * len(input_labels))]\n",
    "test_data_rnn = input_feature_rnn[int(0.8 * len(input_feature)):]\n",
    "test_label_rnn =  input_labels_rnn[int(0.8 * len(input_labels)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ee0edc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_label_0 = [x-1 for x in train_label_rnn]\n",
    "test_label_0 = [x-1 for x in test_label_rnn]\n",
    "\n",
    "x_train_rnn = torch.Tensor(train_data_rnn)\n",
    "y_train_rnn = torch.Tensor(train_label_0).type(torch.LongTensor)\n",
    "x_cv_rnn = torch.Tensor(test_data_rnn)\n",
    "y_cv_rnn = torch.Tensor(test_label_0).type(torch.LongTensor)\n",
    "\n",
    "# Create Torch datasets\n",
    "train_rnn = torch.utils.data.TensorDataset(x_train_rnn, y_train_rnn)\n",
    "test_rnn = torch.utils.data.TensorDataset(x_cv_rnn, y_cv_rnn)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader_rnn = torch.utils.data.DataLoader(train_rnn, batch_size=1,shuffle=False)\n",
    "test_loader_rnn = torch.utils.data.DataLoader(test_rnn, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "20dc1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "def categoryFromOutput(output):\n",
    "    return torch.argmax(output).item()\n",
    "    \n",
    "rnn_model = RNN(300,20,3)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.0005, betas=(0.9,0.999),eps=1e-08,weight_decay=5e-5)  \n",
    "\n",
    "def train(line_tensor, category):\n",
    "    hidden = rnn_model.initHidden()\n",
    "    rnn_model.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.shape[1]):\n",
    "        inp_tensor = line_tensor[0][i]\n",
    "        output, hidden = rnn_model(inp_tensor, hidden)        \n",
    "    \n",
    "    loss = criterion(output, category)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 5)\n",
    "    optimizer.step()\n",
    "\n",
    "    for p in rnn_model.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-0.005)\n",
    "    \n",
    "    return output, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "9439bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0% (0m 0s) 1.0557 / 2 \n",
      "10000 20% (1m 2s) 0.8146 / 1 \n",
      "20000 41% (2m 5s) 1.0425 / 0  (1)\n",
      "30000 62% (3m 7s) 0.2258 / 2 \n",
      "40000 83% (4m 8s) 0.4790 / 1 \n",
      "0 0% (4m 58s) 0.5283 / 2 \n",
      "10000 20% (5m 56s) 0.7473 / 1 \n",
      "20000 41% (6m 56s) 1.0882 / 0  (1)\n",
      "30000 62% (7m 58s) 0.1358 / 2 \n",
      "40000 83% (8m 54s) 0.5135 / 1 \n",
      "0 0% (9m 38s) 0.4788 / 2 \n",
      "10000 20% (10m 34s) 0.7297 / 1 \n",
      "20000 41% (11m 28s) 0.9665 / 0  (1)\n",
      "30000 62% (12m 22s) 0.1105 / 2 \n",
      "40000 83% (13m 19s) 0.4415 / 1 \n",
      "0 0% (14m 2s) 0.4144 / 2 \n",
      "10000 20% (14m 56s) 0.7353 / 1 \n",
      "20000 41% (15m 50s) 0.9865 / 0  (1)\n",
      "30000 62% (16m 48s) 0.0959 / 2 \n",
      "40000 83% (17m 45s) 0.4923 / 1 \n",
      "0 0% (18m 29s) 0.4149 / 2 \n",
      "10000 20% (19m 26s) 0.6446 / 1 \n",
      "20000 41% (20m 22s) 0.5556 / 1 \n",
      "30000 62% (21m 19s) 0.0837 / 2 \n",
      "40000 83% (22m 16s) 0.9246 / 0  (1)\n"
     ]
    }
   ],
   "source": [
    "# Keep track of losses for plotting\n",
    "print_every = 10000\n",
    "plot_every = 1000\n",
    "\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epcoh in range(5):\n",
    "    print_counter = 0\n",
    "    for i, (review, rating) in enumerate(train_loader_rnn):\n",
    "        output, loss = train(review, rating)\n",
    "        current_loss += loss\n",
    "        # Print iter number, loss, name and guess\n",
    "        if print_counter % print_every == 0:\n",
    "            guess = categoryFromOutput(output)\n",
    "            correct = '' if guess == rating.item() else ' (%s)' % rating.item()\n",
    "            print('%d %d%% (%s) %.4f / %s %s' % (print_counter, (print_counter / 48000) * 100, timeSince(start), loss, guess, correct))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if print_counter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "            \n",
    "        print_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "5acc131f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 = 0.5383320973765393 , 0.5526666666666666 , 0.5293428755454231\n",
      "Class 2 = 0.5725244072524407 , 0.6238601823708206 , 0.597090909090909\n",
      "Class 3 = 0.4747974797479748 , 0.26113861386138615 , 0.33695305014372406\n",
      "average = 0.5409409373078372 , 0.553371173727906 , 0.5337687134063948\n",
      "Accuracy =  0.5526666666666666\n"
     ]
    }
   ],
   "source": [
    "def evaluate(line_tensor):\n",
    "    hidden = rnn_model.initHidden()\n",
    "    for i in range(line_tensor.shape[1]):\n",
    "        inp_tensor = line_tensor[0][i]\n",
    "        output, hidden = rnn_model(inp_tensor, hidden)\n",
    "    return output\n",
    "\n",
    "y_pred = []\n",
    "y_test = []\n",
    "rnn_model.eval()\n",
    "for i, (review, rating) in enumerate(test_loader_rnn):\n",
    "    guess = evaluate(review)\n",
    "    guess = categoryFromOutput(guess)\n",
    "    y_test.append(rating.item())\n",
    "    y_pred.append(guess)\n",
    "\n",
    "findAccuracy( y_test, y_pred)\n",
    "\n",
    "\n",
    "# def evaluate2(dataloader):\n",
    "#     rnn_model.eval()\n",
    "#     total_acc, total_count = 0, 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for idx, (review, rating) in enumerate(dataloader):\n",
    "#             predicted_label = rnn_model(review, rnn_model.initHidden())\n",
    "#             print(predicted_label)\n",
    "# #             total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "# #             total_count += label.size(0)\n",
    "# #     return total_acc/total_count\n",
    "\n",
    "# evaluate2(test_loader_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf016e",
   "metadata": {},
   "source": [
    "### Gated RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "7332038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_gru = torch.utils.data.DataLoader(train_rnn, batch_size=20,shuffle=False)\n",
    "test_loader_gru = torch.utils.data.DataLoader(test_rnn, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "af9b2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers=1, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim)\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0)\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "gru_model = GRUNet(300,20,3)\n",
    "\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=0.0005, betas=(0.9,0.999),eps=1e-08,weight_decay=5e-5)  \n",
    "\n",
    "def train_gru(line_tensor, category):\n",
    "    gru_model.zero_grad()\n",
    "    line_tensor = line_tensor.reshape(-1, 20, 300)\n",
    "    output = gru_model(line_tensor)        \n",
    "    \n",
    "    loss = criterion(output, category)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(gru_model.parameters(), 5)\n",
    "    optimizer.step()\n",
    "\n",
    "    for p in gru_model.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-0.005)\n",
    "    \n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "41ad15d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of losses for plotting\n",
    "print_every = 10000\n",
    "plot_every = 1000\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "\n",
    "for epcoh in range(20):\n",
    "    print_counter = 0\n",
    "    for i, (review, rating) in enumerate(train_loader_gru):\n",
    "        print_counter += 1\n",
    "        output, loss = train_gru(review, rating)\n",
    "        \n",
    "        current_loss += loss\n",
    "        if print_counter%5000 == 0:\n",
    "            print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, print_counter, len(train_loader_rnn), current_loss/print_counter))\n",
    "            current_loss = 0\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "1c5c67d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 = 0.6542906998156957 , 0.6528333333333334 , 0.6533128128539667\n",
      "Class 2 = 0.683971099812684 , 0.6474164133738601 , 0.6651919323357188\n",
      "Class 3 = 0.5673488814850072 , 0.5900990099009901 , 0.5785003639893229\n",
      "average = 0.6542878583826414 , 0.6529114256219507 , 0.6533910369781196\n",
      "Accuracy =  0.6528333333333334\n"
     ]
    }
   ],
   "source": [
    "def evaluateGRU(line_tensor):\n",
    "    line_tensor = line_tensor.reshape(-1, 20, 300)\n",
    "    output = gru_model(line_tensor)  \n",
    "    return output\n",
    "\n",
    "y_pred = []\n",
    "y_test = []\n",
    "rnn_model.eval()\n",
    "for i, (review, rating) in enumerate(test_loader_rnn):\n",
    "    guess = evaluateGRU(review)\n",
    "    guess = categoryFromOutput(guess)\n",
    "    y_test.append(rating.item())\n",
    "    y_pred.append(guess)\n",
    "\n",
    "findAccuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2d2f2",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "293eab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_lstm = torch.utils.data.DataLoader(train_rnn, batch_size=20,shuffle=False)\n",
    "test_loader_lstm = torch.utils.data.DataLoader(test_rnn, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "965ae8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 1.0333196684718131\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.4442861386835575\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 1.2093686475157737\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.4200806087851524\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 1.163862690538168\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.40443937595188617\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 1.121079615175724\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.3905376136898994\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 1.0901985815763473\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.3814742360562086\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 1.0690436508059502\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.3746084191054106\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 1.0519699056446552\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.3688708261102438\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 1.037396578669548\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.3638345798403025\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 1.0245144122838974\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.3592896606475115\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 1.0128319492936135\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.35511218319833276\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 1.0019963116645814\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.35123081125319006\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 0.9917836290895938\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.3475996650606394\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 0.9820828582048416\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.3441888230443001\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 0.9728594801127911\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.34097162617743015\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 0.9640175167620182\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.33792930978536606\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 0.9554503385424614\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.3350325872451067\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 0.9471323455870152\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.3322274000644684\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 0.9390380810797214\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.32950284746289255\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 0.9312066808640956\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.3268719286620617\n",
      "Epoch 99......Step: 1000/48000....... Average Loss for Epoch: 0.9235175625085831\n",
      "Epoch 99......Step: 2000/48000....... Average Loss for Epoch: 0.3243387123644352\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layer_dim=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "\n",
    "lstm_model = LSTMModel(300, 20, 3)\n",
    "optimizer_lstm = torch.optim.Adam(lstm_model.parameters(), lr=0.0005, betas=(0.9,0.999),eps=1e-08,weight_decay=5e-5)  \n",
    "\n",
    "\n",
    "def train_lstm(line_tensor, category):\n",
    "    lstm_model.zero_grad()\n",
    "    line_tensor = line_tensor.reshape(-1, 20, 300)\n",
    "    output = lstm_model(line_tensor)        \n",
    "    \n",
    "    loss = criterion(output, category)\n",
    "    optimizer_lstm.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), 5)\n",
    "    optimizer_lstm.step()\n",
    "\n",
    "    for p in lstm_model.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-0.005)\n",
    "    \n",
    "    return output, loss.item()\n",
    "\n",
    "current_loss = 0\n",
    "for epcoh in range(20):\n",
    "    print_counter = 0\n",
    "    for i, (review, rating) in enumerate(train_loader_lstm):\n",
    "        print_counter += 1\n",
    "        output, loss = train_lstm(review, rating)        \n",
    "        current_loss += loss\n",
    "        if print_counter%1000 == 0:\n",
    "            print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, print_counter, len(train_loader_rnn), current_loss/print_counter))\n",
    "            current_loss = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "45ad8d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test reviews: 65.13333333333334 %\n"
     ]
    }
   ],
   "source": [
    "def evaluateLSTM(line_tensor):\n",
    "    line_tensor = line_tensor.reshape(-1, 20, 300)\n",
    "    output = lstm_model(line_tensor)  \n",
    "    return output\n",
    "\n",
    "y_pred = []\n",
    "y_test = []\n",
    "# lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for i, (review, rating) in enumerate(test_loader_lstm):\n",
    "        guess = evaluateLSTM(review)\n",
    "        _, predicted = torch.max(guess.data, 1)\n",
    "        n_samples += rating.size(0)\n",
    "        n_correct += (predicted == rating).sum().item()\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the test reviews: {acc} %')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "31572944",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[]\n",
    "for i, (review, rating) in enumerate(test_loader_lstm):\n",
    "    if i >= 2:\n",
    "        break\n",
    "    l= rating.tolist()\n",
    "    for k in l:\n",
    "        t.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "e6d38c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97848dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
