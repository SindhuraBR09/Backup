{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "42437534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "nltk.download('wordnet',quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt',quiet=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "102bbc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz\"\n",
    "data = pd.read_csv(reviews, sep='\\t', on_bad_lines='skip', low_memory=False)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c267c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "review_data = data[['star_rating', 'review_body']]\n",
    "review_data.dropna(inplace=True)\n",
    "review_data['star_rating'] = review_data['star_rating'].astype('int32')\n",
    "\n",
    "for index, row in review_data.iterrows():\n",
    "    if row['star_rating'] in {1,2}:\n",
    "        review_data.loc[index, 'star_rating'] = 1\n",
    "    elif row['star_rating'] in {3}:\n",
    "        review_data.loc[index, 'star_rating'] = 2\n",
    "    elif row['star_rating'] in {4,5}:\n",
    "        review_data.loc[index, 'star_rating'] = 3\n",
    "        \n",
    "# bal_data = review_data.groupby('star_rating').apply(lambda group: group.sample(20000)).reset_index(drop = True)\n",
    "bal_data = review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "293f7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_data.to_csv('data_full', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525418a",
   "metadata": {},
   "source": [
    "### 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3675666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_data = pd.read_csv(\"data_full\", sep='\\t', names = ['star_rating', 'review_body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32843d78",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8078d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "bal_data = review_data\n",
    "def remove_stopword(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw =  \" \".join([word for word in text_tokens if word not in stop_words])\n",
    "    return tokens_without_sw\n",
    "\n",
    "def lemmetize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text_tokens = word_tokenize(text)\n",
    "    lemmatized_string = \" \".join([lemmatizer.lemmatize(words) for words in text_tokens])\n",
    "\n",
    "    return lemmatized_string\n",
    "\n",
    "def clean_data(data):\n",
    "    #covert to lower\n",
    "    data= data.lower()\n",
    "    #remove html and url\n",
    "    data = re.sub(r'http\\S+', '', data)\n",
    "    urls = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    data = re.sub(urls, '', data)\n",
    "    #remove non-alphabetical chars\n",
    "    non_alpha = re.compile('[^a-zA-Z]')\n",
    "    data = non_alpha.sub(' ', data)\n",
    "    #remove extra spaces\n",
    "    data = re.sub(' +', ' ', data)\n",
    "    #perform contractions\n",
    "    data = contractions.fix(data)  \n",
    "    \n",
    "    return data\n",
    "\n",
    "bal_data['review_body'] = bal_data.apply(lambda row : clean_data(row['review_body']), axis = 1)\n",
    "bal_data['review_body'] = bal_data.apply(lambda row : remove_stopword(row['review_body']), axis = 1)\n",
    "bal_data['review_body'] = bal_data.apply(lambda row : lemmetize(row['review_body']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb34464",
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_data.to_csv('data_cleaned', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f149c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_data = pd.read_csv(\"data_cleaned\", sep='\\t', names = ['star_rating', 'review_body'])\n",
    "bal_data.dropna(inplace=True)\n",
    "review_samples = []\n",
    "\n",
    "for rating in [1,2,3]:\n",
    "    rating_df = bal_data[ bal_data['star_rating'] == rating ]\n",
    "\n",
    "    reviews = rating_df['review_body'].tolist()\n",
    "\n",
    "    tfIdfVect = TfidfVectorizer(use_idf=True)\n",
    "    # getting tf-idf vectors\n",
    "    vect_review = tfIdfVect.fit_transform(rating_df['review_body'])\n",
    "\n",
    "    # take sum of tf-idf vector values -> indicates \"total\" importance of the review in the pool of other reviews \n",
    "    # having the same rating\n",
    "    vals = list(np.squeeze(np.asarray(np.sum(vect_review, axis = 1).astype(np.float32))))\n",
    "    \n",
    "    vals = [vals[i]/len(reviews[i]) for i in range(len(vals))]\n",
    "    \n",
    "    # sort by tf-idf sum value and take top 20000 reviews for each class\n",
    "    rating_df = pd.DataFrame(list(zip(reviews, vals, rating_df['star_rating'].tolist())), \n",
    "                                    columns=['review_body','tfidf_score', 'star_rating'])\n",
    "\n",
    "    rating_df = rating_df.sort_values(by=['tfidf_score'], ascending=False)\n",
    "    review_samples.append(rating_df.head(20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ad8d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_data = pd.concat(review_samples)\n",
    "\n",
    "bal_data.drop(columns=['tfidf_score'], inplace=True)\n",
    "\n",
    "bal_data = shuffle(bal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "715934a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_data.to_csv('balanced_data', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ebedf1",
   "metadata": {},
   "source": [
    "### 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab9dc86",
   "metadata": {},
   "source": [
    "#### (a)  Google news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7092579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4783c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generating word embedding\n",
    "words = [\"cat\", \"dog\", \"man\", \"woman\"]\n",
    "vectors = [wv[word] for word in words]\n",
    "\n",
    "# print(\"Words and their Word embedding\")\n",
    "# print(\"------------------------------\")\n",
    "# for word, vector in zip(words, vectors):\n",
    "#     print(f\"Word: {word}  Vector: {vector}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31735994",
   "metadata": {},
   "source": [
    "To learn the semantic similarity, I am considering the below three examples:\n",
    "* Finding words similar to \"happy\" using its vector\n",
    "* Performing \"happy - smile + cry = sad\" using word vector\n",
    "* Finding cosine similarity between words \"fight\" and \"battle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "731c6578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 similar words to happy:\n",
      "\thappy: 1.0000\n",
      "\tglad: 0.7409\n",
      "\tpleased: 0.6632\n",
      "\tecstatic: 0.6627\n",
      "\toverjoyed: 0.6599\n",
      "\n",
      "Cosine similarity between love and like: 0.36713877\n",
      "\n",
      "Most similar words to 'big-large+small':\n",
      "\tbig: 0.7968\n",
      "\tsmall: 0.6329\n",
      "\tbigger: 0.5330\n",
      "\thuge: 0.4986\n",
      "\tlittle_bitty: 0.4698\n",
      "\tbiggest: 0.4613\n",
      "\ttiny: 0.4609\n",
      "\tSmall: 0.4602\n",
      "\tnice: 0.4599\n",
      "\tabig: 0.4512\n"
     ]
    }
   ],
   "source": [
    "print(\"3 similar words to happy:\")\n",
    "res = wv.similar_by_vector(wv[\"happy\"], topn=5)\n",
    "for word, score in res:\n",
    "    print(\"\\t{}: {:.4f}\".format(word, score))\n",
    "print()\n",
    "\n",
    "cosine = wv.similarity(\"love\", \"like\")\n",
    "print(\"Cosine similarity between love and like:\", cosine)\n",
    "print()\n",
    "\n",
    "\n",
    "big = wv['big']\n",
    "large = wv['large']\n",
    "small = wv['small']\n",
    "result = big-large+small\n",
    "similarity = wv.similar_by_vector(result)\n",
    "print(\"Most similar words to 'big-large+small':\")\n",
    "for word, score in similarity:\n",
    "    print(\"\\t{}: {:.4f}\".format(word, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e3242",
   "metadata": {},
   "source": [
    "#### (b) Training Word2Vec on Review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ba4634e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10820456, 15495095)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "r_data =bal_data['review_body']\n",
    "sentences = []\n",
    "for s in r_data:\n",
    "    sentences.append(list(s.split(\" \")))\n",
    "    \n",
    "my_model = Word2Vec(sentences, vector_size=300, window=13, min_count=9)\n",
    "my_model.train(sentences, total_examples=my_model.corpus_count, epochs=my_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "912af4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 similar words to happy:\n",
      "\tmoved: 0.2399\n",
      "\thaving: 0.2251\n",
      "\towners: 0.2117\n",
      "\tbeing: 0.1966\n",
      "\tdropping: 0.1966\n",
      "\n",
      "Cosine similarity between love and like: 0.3219804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"3 similar words to happy:\")\n",
    "res = my_model.wv.similar_by_vector(wv[\"happy\"], topn=5)\n",
    "for word, score in res:\n",
    "    print(\"\\t{}: {:.4f}\".format(word, score))\n",
    "print()\n",
    "\n",
    "cosine = my_model.wv.similarity(\"love\", \"like\")\n",
    "print(\"Cosine similarity between love and like:\", cosine)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f218f6",
   "metadata": {},
   "source": [
    "### 3. Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "428c78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAccuracy(y_test, y_pred):\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, digits=4)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    for i in range(3):\n",
    "        print(f\"Class {i+1} = {df['precision'][i-1]} , {df['recall'][i-1]} , {df['f1-score'][i-1]}\")\n",
    "    print(f\"average = {df['precision'].mean()} , {df['recall'].mean()} , {df['f1-score'].mean()}\")\n",
    "    print(\"Accuracy = \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d152e20",
   "metadata": {},
   "source": [
    "#### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "adb6b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "reviews = bal_data['review_body']\n",
    "review_vectors = []\n",
    "for review in reviews:\n",
    "    words = review.split()\n",
    "    review_vector = np.zeros(300)\n",
    "    for word in words:\n",
    "        if word in wv:\n",
    "            review_vector += wv[word]\n",
    "    review_vector /= len(words)\n",
    "    review_vectors.append(review_vector)\n",
    "review_vectors = np.array(review_vectors)\n",
    "\n",
    "ratings = bal_data['star_rating']\n",
    "train_data = review_vectors[:int(0.8 * len(review_vectors))]\n",
    "train_labels = ratings[:int(0.8 * len(review_vectors))]\n",
    "test_data = review_vectors[int(0.8 * len(review_vectors)):]\n",
    "test_labels =  ratings[int(0.8 * len(review_vectors)):]\n",
    "train_data=np.nan_to_num(train_data, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "test_data=np.nan_to_num(test_data,copy=True, nan=0.0, posinf=None, neginf=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4f810b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 = 0.6291793242757951 , 0.6139166666666667 , 0.6170392014900615\n",
      "Class 2 = 0.6963562753036437 , 0.599601593625498 , 0.6443671394166444\n",
      "Class 3 = 0.5105409705648369 , 0.6412690482138396 , 0.5684863248809655\n",
      "average = 0.6266499972244798 , 0.6139110441353318 , 0.6165250133803749\n",
      "Accuracy =  0.6139166666666667\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(max_iter=9000)\n",
    "perceptron.fit(train_data, train_labels)\n",
    "perceptron_predictions = perceptron.predict(test_data)\n",
    "findAccuracy(test_labels, perceptron_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "feb6a154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 = 0.7115481186378178 , 0.7074166666666667 , 0.6977263581801021\n",
      "Class 2 = 0.7105393369619001 , 0.7269045811187042 , 0.7186288002001752\n",
      "Class 3 = 0.732420429311621 , 0.4978616352201258 , 0.5927811891568069\n",
      "average = 0.7109654877052217 , 0.7064157974889214 , 0.6989149343114422\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(C=0.1)\n",
    "svm.fit(train_data, train_labels)\n",
    "svm_predictions = svm.predict(test_data)\n",
    "findAccuracy(test_labels, svm_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff1466d",
   "metadata": {},
   "source": [
    "### 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82bddfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward_MLP(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Feedforward_MLP(torch.nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(Feedforward_MLP, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, 50)\n",
    "            self.dropout = torch.nn.Dropout(0.2)\n",
    "            \n",
    "#             self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(50, 10)\n",
    "            self.fc3 = torch.nn.Linear(10, 3)\n",
    "#             self.sigmoid = torch.nn.Sigmoid()\n",
    "            self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "        \n",
    "mlp_model = Feedforward_MLP(300)\n",
    "print(mlp_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8319e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_only = pd.read_csv(\"data_cleaned\", sep='\\t', names = ['star_rating', 'review_body'])\n",
    "test_only.dropna(inplace=True)\n",
    "test_60 = test_only.groupby('star_rating').apply(lambda group: group.sample(20000)).reset_index(drop = True)\n",
    "test_60 = shuffle(test_60)\n",
    "vocab = defaultdict(int)\n",
    "for l in test_60['review_body']:\n",
    "    for w in l.split():\n",
    "        vocab[w] += 1 \n",
    "        \n",
    "for key in list(vocab.keys()):\n",
    "    if vocab[key] < 3:\n",
    "        del vocab[key]\n",
    "\n",
    "test_reviews = test_60['review_body']\n",
    "rvs = []\n",
    "for review in test_reviews:\n",
    "    words = review.split()\n",
    "    rv = np.zeros(300)\n",
    "    for word in words:\n",
    "        if word in wv and word in vocab:\n",
    "            rv += wv[word]\n",
    "    rv /= len(words)\n",
    "    rvs.append(rv)\n",
    "rvs = np.array(rvs)\n",
    "\n",
    "test_ratings = test_60['star_rating']\n",
    "tr_d = rvs[:int(0.8 * len(rvs))]\n",
    "tr_l = test_ratings[:int(0.8 * len(rvs))]\n",
    "ts_d = rvs[int(0.8 * len(rvs)):]\n",
    "ts_l =  test_ratings[int(0.8 * len(rvs)):]\n",
    "tr_d=np.nan_to_num(tr_d, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "ts_d=np.nan_to_num(ts_d,copy=True, nan=0.0, posinf=None, neginf=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aa18d62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.825234 \tTest Loss: 1.462071\n",
      "Test loss decreased (inf --> 1.462071).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.830298 \tTest Loss: 1.334126\n",
      "Test loss decreased (1.462071 --> 1.334126).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.821584 \tTest Loss: 1.374535\n",
      "Epoch: 4 \tTraining Loss: 0.822289 \tTest Loss: 1.213783\n",
      "Test loss decreased (1.334126 --> 1.213783).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.820358 \tTest Loss: 1.153229\n",
      "Test loss decreased (1.213783 --> 1.153229).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.819128 \tTest Loss: 1.040245\n",
      "Test loss decreased (1.153229 --> 1.040245).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.816706 \tTest Loss: 1.105544\n",
      "Epoch: 8 \tTraining Loss: 0.813357 \tTest Loss: 1.209472\n",
      "Epoch: 9 \tTraining Loss: 0.816107 \tTest Loss: 1.178686\n",
      "Epoch: 10 \tTraining Loss: 0.814381 \tTest Loss: 1.147119\n",
      "Epoch: 11 \tTraining Loss: 0.816718 \tTest Loss: 1.093866\n",
      "Epoch: 12 \tTraining Loss: 0.818749 \tTest Loss: 1.155726\n",
      "Epoch: 13 \tTraining Loss: 0.819823 \tTest Loss: 1.114640\n",
      "Epoch: 14 \tTraining Loss: 0.819742 \tTest Loss: 1.131737\n",
      "Epoch: 15 \tTraining Loss: 0.820929 \tTest Loss: 1.122296\n",
      "Epoch: 16 \tTraining Loss: 0.819424 \tTest Loss: 1.118051\n",
      "Epoch: 17 \tTraining Loss: 0.820941 \tTest Loss: 1.105814\n",
      "Epoch: 18 \tTraining Loss: 0.817937 \tTest Loss: 1.131387\n",
      "Epoch: 19 \tTraining Loss: 0.822670 \tTest Loss: 1.218237\n",
      "Epoch: 20 \tTraining Loss: 0.814193 \tTest Loss: 1.185323\n",
      "Epoch: 21 \tTraining Loss: 0.817503 \tTest Loss: 1.090307\n",
      "Epoch: 22 \tTraining Loss: 0.821162 \tTest Loss: 1.184783\n",
      "Epoch: 23 \tTraining Loss: 0.820763 \tTest Loss: 1.102368\n",
      "Epoch: 24 \tTraining Loss: 0.818096 \tTest Loss: 1.226094\n",
      "Epoch: 25 \tTraining Loss: 0.813939 \tTest Loss: 1.175149\n",
      "Epoch: 26 \tTraining Loss: 0.817331 \tTest Loss: 1.242101\n",
      "Epoch: 27 \tTraining Loss: 0.815690 \tTest Loss: 1.166069\n",
      "Epoch: 28 \tTraining Loss: 0.812468 \tTest Loss: 1.313767\n",
      "Epoch: 29 \tTraining Loss: 0.814700 \tTest Loss: 1.287794\n",
      "Epoch: 30 \tTraining Loss: 0.813666 \tTest Loss: 1.161961\n",
      "Epoch: 31 \tTraining Loss: 0.820813 \tTest Loss: 1.185528\n",
      "Epoch: 32 \tTraining Loss: 0.815341 \tTest Loss: 1.223590\n",
      "Epoch: 33 \tTraining Loss: 0.822185 \tTest Loss: 1.275085\n",
      "Epoch: 34 \tTraining Loss: 0.816398 \tTest Loss: 1.338450\n",
      "Epoch: 35 \tTraining Loss: 0.818806 \tTest Loss: 1.308126\n",
      "Epoch: 36 \tTraining Loss: 0.818942 \tTest Loss: 1.244946\n",
      "Epoch: 37 \tTraining Loss: 0.816408 \tTest Loss: 1.185464\n",
      "Epoch: 38 \tTraining Loss: 0.820976 \tTest Loss: 1.205923\n",
      "Epoch: 39 \tTraining Loss: 0.818424 \tTest Loss: 1.267139\n",
      "Epoch: 40 \tTraining Loss: 0.815637 \tTest Loss: 1.450175\n",
      "Epoch: 41 \tTraining Loss: 0.813752 \tTest Loss: 1.219278\n",
      "Epoch: 42 \tTraining Loss: 0.813341 \tTest Loss: 1.261481\n",
      "Epoch: 43 \tTraining Loss: 0.819583 \tTest Loss: 1.334807\n",
      "Epoch: 44 \tTraining Loss: 0.817676 \tTest Loss: 1.345037\n",
      "Epoch: 45 \tTraining Loss: 0.820751 \tTest Loss: 1.258298\n",
      "Epoch: 46 \tTraining Loss: 0.813978 \tTest Loss: 1.284431\n",
      "Epoch: 47 \tTraining Loss: 0.821499 \tTest Loss: 1.286955\n",
      "Epoch: 48 \tTraining Loss: 0.820401 \tTest Loss: 1.254191\n",
      "Epoch: 49 \tTraining Loss: 0.831449 \tTest Loss: 1.202462\n",
      "Epoch: 50 \tTraining Loss: 0.811655 \tTest Loss: 1.201349\n",
      "Epoch: 51 \tTraining Loss: 0.816900 \tTest Loss: 1.266860\n",
      "Epoch: 52 \tTraining Loss: 0.818967 \tTest Loss: 1.308711\n",
      "Epoch: 53 \tTraining Loss: 0.817105 \tTest Loss: 1.195534\n",
      "Epoch: 54 \tTraining Loss: 0.814356 \tTest Loss: 1.300610\n",
      "Epoch: 55 \tTraining Loss: 0.816941 \tTest Loss: 1.204701\n",
      "Epoch: 56 \tTraining Loss: 0.812930 \tTest Loss: 1.309965\n",
      "Epoch: 57 \tTraining Loss: 0.821661 \tTest Loss: 1.213934\n",
      "Epoch: 58 \tTraining Loss: 0.813155 \tTest Loss: 1.251876\n",
      "Epoch: 59 \tTraining Loss: 0.820022 \tTest Loss: 1.227225\n",
      "Epoch: 60 \tTraining Loss: 0.814215 \tTest Loss: 1.278534\n",
      "Epoch: 61 \tTraining Loss: 0.815494 \tTest Loss: 1.298429\n",
      "Epoch: 62 \tTraining Loss: 0.814142 \tTest Loss: 1.177060\n",
      "Epoch: 63 \tTraining Loss: 0.808733 \tTest Loss: 1.211813\n",
      "Epoch: 64 \tTraining Loss: 0.816795 \tTest Loss: 1.245723\n",
      "Epoch: 65 \tTraining Loss: 0.817353 \tTest Loss: 1.183227\n",
      "Epoch: 66 \tTraining Loss: 0.820927 \tTest Loss: 1.312463\n",
      "Epoch: 67 \tTraining Loss: 0.818609 \tTest Loss: 1.406628\n",
      "Epoch: 68 \tTraining Loss: 0.815532 \tTest Loss: 1.340242\n",
      "Epoch: 69 \tTraining Loss: 0.813700 \tTest Loss: 1.463863\n",
      "Epoch: 70 \tTraining Loss: 0.815208 \tTest Loss: 1.415139\n",
      "Epoch: 71 \tTraining Loss: 0.817420 \tTest Loss: 1.328132\n",
      "Epoch: 72 \tTraining Loss: 0.806303 \tTest Loss: 1.379822\n",
      "Epoch: 73 \tTraining Loss: 0.812344 \tTest Loss: 1.385707\n",
      "Epoch: 74 \tTraining Loss: 0.812922 \tTest Loss: 1.396515\n",
      "Epoch: 75 \tTraining Loss: 0.814033 \tTest Loss: 1.446421\n",
      "Epoch: 76 \tTraining Loss: 0.817200 \tTest Loss: 1.402757\n",
      "Epoch: 77 \tTraining Loss: 0.818524 \tTest Loss: 1.387233\n",
      "Epoch: 78 \tTraining Loss: 0.812696 \tTest Loss: 1.313026\n",
      "Epoch: 79 \tTraining Loss: 0.818854 \tTest Loss: 1.367596\n",
      "Epoch: 80 \tTraining Loss: 0.811374 \tTest Loss: 1.273305\n",
      "Epoch: 81 \tTraining Loss: 0.817471 \tTest Loss: 1.324132\n",
      "Epoch: 82 \tTraining Loss: 0.818448 \tTest Loss: 1.391223\n",
      "Epoch: 83 \tTraining Loss: 0.816854 \tTest Loss: 1.288296\n",
      "Epoch: 84 \tTraining Loss: 0.816018 \tTest Loss: 1.311742\n",
      "Epoch: 85 \tTraining Loss: 0.813834 \tTest Loss: 1.450492\n",
      "Epoch: 86 \tTraining Loss: 0.814121 \tTest Loss: 1.340977\n",
      "Epoch: 87 \tTraining Loss: 0.811904 \tTest Loss: 1.297805\n",
      "Epoch: 88 \tTraining Loss: 0.820402 \tTest Loss: 1.240226\n",
      "Epoch: 89 \tTraining Loss: 0.812399 \tTest Loss: 1.257751\n",
      "Epoch: 90 \tTraining Loss: 0.811986 \tTest Loss: 1.218605\n",
      "Epoch: 91 \tTraining Loss: 0.815713 \tTest Loss: 1.347570\n",
      "Epoch: 92 \tTraining Loss: 0.816024 \tTest Loss: 1.360696\n",
      "Epoch: 93 \tTraining Loss: 0.813410 \tTest Loss: 1.289533\n",
      "Epoch: 94 \tTraining Loss: 0.818714 \tTest Loss: 1.293643\n",
      "Epoch: 95 \tTraining Loss: 0.822663 \tTest Loss: 1.392459\n",
      "Epoch: 96 \tTraining Loss: 0.814709 \tTest Loss: 1.307855\n",
      "Epoch: 97 \tTraining Loss: 0.817133 \tTest Loss: 1.313517\n",
      "Epoch: 98 \tTraining Loss: 0.810213 \tTest Loss: 1.311612\n",
      "Epoch: 99 \tTraining Loss: 0.811550 \tTest Loss: 1.497221\n",
      "Epoch: 100 \tTraining Loss: 0.819064 \tTest Loss: 1.167414\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size= 15\n",
    "n_classes = 3\n",
    "input_size = 300\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_label_0 = [x-1 for x in tr_l]\n",
    "test_label_0 = [x-1 for x in ts_l]\n",
    "\n",
    "\n",
    "x_train = torch.Tensor(tr_d)\n",
    "y_train = torch.Tensor(train_label_0).type(torch.LongTensor)\n",
    "x_cv = torch.Tensor(ts_d)\n",
    "y_cv = torch.Tensor(test_label_0).type(torch.LongTensor)\n",
    "\n",
    "# Create Torch datasets\n",
    "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test = torch.utils.data.TensorDataset(x_cv, y_cv)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,  sampler = sampler,shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(),lr=0.005)\n",
    "test_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    mlp_model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = mlp_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    mlp_model.eval() # prep model for evaluation\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = mlp_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if test_loss <= test_loss_min:\n",
    "        print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        test_loss_min,\n",
    "        test_loss))\n",
    "        torch.save(mlp_model.state_dict(), 'mlp_model.pt')\n",
    "        test_loss_min = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "00be2a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 = 0.6226853177888603 , 0.6250833333333333 , 0.6173578675732784\n",
      "Class 2 = 0.5997079682937004 , 0.7282168186423505 , 0.657744223289865\n",
      "Class 3 = 0.5912954777286638 , 0.43044554455445544 , 0.49820942558372733\n",
      "average = 0.6230522167545917 , 0.6257507976706318 , 0.6189733992899935\n",
      "Accuracy =  0.6250833333333333\n"
     ]
    }
   ],
   "source": [
    "y_pred_test=torch.max(mlp_model(x_cv).data,1).indices\n",
    "findAccuracy(y_cv.tolist(), y_pred_test.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21b06b",
   "metadata": {},
   "source": [
    "#### (b) Input feature as concatenated vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b12553ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenate_MLP(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Concatenate_MLP(torch.nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(Concatenate_MLP, self).__init__()\n",
    "            self.input_size = input_size            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, 50)\n",
    "            self.dropout = torch.nn.Dropout(0.2)\n",
    "            self.fc2 = torch.nn.Linear(50, 10)\n",
    "            self.fc3 = torch.nn.Linear(10, 3)\n",
    "            self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            x= self.softmax(x)\n",
    "            return x\n",
    "        \n",
    "concatenated_model = Concatenate_MLP(3000)\n",
    "print(concatenated_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "02e69dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_feature_vector(reviews):\n",
    "    review_words = [review.split() for review in reviews]    \n",
    "    vector_size = wv.vector_size    \n",
    "    num_reviews = len(reviews)\n",
    "    input_features = np.zeros((num_reviews, 10*vector_size))\n",
    "    \n",
    "    for i, words in enumerate(review_words):\n",
    "        vectors = []\n",
    "        for j in range(min(len(words), 10)):\n",
    "            word = words[j]\n",
    "            if word in wv:\n",
    "                vectors.append(wv[word])\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        num_missing_vectors = max(0, 10 - len(vectors))\n",
    "        padded_vectors = vectors + [np.zeros(vector_size)]*num_missing_vectors        \n",
    "        feature_vector = np.concatenate(padded_vectors)        \n",
    "        input_features[i,:] = feature_vector\n",
    "    \n",
    "    return input_features\n",
    "\n",
    "input_feature = generate_input_feature_vector(test_60['review_body'])\n",
    "input_labels = test_60['star_rating']\n",
    "\n",
    "train_data_concat = input_feature[:int(0.8 * len(input_feature))]\n",
    "train_label_concat = input_labels[:int(0.8 * len(input_labels))]\n",
    "test_data_concat = input_feature[int(0.8 * len(input_feature)):]\n",
    "test_label_concat =  input_labels[int(0.8 * len(input_labels)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f2e1454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.981438 \tTest Loss: 0.969641\n",
      "Test loss decreased (inf --> 0.969641).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.943694 \tTest Loss: 0.966551\n",
      "Test loss decreased (0.969641 --> 0.966551).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.933607 \tTest Loss: 0.969582\n",
      "Epoch: 4 \tTraining Loss: 0.919472 \tTest Loss: 0.978174\n",
      "Epoch: 5 \tTraining Loss: 0.909904 \tTest Loss: 0.975660\n",
      "Epoch: 6 \tTraining Loss: 0.902423 \tTest Loss: 0.975728\n",
      "Epoch: 7 \tTraining Loss: 0.894346 \tTest Loss: 0.980218\n",
      "Epoch: 8 \tTraining Loss: 0.894622 \tTest Loss: 0.981943\n",
      "Epoch: 9 \tTraining Loss: 0.891956 \tTest Loss: 0.983494\n",
      "Epoch: 10 \tTraining Loss: 0.884797 \tTest Loss: 0.980240\n",
      "Epoch: 11 \tTraining Loss: 0.880723 \tTest Loss: 0.984680\n",
      "Epoch: 12 \tTraining Loss: 0.880338 \tTest Loss: 0.984363\n",
      "Epoch: 13 \tTraining Loss: 0.876303 \tTest Loss: 0.984842\n",
      "Epoch: 14 \tTraining Loss: 0.875385 \tTest Loss: 0.982481\n",
      "Epoch: 15 \tTraining Loss: 0.874550 \tTest Loss: 0.986569\n",
      "Epoch: 16 \tTraining Loss: 0.872302 \tTest Loss: 0.987308\n",
      "Epoch: 17 \tTraining Loss: 0.864809 \tTest Loss: 0.988502\n",
      "Epoch: 18 \tTraining Loss: 0.861976 \tTest Loss: 0.986268\n",
      "Epoch: 19 \tTraining Loss: 0.865171 \tTest Loss: 0.985334\n",
      "Epoch: 20 \tTraining Loss: 0.868546 \tTest Loss: 0.989591\n",
      "Epoch: 21 \tTraining Loss: 0.861184 \tTest Loss: 0.984373\n",
      "Epoch: 22 \tTraining Loss: 0.860220 \tTest Loss: 0.987790\n",
      "Epoch: 23 \tTraining Loss: 0.864486 \tTest Loss: 0.984953\n",
      "Epoch: 24 \tTraining Loss: 0.858903 \tTest Loss: 0.987123\n",
      "Epoch: 25 \tTraining Loss: 0.860788 \tTest Loss: 0.986555\n",
      "Epoch: 26 \tTraining Loss: 0.861608 \tTest Loss: 0.983194\n",
      "Epoch: 27 \tTraining Loss: 0.860977 \tTest Loss: 0.986259\n",
      "Epoch: 28 \tTraining Loss: 0.869476 \tTest Loss: 0.991425\n",
      "Epoch: 29 \tTraining Loss: 0.864037 \tTest Loss: 0.991043\n",
      "Epoch: 30 \tTraining Loss: 0.858675 \tTest Loss: 0.985619\n",
      "Epoch: 31 \tTraining Loss: 0.862134 \tTest Loss: 0.983905\n",
      "Epoch: 32 \tTraining Loss: 0.859642 \tTest Loss: 0.986360\n",
      "Epoch: 33 \tTraining Loss: 0.862738 \tTest Loss: 0.985660\n",
      "Epoch: 34 \tTraining Loss: 0.861813 \tTest Loss: 0.984545\n",
      "Epoch: 35 \tTraining Loss: 0.861526 \tTest Loss: 0.984128\n",
      "Epoch: 36 \tTraining Loss: 0.866856 \tTest Loss: 0.983820\n",
      "Epoch: 37 \tTraining Loss: 0.858776 \tTest Loss: 0.985865\n",
      "Epoch: 38 \tTraining Loss: 0.855649 \tTest Loss: 0.984455\n",
      "Epoch: 39 \tTraining Loss: 0.856829 \tTest Loss: 0.981518\n",
      "Epoch: 40 \tTraining Loss: 0.854879 \tTest Loss: 0.985944\n",
      "Epoch: 41 \tTraining Loss: 0.859743 \tTest Loss: 0.984331\n",
      "Epoch: 42 \tTraining Loss: 0.864908 \tTest Loss: 0.983340\n",
      "Epoch: 43 \tTraining Loss: 0.861775 \tTest Loss: 0.984374\n",
      "Epoch: 44 \tTraining Loss: 0.870991 \tTest Loss: 0.984691\n",
      "Epoch: 45 \tTraining Loss: 0.860119 \tTest Loss: 0.987041\n",
      "Epoch: 46 \tTraining Loss: 0.860542 \tTest Loss: 0.989699\n",
      "Epoch: 47 \tTraining Loss: 0.860122 \tTest Loss: 0.984116\n",
      "Epoch: 48 \tTraining Loss: 0.863050 \tTest Loss: 0.985601\n",
      "Epoch: 49 \tTraining Loss: 0.866424 \tTest Loss: 0.990249\n",
      "Epoch: 50 \tTraining Loss: 0.875755 \tTest Loss: 0.991025\n",
      "Epoch: 51 \tTraining Loss: 0.886241 \tTest Loss: 0.991978\n",
      "Epoch: 52 \tTraining Loss: 0.871918 \tTest Loss: 0.990205\n",
      "Epoch: 53 \tTraining Loss: 0.874637 \tTest Loss: 0.992236\n",
      "Epoch: 54 \tTraining Loss: 0.872426 \tTest Loss: 1.000720\n",
      "Epoch: 55 \tTraining Loss: 0.887706 \tTest Loss: 1.001729\n",
      "Epoch: 56 \tTraining Loss: 0.878036 \tTest Loss: 0.995069\n",
      "Epoch: 57 \tTraining Loss: 0.886312 \tTest Loss: 0.993221\n",
      "Epoch: 58 \tTraining Loss: 0.881831 \tTest Loss: 0.998236\n",
      "Epoch: 59 \tTraining Loss: 0.886218 \tTest Loss: 0.991543\n",
      "Epoch: 60 \tTraining Loss: 0.888538 \tTest Loss: 0.993542\n",
      "Epoch: 61 \tTraining Loss: 0.890826 \tTest Loss: 0.994292\n",
      "Epoch: 62 \tTraining Loss: 0.890777 \tTest Loss: 0.993898\n",
      "Epoch: 63 \tTraining Loss: 0.885428 \tTest Loss: 0.987652\n",
      "Epoch: 64 \tTraining Loss: 0.885875 \tTest Loss: 0.990550\n",
      "Epoch: 65 \tTraining Loss: 0.886857 \tTest Loss: 0.989803\n",
      "Epoch: 66 \tTraining Loss: 0.887868 \tTest Loss: 0.991777\n",
      "Epoch: 67 \tTraining Loss: 0.888951 \tTest Loss: 0.997322\n",
      "Epoch: 68 \tTraining Loss: 0.888860 \tTest Loss: 0.992075\n",
      "Epoch: 69 \tTraining Loss: 0.890507 \tTest Loss: 0.988733\n",
      "Epoch: 70 \tTraining Loss: 0.892721 \tTest Loss: 0.990675\n",
      "Epoch: 71 \tTraining Loss: 0.894878 \tTest Loss: 0.991604\n",
      "Epoch: 72 \tTraining Loss: 0.894431 \tTest Loss: 0.992534\n",
      "Epoch: 73 \tTraining Loss: 0.893885 \tTest Loss: 0.991292\n",
      "Epoch: 74 \tTraining Loss: 0.889771 \tTest Loss: 0.991973\n",
      "Epoch: 75 \tTraining Loss: 0.894038 \tTest Loss: 0.990154\n",
      "Epoch: 76 \tTraining Loss: 0.898646 \tTest Loss: 0.985118\n",
      "Epoch: 77 \tTraining Loss: 0.901278 \tTest Loss: 0.988293\n",
      "Epoch: 78 \tTraining Loss: 0.898147 \tTest Loss: 0.993740\n",
      "Epoch: 79 \tTraining Loss: 0.898727 \tTest Loss: 0.993907\n",
      "Epoch: 80 \tTraining Loss: 0.907851 \tTest Loss: 0.996737\n",
      "Epoch: 81 \tTraining Loss: 0.905964 \tTest Loss: 0.996047\n",
      "Epoch: 82 \tTraining Loss: 0.904862 \tTest Loss: 0.996441\n",
      "Epoch: 83 \tTraining Loss: 0.903176 \tTest Loss: 1.000034\n",
      "Epoch: 84 \tTraining Loss: 0.901566 \tTest Loss: 0.993457\n",
      "Epoch: 85 \tTraining Loss: 0.901089 \tTest Loss: 0.992381\n",
      "Epoch: 86 \tTraining Loss: 0.896552 \tTest Loss: 0.990057\n",
      "Epoch: 87 \tTraining Loss: 0.902366 \tTest Loss: 0.991845\n",
      "Epoch: 88 \tTraining Loss: 0.899213 \tTest Loss: 0.992677\n",
      "Epoch: 89 \tTraining Loss: 0.900496 \tTest Loss: 0.997308\n",
      "Epoch: 90 \tTraining Loss: 0.897071 \tTest Loss: 0.995824\n",
      "Epoch: 91 \tTraining Loss: 0.898931 \tTest Loss: 0.993532\n",
      "Epoch: 92 \tTraining Loss: 0.906630 \tTest Loss: 0.995948\n",
      "Epoch: 93 \tTraining Loss: 0.914332 \tTest Loss: 1.005979\n",
      "Epoch: 94 \tTraining Loss: 0.907061 \tTest Loss: 0.993987\n",
      "Epoch: 95 \tTraining Loss: 0.901320 \tTest Loss: 0.991552\n",
      "Epoch: 96 \tTraining Loss: 0.907478 \tTest Loss: 0.991125\n",
      "Epoch: 97 \tTraining Loss: 0.923453 \tTest Loss: 0.996903\n",
      "Epoch: 98 \tTraining Loss: 0.907103 \tTest Loss: 0.989741\n",
      "Epoch: 99 \tTraining Loss: 0.902915 \tTest Loss: 0.991113\n",
      "Epoch: 100 \tTraining Loss: 0.904620 \tTest Loss: 0.991050\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size= 15\n",
    "n_classes = 3\n",
    "input_size = 3000\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_label_0 = [x-1 for x in train_label_concat]\n",
    "test_label_0 = [x-1 for x in test_label_concat]\n",
    "\n",
    "\n",
    "x_train = torch.Tensor(train_data_concat)\n",
    "y_train = torch.Tensor(train_label_0).type(torch.LongTensor)\n",
    "x_cv = torch.Tensor(test_data_concat)\n",
    "y_cv = torch.Tensor(test_label_0).type(torch.LongTensor)\n",
    "\n",
    "# Create Torch datasets\n",
    "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test = torch.utils.data.TensorDataset(x_cv, y_cv)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,  sampler = sampler,shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(concatenated_model.parameters(),lr=0.005,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "test_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    concatenated_model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = concatenated_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    concatenated_model.eval() # prep model for evaluation\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = concatenated_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if test_loss <= test_loss_min:\n",
    "        print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        test_loss_min,\n",
    "        test_loss))\n",
    "        torch.save(concatenated_model.state_dict(), 'concatenated_model.pt')\n",
    "        test_loss_min = test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ecafd6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 = 0.5539093719354284 , 0.5538333333333333 , 0.5520550539586373\n",
      "Class 2 = 0.5922244957614733 , 0.5131712259371833 , 0.5498710815578776\n",
      "Class 3 = 0.4859067099027189 , 0.48217821782178216 , 0.48403528388619704\n",
      "average = 0.5541379884363744 , 0.5538003195059803 , 0.5524491839003034\n",
      "Accuracy =  0.5538333333333333\n"
     ]
    }
   ],
   "source": [
    "y_pred_test=torch.max(concatenated_model(x_cv).data,1).indices\n",
    "findAccuracy(y_cv.tolist(), y_pred_test.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7dcca",
   "metadata": {},
   "source": [
    "### 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c6ce7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def generate_input_vec_20(reviews):\n",
    "    sequences = []\n",
    "    for review in reviews:\n",
    "        text_tokens = word_tokenize(review)        \n",
    "        vectors=[]\n",
    "        for word in text_tokens:\n",
    "            if word in wv and word in vocab:\n",
    "                vectors.append(torch.tensor(wv[word], dtype=torch.float32))\n",
    "        \n",
    "        if(len(vectors)>=20):\n",
    "            sequences.append(vectors[:20])\n",
    "        else:\n",
    "            missing_nums = 20-len(vectors)\n",
    "            for i in range(missing_nums):\n",
    "                 vectors.append(np.zeros((1,300)))\n",
    "            sequences.append(vectors[:])\n",
    "    return sequences\n",
    "            \n",
    "input_feature = generate_input_vec_20(test_60['review_body'])\n",
    "input_labels = test_60['star_rating']\n",
    "\n",
    "train_data_rnn = input_feature[:int(0.8 * len(input_feature))]\n",
    "train_label_rnn = input_labels[:int(0.8 * len(input_labels))]\n",
    "test_data_rnn = input_feature[int(0.8 * len(input_feature)):]\n",
    "test_label_rnn =  input_labels[int(0.8 * len(input_labels)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "    \n",
    "rnn_model = RNN(300,20,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c9712056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.04052734,  0.0625    , -0.01745605,  0.07861328,  0.03271484,\n",
       "        -0.01263428,  0.00964355,  0.12353516, -0.02148438,  0.15234375,\n",
       "        -0.05834961, -0.10644531,  0.02124023,  0.13574219, -0.13183594,\n",
       "         0.17675781,  0.27148438,  0.13769531, -0.17382812, -0.14160156,\n",
       "        -0.03076172,  0.19628906, -0.03295898,  0.125     ,  0.25390625,\n",
       "         0.12695312, -0.15234375,  0.03198242,  0.01135254, -0.01361084,\n",
       "        -0.12890625,  0.01019287,  0.23925781, -0.08447266,  0.140625  ,\n",
       "         0.13085938, -0.04516602,  0.06494141,  0.02539062,  0.05615234,\n",
       "         0.24609375, -0.20507812,  0.23632812, -0.00860596, -0.02294922,\n",
       "         0.05078125,  0.10644531, -0.03564453,  0.08740234, -0.05712891,\n",
       "         0.08496094,  0.23535156, -0.10107422, -0.03564453, -0.04736328,\n",
       "         0.04736328, -0.14550781, -0.10986328,  0.14746094, -0.23242188,\n",
       "        -0.07275391,  0.19628906, -0.37890625, -0.07226562,  0.04833984,\n",
       "         0.11914062,  0.06103516, -0.12109375, -0.27929688,  0.05200195,\n",
       "         0.04907227, -0.02709961,  0.1328125 ,  0.03369141, -0.32226562,\n",
       "         0.04223633, -0.08789062,  0.15429688,  0.09472656,  0.10351562,\n",
       "        -0.02856445,  0.00128174, -0.00427246,  0.24609375, -0.05957031,\n",
       "        -0.16894531, -0.09619141,  0.16796875,  0.0133667 ,  0.04882812,\n",
       "         0.08349609,  0.06347656, -0.00872803, -0.08642578, -0.03857422,\n",
       "        -0.08251953,  0.15722656,  0.22753906, -0.00762939, -0.19921875,\n",
       "        -0.06347656,  0.12792969, -0.06347656, -0.03027344,  0.0456543 ,\n",
       "         0.06298828, -0.02526855, -0.06787109, -0.01141357, -0.13574219,\n",
       "         0.02978516,  0.10400391, -0.15917969, -0.08447266,  0.29882812,\n",
       "        -0.12597656,  0.11425781, -0.08105469, -0.09082031, -0.07910156,\n",
       "        -0.11181641, -0.09619141,  0.02770996,  0.14257812, -0.26757812,\n",
       "        -0.09375   ,  0.03979492, -0.17871094, -0.02819824,  0.01464844,\n",
       "        -0.31640625, -0.24511719, -0.08935547,  0.09716797, -0.00964355,\n",
       "        -0.14746094,  0.15234375,  0.21582031,  0.05981445,  0.23828125,\n",
       "        -0.05151367,  0.14941406,  0.13574219, -0.03222656, -0.265625  ,\n",
       "        -0.11181641, -0.23046875, -0.140625  ,  0.25585938, -0.15429688,\n",
       "         0.1796875 ,  0.15527344, -0.21582031,  0.36328125, -0.1015625 ,\n",
       "         0.04980469,  0.07177734, -0.14550781, -0.03198242,  0.00952148,\n",
       "        -0.12109375,  0.12109375,  0.09765625,  0.07763672,  0.3203125 ,\n",
       "        -0.22265625, -0.08447266, -0.10742188,  0.11279297, -0.13867188,\n",
       "        -0.21875   ,  0.0145874 ,  0.13378906, -0.00921631,  0.00921631,\n",
       "         0.16894531,  0.16894531, -0.078125  , -0.00665283,  0.03735352,\n",
       "        -0.10888672, -0.25390625,  0.01452637, -0.09716797, -0.19628906,\n",
       "        -0.01782227, -0.28125   , -0.02050781, -0.02905273, -0.09375   ,\n",
       "        -0.17675781,  0.21484375, -0.05224609, -0.11572266, -0.01977539,\n",
       "        -0.10839844, -0.01342773, -0.15332031, -0.140625  , -0.11816406,\n",
       "         0.09228516,  0.109375  ,  0.05761719, -0.03466797,  0.03564453,\n",
       "        -0.12011719, -0.14257812, -0.00072479, -0.06689453,  0.11914062,\n",
       "        -0.10449219,  0.07861328, -0.12792969,  0.09570312, -0.00817871,\n",
       "         0.07128906,  0.20703125, -0.03149414,  0.09570312,  0.17285156,\n",
       "        -0.07958984, -0.02429199, -0.07519531, -0.07568359,  0.09521484,\n",
       "        -0.06494141, -0.00689697, -0.09033203,  0.03100586,  0.19921875,\n",
       "        -0.10644531, -0.11474609,  0.18652344, -0.05078125,  0.0859375 ,\n",
       "         0.00128937, -0.18847656, -0.20019531, -0.02832031,  0.11328125,\n",
       "         0.25976562,  0.22070312,  0.04101562,  0.00171661,  0.07568359,\n",
       "        -0.01196289,  0.0177002 , -0.05883789, -0.25976562, -0.234375  ,\n",
       "        -0.04956055,  0.25976562,  0.15332031,  0.15136719,  0.08300781,\n",
       "        -0.15527344,  0.04931641,  0.07519531, -0.05078125, -0.1328125 ,\n",
       "        -0.13574219,  0.04199219, -0.14257812,  0.02099609,  0.07861328,\n",
       "         0.01611328,  0.01623535, -0.21582031,  0.01599121, -0.04882812,\n",
       "        -0.02404785,  0.13476562,  0.08496094, -0.01196289,  0.10009766,\n",
       "        -0.13867188,  0.08056641, -0.22070312, -0.12011719,  0.18945312,\n",
       "         0.05444336, -0.05053711,  0.00147247,  0.14160156, -0.06494141,\n",
       "        -0.05566406, -0.09033203, -0.0267334 , -0.10498047,  0.02416992,\n",
       "         0.01422119,  0.1875    , -0.16503906,  0.01538086, -0.04174805,\n",
       "         0.05444336, -0.01184082, -0.15625   ,  0.00193024, -0.06982422],\n",
       "       dtype=float32),\n",
       " array([ 0.06298828,  0.12451172,  0.11328125,  0.07324219,  0.03881836,\n",
       "         0.07910156,  0.05078125,  0.171875  ,  0.09619141,  0.22070312,\n",
       "        -0.04150391, -0.09277344, -0.02209473,  0.14746094, -0.21582031,\n",
       "         0.15234375,  0.19238281, -0.05078125, -0.11181641, -0.3203125 ,\n",
       "         0.00506592,  0.15332031, -0.02563477, -0.0234375 ,  0.36328125,\n",
       "         0.20605469,  0.04760742, -0.02624512,  0.09033203,  0.00457764,\n",
       "        -0.15332031,  0.06591797,  0.3515625 , -0.12451172,  0.03015137,\n",
       "         0.16210938,  0.00242615, -0.02282715,  0.02978516,  0.00531006,\n",
       "         0.25976562, -0.22460938,  0.29492188, -0.18066406,  0.07910156,\n",
       "         0.02282715,  0.12109375, -0.17382812, -0.03735352, -0.06933594,\n",
       "        -0.21972656,  0.1875    , -0.03320312, -0.06225586, -0.04492188,\n",
       "         0.11621094, -0.23339844, -0.11669922,  0.09814453, -0.11962891,\n",
       "         0.13964844,  0.28710938, -0.26953125, -0.05493164,  0.03112793,\n",
       "        -0.05029297,  0.1328125 , -0.01831055, -0.37695312, -0.06298828,\n",
       "         0.12597656, -0.07910156, -0.04467773,  0.10400391, -0.41210938,\n",
       "         0.22851562, -0.07080078,  0.24511719,  0.06494141,  0.12890625,\n",
       "        -0.05102539, -0.00308228, -0.17871094,  0.25976562, -0.13476562,\n",
       "        -0.21289062, -0.234375  ,  0.21777344, -0.07910156,  0.01977539,\n",
       "         0.19726562,  0.17285156,  0.03613281, -0.17578125, -0.02966309,\n",
       "        -0.00939941,  0.25976562,  0.12353516,  0.19140625, -0.03930664,\n",
       "         0.15917969,  0.05664062, -0.01977539, -0.14941406,  0.12597656,\n",
       "        -0.00350952, -0.05957031, -0.14648438,  0.01660156, -0.35742188,\n",
       "        -0.0300293 ,  0.03149414, -0.0324707 , -0.3203125 ,  0.35351562,\n",
       "        -0.19433594,  0.13964844,  0.07470703, -0.10888672,  0.10107422,\n",
       "        -0.296875  , -0.01348877, -0.14160156,  0.06982422, -0.20703125,\n",
       "        -0.25195312,  0.03955078,  0.04345703,  0.05957031, -0.15429688,\n",
       "        -0.43359375, -0.13671875,  0.00436401,  0.13867188, -0.13867188,\n",
       "        -0.125     ,  0.00118256,  0.08203125, -0.01989746, -0.10449219,\n",
       "         0.04638672,  0.03735352,  0.078125  , -0.00656128, -0.12402344,\n",
       "        -0.3125    , -0.23046875,  0.0065918 ,  0.22949219, -0.21875   ,\n",
       "         0.2421875 , -0.01062012, -0.26367188,  0.3359375 , -0.19140625,\n",
       "         0.02636719, -0.0112915 , -0.20898438,  0.06298828, -0.07763672,\n",
       "        -0.11572266,  0.14648438,  0.10400391, -0.02819824,  0.12109375,\n",
       "        -0.11083984, -0.02893066, -0.171875  ,  0.1953125 , -0.12451172,\n",
       "        -0.19140625, -0.03857422, -0.01507568,  0.05151367, -0.06884766,\n",
       "         0.07177734,  0.25195312, -0.09570312,  0.08251953, -0.0135498 ,\n",
       "         0.07177734, -0.27734375,  0.00350952, -0.11035156, -0.15039062,\n",
       "         0.08642578, -0.27148438,  0.10009766, -0.02746582,  0.07470703,\n",
       "         0.11865234,  0.08740234, -0.03955078,  0.05004883, -0.03735352,\n",
       "         0.03369141, -0.01977539, -0.16210938,  0.00460815, -0.0390625 ,\n",
       "         0.10302734,  0.18066406, -0.01495361, -0.08105469,  0.02905273,\n",
       "        -0.02490234, -0.21875   ,  0.04492188, -0.09472656, -0.07519531,\n",
       "        -0.1640625 , -0.13476562,  0.02111816,  0.10888672, -0.08251953,\n",
       "         0.10644531,  0.04345703, -0.1484375 , -0.02038574,  0.02734375,\n",
       "        -0.11767578, -0.03735352,  0.10400391, -0.11572266,  0.0546875 ,\n",
       "        -0.05664062, -0.11669922,  0.00180817, -0.04736328,  0.13085938,\n",
       "        -0.00089645,  0.01831055,  0.13378906, -0.12060547,  0.13671875,\n",
       "         0.05053711, -0.19238281, -0.24414062,  0.02062988,  0.11035156,\n",
       "         0.42773438,  0.11572266,  0.0480957 , -0.11572266,  0.00787354,\n",
       "        -0.08251953,  0.03808594,  0.06542969, -0.14453125, -0.13769531,\n",
       "         0.02001953, -0.05395508,  0.17675781,  0.06298828, -0.05981445,\n",
       "        -0.25195312,  0.24414062,  0.17382812,  0.09619141, -0.30664062,\n",
       "        -0.21875   ,  0.28710938, -0.00897217,  0.01818848,  0.06445312,\n",
       "         0.01660156, -0.07177734, -0.15625   ,  0.06738281, -0.05371094,\n",
       "         0.08154297,  0.29101562,  0.11523438, -0.02258301,  0.01306152,\n",
       "        -0.10595703,  0.19824219, -0.03393555, -0.05419922,  0.07763672,\n",
       "         0.05859375, -0.07910156,  0.09863281, -0.06054688, -0.09765625,\n",
       "        -0.01269531, -0.12695312, -0.06982422, -0.13574219, -0.10058594,\n",
       "         0.01135254,  0.34179688, -0.09033203,  0.07666016, -0.0324707 ,\n",
       "         0.13378906, -0.15429688, -0.06347656,  0.11474609,  0.03100586],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec= []\n",
    "vec.append(wv['good'])\n",
    "vec.append(wv['bad'])\n",
    "vec[0].reshape((1,300))\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccecda50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
