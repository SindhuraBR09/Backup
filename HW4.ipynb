{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "837b57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import math\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb6cb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "tag2idx={}\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<UNK>'] = 1\n",
    "tag2idx['<PAD>'] = 0\n",
    "\n",
    "def read_data(file_path):\n",
    "    \"\"\"\n",
    "    Reads the data from the file and returns the sentences, words and tags in separate lists.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = f.read().strip().split(\"\\n\\n\")\n",
    "    sentences = []\n",
    "    words = []\n",
    "    tags = []\n",
    "    for sentence in data:\n",
    "        sentence_words = []\n",
    "        sentence_tags = []\n",
    "        for line in sentence.strip().split(\"\\n\"):\n",
    "            line = line.strip().split()\n",
    "            sentence_words.append(line[1])\n",
    "            sentence_tags.append(line[2])\n",
    "            # add words and tags to the dictionaries\n",
    "            if line[1] not in word2idx:\n",
    "                word2idx[line[1]] = len(word2idx)\n",
    "            if line[2] not in tag2idx:\n",
    "                tag2idx[line[2]] = len(tag2idx)\n",
    "        words.append(sentence_words)\n",
    "        tags.append(sentence_tags)\n",
    "        sentences.append(sentence_words)\n",
    "    return sentences, words, tags\n",
    "\n",
    "# read the train and dev data\n",
    "train_sentences, train_words, train_tags = read_data(\"data/train\")\n",
    "dev_sentences, dev_words, dev_tags = read_data(\"data/dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6cc12e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(nn.Module):\n",
    "    def __init__(self, word2idx, tag2idx, pretrained_embeddings,embedding_dim=100, hidden_dim=256, output_dim=128, dropout=0.33):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.tag2idx = tag2idx\n",
    "        self.word_embeddings = pretrained_embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,num_layers=1,batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier = nn.Linear(hidden_dim, len(tag2idx))\n",
    "        \n",
    "    def forward(self, sentence, length):\n",
    "#         embeds = self.word_embeddings(sentence)\n",
    "#         embeds = nn.utils.rnn.pack_padded_sequence(embeds, length, batch_first=True, enforce_sorted=False)\n",
    "#         lstm_out, _ = self.lstm(embeds) ##.view(len(sentence), 1, -1)\n",
    "#         lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "#         lstm_out = self.dropout(lstm_out)\n",
    "#         fc_out = self.fc(lstm_out)\n",
    "#         elu_out = self.elu(fc_out)\n",
    "#         tag_space = self.classifier(elu_out.view(1, -1, hidden_dim*2)) ##\n",
    "# #         tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "#         return tag_space\n",
    "\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        embeds = nn.utils.rnn.pack_padded_sequence(embeds, length, batch_first=True, enforce_sorted=False)\n",
    "        lstm_out, _ = self.lstm(embeds)  # shape: batch_size x sequence_length x (hidden_dim * 2)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        fc_out = self.fc(lstm_out)  # shape: batch_size x sequence_length x hidden_dim\n",
    "        elu_out = self.elu(fc_out)  # shape: batch_size x sequence_length x hidden_dim\n",
    "        tag_space = self.classifier(elu_out)  # shape: batch_size x sequence_length x len(tag2idx)\n",
    "        return tag_space\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, file_path, word_to_idx, label_to_idx):\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.sentences, self.labels = self.read_data(file_path)\n",
    "        self.max_len=113\n",
    "        \n",
    "    def read_data(self, file_path):\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            words = []\n",
    "            tags = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    parts = line.split()\n",
    "                    word = parts[1]\n",
    "                    tag = parts[2]\n",
    "                    words.append(word)\n",
    "                    tags.append(tag)\n",
    "                else:\n",
    "                    if words:\n",
    "                        sentences.append(words)\n",
    "                        labels.append(tags)\n",
    "                        words = []\n",
    "                        tags = []\n",
    "        return sentences, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.sentences[index]\n",
    "        label = self.labels[index]\n",
    "        sentence_idx = [self.word_to_idx.get(word, 0) for word in sentence]\n",
    "        label_idx = [self.label_to_idx[tag] for tag in label]\n",
    "        \n",
    "        pad_len = self.max_len - len(sentence)\n",
    "        sentence_idx += [self.word_to_idx['<PAD>']] * pad_len\n",
    "        label_idx += [self.label_to_idx['<PAD>']] * pad_len\n",
    "        \n",
    "        return torch.LongTensor(sentence_idx), torch.LongTensor(label_idx), len(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "be8df75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    lengths = [item[2] for item in batch]\n",
    "    \n",
    "    # Pad sequences to max_len\n",
    "    max_len = max(lengths)\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=word2idx['<PAD>'])\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=word2idx['<PAD>'])\n",
    "    \n",
    "    return inputs, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0aaae19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset('data/train', word2idx,tag2idx)\n",
    "dev_dataset = NERDataset('data/dev', word2idx, tag2idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True,drop_last=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=20, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "99feb267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 [5, 5, 9, 43, 1, 6, 28, 8, 7, 27, 10, 7, 7, 7, 1, 8, 24, 8, 36, 10]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (860) to match target batch_size (2400).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28244/3343892696.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;31m#     dev_acc = evaluate(model, dev_loader, device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch {epoch + 1}, Train Acc {train_acc}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28244/3343892696.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, data_loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<PAD>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# average the loss over non-padding tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1174\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3024\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3026\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (860) to match target batch_size (2400)."
     ]
    }
   ],
   "source": [
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    num_classes = len(model.tag2idx)\n",
    "    for batch in data_loader:\n",
    "        sentences, tags, lengths = batch\n",
    "        print(len(sentences), len(tags), lengths)        \n",
    "        sentences = sentences.to(device)\n",
    "        tags = tags.to(device)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.int64).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sentences, lengths)\n",
    "        # create a mask to ignore the padding tokens while computing the loss\n",
    "        mask = (sentences != word2idx['<PAD>'])\n",
    "        mask = mask.float()\n",
    "        loss = criterion(output.view(-1, num_classes), tags.view(-1)) * mask.view(-1)\n",
    "        loss = loss.sum() / mask.sum()  # average the loss over non-padding tokens\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(output, dim=1)\n",
    "        total_correct += torch.sum(predictions == tags)\n",
    "        total += tags.numel()\n",
    "        \n",
    "    return total_loss / len(data_loader), total_correct / total\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(X)\n",
    "            predictions = torch.argmax(output, dim=2)\n",
    "            total_correct += torch.sum(predictions == y)\n",
    "            total += y.numel()\n",
    "    return total_correct / total\n",
    "\n",
    "# embedding = nn.Embedding.from_pretrained(torch.from_numpy(embeddings).to(device, dtype=torch.float), padding_idx=0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NERModel(word2idx, tag2idx).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "#     dev_acc = evaluate(model, dev_loader, device)\n",
    "    print(f'Epoch {epoch + 1}, Train Acc {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03621608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14987"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e9baf50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "097a3471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-ORG': 0,\n",
       " 'O': 1,\n",
       " 'B-MISC': 2,\n",
       " 'B-PER': 3,\n",
       " 'I-PER': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-ORG': 6,\n",
       " 'I-MISC': 7,\n",
       " 'I-LOC': 8,\n",
       " '<PAD>': 0}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da91280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = np.zeros((len(word2idx), 100))\n",
    "# with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         parts = line.split()\n",
    "#         word = parts[0]\n",
    "#         if word in word2idx:\n",
    "#             idx = word2idx[word]\n",
    "#             embedding = np.array(parts[1:], dtype=np.float32)\n",
    "#             embeddings[idx] = embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
