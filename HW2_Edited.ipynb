{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "063c969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7440443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data=[]\n",
    "dev_data=[]\n",
    "with open(\"data/train\", 'r') as f:\n",
    "    sentence_lines = f.readlines()    \n",
    "    for line in sentence_lines:\n",
    "        data = line.split()\n",
    "        train_data.append(data)\n",
    "with open(\"data/dev\", 'r') as f:\n",
    "    sentence_lines = f.readlines()\n",
    "    for line in sentence_lines:\n",
    "        data = line.split()\n",
    "        dev_data.append(data)\n",
    "with open(\"data/test\", 'r') as f:\n",
    "    sentence_lines = f.readlines()\n",
    "    for line in sentence_lines:\n",
    "        data = line.split()\n",
    "        test_data.append(data)\n",
    "        \n",
    "train_data.append([])\n",
    "test_data.append([])\n",
    "dev_data.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfe69bf",
   "metadata": {},
   "source": [
    "### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "a8e45253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold considered = 2\n",
      "Total size of vocabulary =  23183\n",
      "Total number of < unk > =  20011\n"
     ]
    }
   ],
   "source": [
    "vocab = defaultdict(int)\n",
    "for l in train_data:\n",
    "    if l:\n",
    "        vocab[l[1]] += 1    \n",
    "\n",
    "for key in list(vocab.keys()):\n",
    "    if vocab[key] < 2:\n",
    "        vocab['< unk >'] += 1\n",
    "        del vocab[key]\n",
    "\n",
    "index=1\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    f.write('< unk >'+'\\t'+'0'+'\\t'+str(vocab['< unk >'])+'\\n')\n",
    "    for w in sorted(vocab, key=vocab.get, reverse=True):\n",
    "        if w != '< unk >':\n",
    "            f.write(w+'\\t'+str(index)+'\\t'+str(vocab[w])+'\\n')\n",
    "            index += 1\n",
    "\n",
    "print(\"Threshold considered = 2\")\n",
    "print(\"Total size of vocabulary = \", len(vocab.keys()))\n",
    "print(\"Total number of < unk > = \", vocab['< unk >'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71b8bdf",
   "metadata": {},
   "source": [
    "#### Compute emission and transion probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "3c7b3387",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add --s-- and --e-- to beggining and end of sentences\n",
    "\n",
    "train_data_seq = []\n",
    "temp=[('--s--', '--s--')]\n",
    "for l in train_data:\n",
    "    if not l:\n",
    "        temp.append(('--e--', '--e--'))\n",
    "        train_data_seq.append(temp[:])\n",
    "        temp.clear()\n",
    "        temp.append(('--s--', '--s--'))\n",
    "    else:\n",
    "        temp.append((l[1], l[2]))\n",
    "            \n",
    "dev_words_seq = []\n",
    "dev_tags_seq = []\n",
    "temp_words=['--s--']\n",
    "temp_tags=['--s--']\n",
    "\n",
    "for l in dev_data:\n",
    "    if not l:\n",
    "        temp_words.append('--e--')\n",
    "        temp_tags.append('--e--')\n",
    "        dev_words_seq.append(temp_words[:])\n",
    "        dev_tags_seq.append(temp_tags[:])\n",
    "        temp_words.clear()\n",
    "        temp_tags.clear()\n",
    "        temp_words.append('--s--')\n",
    "        temp_tags.append('--s--')\n",
    "    else:\n",
    "        temp_words.append(l[1])\n",
    "        temp_tags.append(l[2])\n",
    "            \n",
    "dev_tags_list = [element for innerList in dev_tags_seq for element in innerList]\n",
    "\n",
    "\n",
    "test_data_seq = []\n",
    "temp_test=['--s--']\n",
    "for l in test_data:\n",
    "    if not l:\n",
    "        temp_test.append('--e--')\n",
    "        test_data_seq.append(temp_test[:])\n",
    "        temp_test.clear()\n",
    "        temp_test.append('--s--')\n",
    "    else:\n",
    "        temp_test.append(l[1])            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "7b7fbee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Emission paramters = 30303\n",
      "Total Transition paramters = 1375\n"
     ]
    }
   ],
   "source": [
    "emission_count = defaultdict(lambda: defaultdict(int))\n",
    "emission_prob =defaultdict(lambda: defaultdict(int))\n",
    "transition_count = defaultdict(lambda: defaultdict(int))\n",
    "transition_prob = defaultdict(lambda: defaultdict(int))\n",
    "tag_count = defaultdict(int)\n",
    "tags_of_word = defaultdict(set)\n",
    "\n",
    "vocab['--e--'] = 1\n",
    "vocab['--s--'] = 1\n",
    "for sentence in train_data_seq:\n",
    "    for (w,t) in sentence:\n",
    "        if w not in vocab:\n",
    "            w='< unk >'\n",
    "        emission_count[t][w]+=1\n",
    "        tag_count[t] += 1\n",
    "        tags_of_word[w].add(t)        \n",
    "        \n",
    "for tag in emission_count.keys():\n",
    "    for word in emission_count[tag].keys():\n",
    "        emission_prob[tag][word]=emission_count[tag][word]/tag_count[tag]\n",
    "        \n",
    "for sentence in train_data_seq:\n",
    "    bigram=list(nltk.bigrams(sentence))\n",
    "    for b1,b2 in bigram:\n",
    "        transition_count[b1[1]][b2[1]]+=1\n",
    "\n",
    "for tag in transition_count.keys():\n",
    "    for next_tag in transition_count[tag].keys():\n",
    "        transition_prob[tag][next_tag]=transition_count[tag][next_tag]/tag_count[tag]\n",
    "        \n",
    "transition_paramter_count=0\n",
    "for key in transition_prob.keys():\n",
    "    if (key == '--s--' or key == '--e--'):\n",
    "        continue\n",
    "    transition_paramter_count += len(transition_prob[key].keys())\n",
    "    \n",
    "emission_paramter_count=0\n",
    "for key in emission_prob.keys():\n",
    "    if (key == '--s--' or key == '--e--'):\n",
    "        continue\n",
    "    emission_paramter_count += len(emission_prob[key].keys())\n",
    "    \n",
    "print(f\"Total Emission paramters = {emission_paramter_count}\")\n",
    "print(f\"Total Transition paramters = {transition_paramter_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "52936717",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#converting nested dictionary to dictionary with key as pair\n",
    "def convert_transion():\n",
    "    new_d = {}\n",
    "    for k1 in transition_prob.keys():\n",
    "         if k1 != '--e--':\n",
    "            for k2 in transition_prob[k1].keys():\n",
    "                if k2 != '--e--':\n",
    "                    new_d[str('('+k1+','+k2+')')] = transition_prob[k1][k2]\n",
    "                    \n",
    "    return new_d\n",
    "\n",
    "def convert_emission():\n",
    "    new_d={}\n",
    "    for k1 in emission_prob.keys():\n",
    "         if k1 != '--s--':\n",
    "            for k2 in emission_prob[k1].keys():\n",
    "                if k2 != '--s--':\n",
    "                    new_d[str('('+k1+','+k2+')')] = emission_prob[k1][k2]\n",
    "                    \n",
    "    return new_d\n",
    "  \n",
    "with open('hmm.json', 'w') as fp:\n",
    "    json.dump(convert_transion(), fp,indent=2)\n",
    "    json.dump(convert_emission(), fp,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302a159",
   "metadata": {},
   "source": [
    "### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "3561f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAccuracy(output_sequence, Y):\n",
    "    correct=0\n",
    "    total_predictions=0\n",
    "    for p,t in zip(output_sequence, Y):\n",
    "        if p==t:\n",
    "            correct += 1\n",
    "#     correct += sum(p == t for p, t in zip(output_sequence, Y))\n",
    "    total_predictions = len(output_sequence)\n",
    "    acc=correct / total_predictions\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ba9e578a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Greedy Decoding algorithm = 0.9169385668874543\n"
     ]
    }
   ],
   "source": [
    "def greedy_decoding(word_seq):  \n",
    "    greedy_preds=[]\n",
    "    for sentence in word_seq:\n",
    "        greedy_preds.append('--s--')\n",
    "        for key, word in enumerate(sentence[1:-1]):\n",
    "            tag_max = \".\";\n",
    "            max_prob = 0;\n",
    "            p = [] \n",
    "            for tag in tag_count.keys():\n",
    "                if key == 0:\n",
    "                    transition_p = transition_prob['--s--'][tag]\n",
    "                else:\n",
    "                    transition_p = transition_prob[greedy_preds[-1]][tag]\n",
    "\n",
    "                tag_prob = emission_prob[tag][word] * transition_p  \n",
    "                if(tag_prob > max_prob):\n",
    "                    tag_max = tag\n",
    "                    max_prob = tag_prob  \n",
    "            greedy_preds.append(tag_max)\n",
    "        greedy_preds.append('--e--')\n",
    "        \n",
    "    return greedy_preds\n",
    "\n",
    "dev_preds_g = greedy_decoding(dev_words_seq)\n",
    "test_preds_g = greedy_decoding(test_data_seq)\n",
    "\n",
    "g_acc=findAccuracy(dev_preds_g, dev_tags_list)\n",
    "print(f\"Accuracy of Greedy Decoding algorithm = {g_acc}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "57729965",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[]\n",
    "for pred_tag in test_preds_g:\n",
    "    if pred_tag == '--s--':\n",
    "        continue\n",
    "    elif pred_tag == '--e--':\n",
    "        t.append([])\n",
    "    else:\n",
    "        t.append(pred_tag)\n",
    "\n",
    "with open('greedy.out', 'w') as f:\n",
    "    for index, s in enumerate(test_data[:-1]):\n",
    "        if s:\n",
    "            f.write(s[0]+'\\t'+s[1]+'\\t'+t[index]+'\\n')\n",
    "        else:\n",
    "            f.write('\\n')     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db17196",
   "metadata": {},
   "source": [
    "### Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ee472a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "v=[]\n",
    "for pred_tag in test_pred_v:\n",
    "    if pred_tag == '--s--':\n",
    "        continue\n",
    "    elif pred_tag == '--e--':\n",
    "        v.append([])\n",
    "    else:\n",
    "        v.append(pred_tag)\n",
    "\n",
    "with open('viterbi.out', 'w') as f:\n",
    "    for index, s in enumerate(test_data[:-1]):\n",
    "        if s:\n",
    "            f.write(s[0]+'\\t'+s[1]+'\\t'+v[index]+'\\n')\n",
    "        else:\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "095f1ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9519961910630015"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=[]\n",
    "for x in range(len(dev_words_seq)):\n",
    "    predicted_tags = defaultdict(lambda: defaultdict(list))\n",
    "    sentence = dev_words_seq[x][:]\n",
    "    for index, word in enumerate(sentence):\n",
    "        word = word if word in vocab.keys() else '< unk >'\n",
    "        possible_tags_for_words = list(tags_of_word[word]) \n",
    "        if index == 1:\n",
    "            for tag in possible_tags_for_words:                \n",
    "                if  emission_prob[tag][word] != 0:\n",
    "                    predicted_tags[index][tag] = ['--s--',transition_prob['--s--'][tag]*emission_prob[tag][word]]\n",
    "                else:\n",
    "                    predicted_tags[index][tag]  = ['--s--', 0.00001]\n",
    "                    \n",
    "        elif index>=1:\n",
    "            for tag in possible_tags_for_words:\n",
    "                previous_tags = list(predicted_tags[index-1].keys())\n",
    "                for prev_tag in previous_tags:                        \n",
    "                    if emission_prob[tag][word] == 0:\n",
    "                        temp = predicted_tags[index-1][prev_tag][1]*0.00001\n",
    "                    else:\n",
    "                        temp = predicted_tags[index-1][prev_tag][1]*transition_prob[prev_tag][tag]*emission_prob[tag][word]\n",
    "                        \n",
    "                    if  len(predicted_tags[index][tag])>0:  \n",
    "                        if temp > predicted_tags[index][tag][1]:                        \n",
    "                             predicted_tags[index][tag] = [prev_tag, temp]\n",
    "                    else:\n",
    "                        predicted_tags[index][tag] = [prev_tag, temp]\n",
    "    \n",
    "    cur_sentence_len = len(sentence)\n",
    "    final_pred = []\n",
    "    p_tag = ''\n",
    "    for i in range(cur_sentence_len-1,0,-1):\n",
    "        if i==cur_sentence_len-1:\n",
    "            last_word_tags = list(predicted_tags[i].keys())            \n",
    "            temp2 = -1000\n",
    "            max_tag = \".\"\n",
    "            for key in last_word_tags:\n",
    "                if temp2 < predicted_tags[i][key][1]:\n",
    "                    temp2 = predicted_tags[i][key][1]\n",
    "                    max_tag = key\n",
    "                    p_tag = predicted_tags[i][key][0]\n",
    "            final_pred.append(max_tag)\n",
    "            \n",
    "        else:\n",
    "            final_pred.append(prev_tag)\n",
    "            prev_tag = predicted_tags[i][prev_tag][0]\n",
    "    final_pred.append('--s--')\n",
    "    final_pred.reverse()       \n",
    "    preds.append(final_pred[:])\n",
    "    \n",
    "dev_preds_v = [element for innerList in preds for element in innerList]\n",
    "v_acc=findAccuracy(dev_tags_list, dev_preds_v)                                    \n",
    "\n",
    "v_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e268828",
   "metadata": {},
   "outputs": [],
   "source": [
    "v=[]\n",
    "for pred_tag in test_pred_v:\n",
    "    if pred_tag == '--s--':\n",
    "        continue\n",
    "    elif pred_tag == '--e--':\n",
    "        v.append([])\n",
    "    else:\n",
    "        v.append(pred_tag)\n",
    "\n",
    "with open('viterbi.out', 'w') as f:\n",
    "    for index, s in enumerate(test_data[:-1]):\n",
    "        if s:\n",
    "            f.write(s[0]+'\\t'+s[1]+'\\t'+v[index]+'\\n')\n",
    "        else:\n",
    "            f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
