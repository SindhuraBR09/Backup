{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42437534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from collections import defaultdict\n",
    "nltk.download('wordnet',quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt',quiet=True)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32843d78",
   "metadata": {},
   "source": [
    "### 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "096f1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bae3a8",
   "metadata": {},
   "source": [
    "* Read the complete dataset from \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz\"\n",
    "* Create three-class classification problem according to the rating.\n",
    "* Performed the below data cleaning on complete data\n",
    "    * convert the all reviews into the lower case\n",
    "    * remove html and url\n",
    "    * remove non-alphabetical chars\n",
    "    * remove extra spaces\n",
    "    * perform contractions\n",
    "* Selectd 20,000 random reviews from each rating class and created a balanced dataset. \n",
    "\n",
    "\n",
    "* Iterate through each review\n",
    "* If word in review is in Word embedding, get the word vector\n",
    "* Compute the average vector of sentence.\n",
    "* Split the dataset into train and test set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0661f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_reviews = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz\"\n",
    "complete_data = pd.read_csv(complete_reviews, sep='\\t', on_bad_lines='skip', low_memory=False)\n",
    "complete_data.dropna(inplace=True)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "full_data = complete_data[['star_rating', 'review_body']]\n",
    "full_data.dropna(inplace=True)\n",
    "full_data['star_rating'] = full_data['star_rating'].astype('int32')\n",
    "\n",
    "for index, row in full_data.iterrows():\n",
    "    if row['star_rating'] in {1,2}:\n",
    "        full_data.loc[index, 'star_rating'] = 1\n",
    "    elif row['star_rating'] in {3}:\n",
    "        full_data.loc[index, 'star_rating'] = 2\n",
    "    elif row['star_rating'] in {4,5}:\n",
    "        full_data.loc[index, 'star_rating'] = 3\n",
    "\n",
    "def clean_data(data):\n",
    "    #covert to lower\n",
    "    data= data.lower()\n",
    "    #remove html and url\n",
    "    data = re.sub(r'http\\S+', '', data)\n",
    "    urls = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    data = re.sub(urls, '', data)\n",
    "    #remove non-alphabetical chars\n",
    "    non_alpha = re.compile('[^a-zA-Z]')\n",
    "    data = non_alpha.sub(' ', data)\n",
    "    #remove extra spaces\n",
    "    data = re.sub(' +', ' ', data)\n",
    "    #perform contractions\n",
    "    data = contractions.fix(data)\n",
    "    return data\n",
    "\n",
    "balanced_data = full_data.groupby('star_rating').apply(lambda group: group.sample(20000)).reset_index(drop = True)\n",
    "balanced_data['review_body'] = balanced_data.apply(lambda row : clean_data(row['review_body']), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72d7625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data = shuffle(balanced_data)\n",
    "test_reviews = balanced_data['review_body']\n",
    "avg_review_vectors = []\n",
    "for review in test_reviews:\n",
    "    words = review.split()\n",
    "    rv = np.zeros(300)\n",
    "    for word in words:\n",
    "        if word in wv:\n",
    "            rv += wv[word]\n",
    "    if(len(words)):\n",
    "        rv /= len(words)\n",
    "    avg_review_vectors.append(rv)\n",
    "\n",
    "avg_review_vectors = np.array(avg_review_vectors)\n",
    "review_ratings = balanced_data['star_rating']\n",
    "train_X = avg_review_vectors[:int(0.8 * len(avg_review_vectors))]\n",
    "train_Y = review_ratings[:int(0.8 * len(avg_review_vectors))]\n",
    "test_X = avg_review_vectors[int(0.8 * len(avg_review_vectors)):]\n",
    "test_Y =  review_ratings[int(0.8 * len(avg_review_vectors)):]\n",
    "train_X=np.nan_to_num(train_X, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "test_X=np.nan_to_num(test_X,copy=True, nan=0.0, posinf=None, neginf=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ebedf1",
   "metadata": {},
   "source": [
    "### 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31735994",
   "metadata": {},
   "source": [
    "To learn the semantic similarity, I am considering the below three examples:\n",
    "* Finding words similar to \"happy\" using its vector\n",
    "* Performing \"big - large + small = tiny\" using word vector\n",
    "* Finding cosine similarity between words \"love\" and \"like\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ce779e",
   "metadata": {},
   "source": [
    "#### (a) Pre-trained Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731c6578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to happy:\n",
      "\thappy: 1.0000\n",
      "\tglad: 0.7409\n",
      "\tpleased: 0.6632\n",
      "\tecstatic: 0.6627\n",
      "\toverjoyed: 0.6599\n",
      "\n",
      "Most similar words to 'big-large+small':\n",
      "\tbig: 0.7968\n",
      "\tsmall: 0.6329\n",
      "\tbigger: 0.5330\n",
      "\thuge: 0.4986\n",
      "\tlittle_bitty: 0.4698\n",
      "\tbiggest: 0.4613\n",
      "\ttiny: 0.4609\n",
      "\tSmall: 0.4602\n",
      "\tnice: 0.4599\n",
      "\tabig: 0.4512\n",
      "\n",
      "Cosine similarity between love and like: 0.36713877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Similar words to happy:\")\n",
    "res = wv.similar_by_vector(wv[\"happy\"], topn=5)\n",
    "for word, score in res:\n",
    "    print(\"\\t{}: {:.4f}\".format(word, score))\n",
    "print()\n",
    "\n",
    "big = wv['big']\n",
    "large = wv['large']\n",
    "small = wv['small']\n",
    "result = big-large+small\n",
    "similarity = wv.similar_by_vector(result)\n",
    "print(\"Most similar words to 'big-large+small':\")\n",
    "for word, score in similarity:\n",
    "    print(\"\\t{}: {:.4f}\".format(word, score))\n",
    "print()    \n",
    "cosine = wv.similarity(\"love\", \"like\")\n",
    "print(\"Cosine similarity between love and like:\", cosine)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e3242",
   "metadata": {},
   "source": [
    "#### (b) Training Word2Vec on Review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba4634e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to happy:\n",
      "\tchest: 0.2259\n",
      "\tperspiration: 0.2153\n",
      "\tcheeks: 0.2106\n",
      "\trash: 0.2047\n",
      "\tunderarm: 0.2008\n",
      "\n",
      "Most similar words to 'big-large+small':\n",
      "\tbig: 0.8179\n",
      "\tsmall: 0.7084\n",
      "\thuge: 0.5634\n",
      "\ttiny: 0.5382\n",
      "\ttall: 0.4235\n",
      "\tpractical: 0.3925\n",
      "\twasteful: 0.3881\n",
      "\tcute: 0.3819\n",
      "\tpricey: 0.3770\n",
      "\tbreaker: 0.3611\n",
      "\n",
      "Cosine similarity between love and like: 0.25789207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r_data =balanced_data['review_body']\n",
    "sentences = []\n",
    "for s in r_data:\n",
    "    sentences.append(list(s.split(\" \")))\n",
    "    \n",
    "my_model = Word2Vec(sentences, vector_size=300, window=13, min_count=9)\n",
    "my_model.train(sentences, total_examples=my_model.corpus_count, epochs=my_model.epochs)\n",
    "\n",
    "print(\"Similar words to happy:\")\n",
    "res = my_model.wv.similar_by_vector(wv[\"happy\"], topn=5)\n",
    "for word, score in res:\n",
    "    print(\"\\t{}: {:.4f}\".format(word, score))\n",
    "print()\n",
    "\n",
    "big_2 = my_model.wv['big']\n",
    "large_2 = my_model.wv['large']\n",
    "small_2 = my_model.wv['small']\n",
    "result_2 = big_2-large_2+small_2\n",
    "similarity_2 = my_model.wv.similar_by_vector(result_2)\n",
    "print(\"Most similar words to 'big-large+small':\")\n",
    "for word, score in similarity_2:\n",
    "    print(\"\\t{}: {:.4f}\".format(word, score))\n",
    "print()\n",
    "\n",
    "cosine = my_model.wv.similarity(\"love\", \"like\")\n",
    "print(\"Cosine similarity between love and like:\", cosine)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b837d5",
   "metadata": {},
   "source": [
    "#### Conclusions:\n",
    "\n",
    "* For the first example, pretrained models gives much better result compared to our custom model. Custome model process the word in refrence to the dataset that it has i.e with respect to reviews. Hence the similar words suggested by custom model are not synonyms of \"happy\"\n",
    "\n",
    "* For the second example \"big-large+small=tiny\", both the models are performing considerably well. The score returned for \"tiny\" by the custom model is slightly more compared to  pretrained model.\n",
    "\n",
    "* For the thrid example, cosine similarity score given by pretrained model is higher but difference is negligible. \n",
    "\n",
    "Overall, pretrained model performs much better in comparision to the custom model. The main reason being, custom model has smaller dataset (only the reviews) where as pretrained model has very vast dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e0310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1f218f6",
   "metadata": {},
   "source": [
    "## 3. Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "428c78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAccuracy(y_test, y_pred):\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, digits=4)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    return 100*accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d152e20",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb6b60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Accuracy using Word2Vec  =  59.550000000000004\n",
      "\n",
      "Perceptron Accuracy using TF-IDF =  65.78333333333333\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "perceptron = Perceptron(max_iter=9000)\n",
    "perceptron.fit(train_X, train_Y)\n",
    "perceptron_predictions = perceptron.predict(test_X)\n",
    "print(\"Perceptron Accuracy using Word2Vec  = \", findAccuracy(test_Y, perceptron_predictions))\n",
    "\n",
    "###########Compute using TFIDF#####################\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopword(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw =  \" \".join([word for word in text_tokens if word not in stop_words])\n",
    "    return tokens_without_sw\n",
    "def lemmetize(text):    \n",
    "    text_tokens = word_tokenize(text)\n",
    "    lemmatized_string = \" \".join([lemmatizer.lemmatize(words) for words in text_tokens])\n",
    "    return lemmatized_string\n",
    "balanced_data['cleaned_data'] = balanced_data.apply(lambda row : remove_stopword(row['review_body']), axis = 1)\n",
    "balanced_data['cleaned_data'] = balanced_data.apply(lambda row : lemmetize(row['review_body']), axis = 1)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=9, ngram_range=(1,2))\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(balanced_data['cleaned_data'])\n",
    "X_tfidf = tfidf_vector\n",
    "y_tfidf = balanced_data['star_rating']\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, y_tfidf, test_size=0.2, random_state=42)\n",
    "perceptron_model = Perceptron(max_iter=10000)\n",
    "perceptron_model.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_perceptron = perceptron_model.predict(X_test_tfidf)\n",
    "print(\"\\nPerceptron Accuracy using TF-IDF = \", findAccuracy(y_test_tfidf, y_pred_perceptron))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb6a154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy using Word2Vec (%) =  65.925\n",
      "\n",
      "SVM Accuracy using TF-IDF (%)=  74.45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = SVC(C=0.1)\n",
    "svm.fit(train_X, train_Y)\n",
    "svm_predictions = svm.predict(test_X)\n",
    "print(\"SVM Accuracy using Word2Vec (%) = \", findAccuracy(test_Y, svm_predictions))\n",
    "\n",
    "######################TF-IDF####################################\n",
    "\n",
    "svm_model = LinearSVC(C=0.1).fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "print(\"\\nSVM Accuracy using TF-IDF (%)= \", findAccuracy(y_test_tfidf, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9825e20",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "From above test accuarcy values, we can conclude that TF-IDF model performing better than Word2Vec for both SM and Perceptron.It's also worth noting that TFIDF and Word2Vec capture different aspects of language. TFIDF is a statistical measure of how important a word is to a document in a corpus, while Word2Vec is a neural network-based model that captures the semantic relationships between words. \n",
    "\n",
    "There could be several reasons why the model using TFIDF is performing better than the Word2Vec model.\n",
    "* Word2Vec models are trained on general language data, and may not perform as well on domain-specific language. In this case reviews are specific to beauty products. Hence review dataset might have many words repeated lot of times.\n",
    "* Word2Vec requires a large amount of data to accurately capture the nuances of language. If you have a small dataset, the Word2Vec model may not have enough data to work with and may not be able to capture the meaning of words accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89958023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ff1466d",
   "metadata": {},
   "source": [
    "## 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3faee7",
   "metadata": {},
   "source": [
    "##### Workflow:\n",
    "\n",
    "* Defined a class 'Feedforward_MLP' which is used for both 4.a and 4.b\n",
    "\n",
    "* __init__ method takes an input_size as argument which specifies the size of the input data.\n",
    "    * self.fc1: The first fully connected layer takes the input of size 'input_size' and produces an output of size 100.\n",
    "    * self.dropout: A dropout layer that randomly sets input elements to zero with a given probability during training. This helps prevent overfitting and improves the generalization of the model.\n",
    "    * self.fc2: The second fully connected layer takes the output from the previous layer (size 100) and produces an output of size 10.\n",
    "    * self.fc3: The third fully connected layer takes the output from the previous layer (size 10) and produces an output of size 3.\n",
    "    * self.softmax: A softmax function applied to the output of the last layer to convert the output values to probabilities that sum to 1 over the 3\n",
    "    \n",
    "* **forward method**  applies the layers defined in the __init__ method in a feedforward manner to produce the output.\n",
    "    * x = F.relu(self.fc1(x)): The input x is passed through the first fully connected layer, self.fc1, followed by the ReLU activation function. This produces an output x of size 100.\n",
    "    * x = self.dropout(x): The output x from the first layer is passed through the dropout layer defined in __init__.\n",
    "    * x = F.relu(self.fc2(x)): The output x from the dropout layer is passed through the second fully connected layer, self.fc2, followed by the ReLU activation function. This produces an output x of size 10.\n",
    "    * x = self.dropout(x): The output x from the second layer is passed through the dropout layer defined in __init__.\n",
    "    * x = self.fc3(x): The output x from the second dropout layer is passed through the third fully connected layer, self.fc3, which produces an output x of size 3.\n",
    "    * x = self.softmax(x): The output x from the third layer is passed through the softmax function defined in __init__, which converts the output values to probabilities that sum to 1 over the 3 output classes.\n",
    "    \n",
    "**Optimizers** : I have used different optimizers for 4.a and 4.b. MLP model with average vectors performed better with SGD optimizer. Where as MLP model with concatenated words performed better with Adam optimizer.\n",
    "\n",
    "**Batch Size** : 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "078ad1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust the class labels to start from 0\n",
    "train_label_0 = [x-1 for x in train_Y]\n",
    "test_label_0 = [x-1 for x in test_Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7140c7",
   "metadata": {},
   "source": [
    "#### (a) Using average Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e8439bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward_MLP(torch.nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(Feedforward_MLP, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, 100)\n",
    "            self.dropout = torch.nn.Dropout(0.2)            \n",
    "#             self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(100, 10)\n",
    "            self.fc3 = torch.nn.Linear(10, 3)\n",
    "#             self.sigmoid = torch.nn.Sigmoid()\n",
    "            self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            x= self.softmax(x)\n",
    "            return x\n",
    "        \n",
    "mlp_model = Feedforward_MLP(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "024ae834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.844494\n",
      "Epoch: 2 \tTraining Loss: 0.843197\n",
      "Epoch: 3 \tTraining Loss: 0.841609\n",
      "Epoch: 4 \tTraining Loss: 0.840858\n",
      "Epoch: 5 \tTraining Loss: 0.840465\n",
      "Epoch: 6 \tTraining Loss: 0.838932\n",
      "Epoch: 7 \tTraining Loss: 0.838825\n",
      "Epoch: 8 \tTraining Loss: 0.837016\n",
      "Epoch: 9 \tTraining Loss: 0.836743\n",
      "Epoch: 10 \tTraining Loss: 0.837468\n",
      "Epoch: 11 \tTraining Loss: 0.836967\n",
      "Epoch: 12 \tTraining Loss: 0.836400\n",
      "Epoch: 13 \tTraining Loss: 0.836334\n",
      "Epoch: 14 \tTraining Loss: 0.834739\n",
      "Epoch: 15 \tTraining Loss: 0.835276\n",
      "Epoch: 16 \tTraining Loss: 0.834645\n",
      "Epoch: 17 \tTraining Loss: 0.834795\n",
      "Epoch: 18 \tTraining Loss: 0.835128\n",
      "Epoch: 19 \tTraining Loss: 0.834249\n",
      "Epoch: 20 \tTraining Loss: 0.834046\n",
      "Epoch: 21 \tTraining Loss: 0.834135\n",
      "Epoch: 22 \tTraining Loss: 0.833982\n",
      "Epoch: 23 \tTraining Loss: 0.833966\n",
      "Epoch: 24 \tTraining Loss: 0.833890\n",
      "Epoch: 25 \tTraining Loss: 0.834418\n",
      "Epoch: 26 \tTraining Loss: 0.832815\n",
      "Epoch: 27 \tTraining Loss: 0.834343\n",
      "Epoch: 28 \tTraining Loss: 0.833762\n",
      "Epoch: 29 \tTraining Loss: 0.832672\n",
      "Epoch: 30 \tTraining Loss: 0.832798\n",
      "Epoch: 31 \tTraining Loss: 0.833962\n",
      "Epoch: 32 \tTraining Loss: 0.833494\n",
      "Epoch: 33 \tTraining Loss: 0.834711\n",
      "Epoch: 34 \tTraining Loss: 0.832784\n",
      "Epoch: 35 \tTraining Loss: 0.834364\n",
      "Epoch: 36 \tTraining Loss: 0.833668\n",
      "Epoch: 37 \tTraining Loss: 0.833223\n",
      "Epoch: 38 \tTraining Loss: 0.832983\n",
      "Epoch: 39 \tTraining Loss: 0.832977\n",
      "Epoch: 40 \tTraining Loss: 0.832135\n",
      "Epoch: 41 \tTraining Loss: 0.832998\n",
      "Epoch: 42 \tTraining Loss: 0.833036\n",
      "Epoch: 43 \tTraining Loss: 0.832461\n",
      "Epoch: 44 \tTraining Loss: 0.833618\n",
      "Epoch: 45 \tTraining Loss: 0.832599\n",
      "Epoch: 46 \tTraining Loss: 0.832042\n",
      "Epoch: 47 \tTraining Loss: 0.833484\n",
      "Epoch: 48 \tTraining Loss: 0.832293\n",
      "Epoch: 49 \tTraining Loss: 0.832208\n",
      "Epoch: 50 \tTraining Loss: 0.831869\n",
      "Epoch: 51 \tTraining Loss: 0.831950\n",
      "Epoch: 52 \tTraining Loss: 0.832766\n",
      "Epoch: 53 \tTraining Loss: 0.833146\n",
      "Epoch: 54 \tTraining Loss: 0.832805\n",
      "Epoch: 55 \tTraining Loss: 0.831280\n",
      "Epoch: 56 \tTraining Loss: 0.832122\n",
      "Epoch: 57 \tTraining Loss: 0.831709\n",
      "Epoch: 58 \tTraining Loss: 0.830265\n",
      "Epoch: 59 \tTraining Loss: 0.830336\n",
      "Epoch: 60 \tTraining Loss: 0.831928\n",
      "Epoch: 61 \tTraining Loss: 0.832054\n",
      "Epoch: 62 \tTraining Loss: 0.831638\n",
      "Epoch: 63 \tTraining Loss: 0.831946\n",
      "Epoch: 64 \tTraining Loss: 0.831714\n",
      "Epoch: 65 \tTraining Loss: 0.833114\n",
      "Epoch: 66 \tTraining Loss: 0.833163\n",
      "Epoch: 67 \tTraining Loss: 0.832233\n",
      "Epoch: 68 \tTraining Loss: 0.832018\n",
      "Epoch: 69 \tTraining Loss: 0.831840\n",
      "Epoch: 70 \tTraining Loss: 0.831864\n",
      "Epoch: 71 \tTraining Loss: 0.831990\n",
      "Epoch: 72 \tTraining Loss: 0.831991\n",
      "Epoch: 73 \tTraining Loss: 0.832198\n",
      "Epoch: 74 \tTraining Loss: 0.831342\n",
      "Epoch: 75 \tTraining Loss: 0.832381\n",
      "Epoch: 76 \tTraining Loss: 0.831140\n",
      "Epoch: 77 \tTraining Loss: 0.830990\n",
      "Epoch: 78 \tTraining Loss: 0.831930\n",
      "Epoch: 79 \tTraining Loss: 0.831972\n",
      "Epoch: 80 \tTraining Loss: 0.831344\n",
      "Epoch: 81 \tTraining Loss: 0.831555\n",
      "Epoch: 82 \tTraining Loss: 0.831186\n",
      "Epoch: 83 \tTraining Loss: 0.832690\n",
      "Epoch: 84 \tTraining Loss: 0.831253\n",
      "Epoch: 85 \tTraining Loss: 0.831046\n",
      "Epoch: 86 \tTraining Loss: 0.832040\n",
      "Epoch: 87 \tTraining Loss: 0.831231\n",
      "Epoch: 88 \tTraining Loss: 0.831911\n",
      "Epoch: 89 \tTraining Loss: 0.831360\n",
      "Epoch: 90 \tTraining Loss: 0.831081\n",
      "Epoch: 91 \tTraining Loss: 0.831060\n",
      "Epoch: 92 \tTraining Loss: 0.831863\n",
      "Epoch: 93 \tTraining Loss: 0.830865\n",
      "Epoch: 94 \tTraining Loss: 0.831616\n",
      "Epoch: 95 \tTraining Loss: 0.832844\n",
      "Epoch: 96 \tTraining Loss: 0.832081\n",
      "Epoch: 97 \tTraining Loss: 0.830276\n",
      "Epoch: 98 \tTraining Loss: 0.833162\n",
      "Epoch: 99 \tTraining Loss: 0.830768\n",
      "Epoch: 100 \tTraining Loss: 0.831542\n"
     ]
    }
   ],
   "source": [
    "tensor_trainX = torch.Tensor(train_X)\n",
    "tensor_trainY = torch.Tensor(train_label_0).type(torch.LongTensor)\n",
    "tensor_testX = torch.Tensor(test_X)\n",
    "tensor_testY = torch.Tensor(test_label_0).type(torch.LongTensor)\n",
    "\n",
    "# Create Torch datasets\n",
    "tensorset_train = torch.utils.data.TensorDataset(tensor_trainX, tensor_trainY)\n",
    "tensorset_test = torch.utils.data.TensorDataset(tensor_testX, tensor_testY)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(tensorset_train, batch_size=20,shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(tensorset_test, batch_size=20, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.SGD(mlp_model.parameters(),lr=0.005)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "test_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(100):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    mlp_model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = mlp_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b618bad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward Neural Network Accuracy (Avg Vectors) =  67.28333333333333\n"
     ]
    }
   ],
   "source": [
    "test_loader_ffn = torch.utils.data.DataLoader(tensorset_test, batch_size=1, shuffle=False)\n",
    "y_pred_mlp = []\n",
    "mlp_model.eval()\n",
    "for i, (review, rating) in enumerate(test_loader_ffn):\n",
    "    outputs = mlp_model(review)\n",
    "    predicted = torch.argmax(outputs).item() \n",
    "    y_pred_mlp.append(predicted)\n",
    "print(\"Feedforward Neural Network Accuracy (Avg Vectors) = \", findAccuracy(tensor_testY.tolist(), y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b79aad",
   "metadata": {},
   "source": [
    "#### (b) Input feature as concatenated vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac3c20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_feature_vector(reviews):\n",
    "    review_words = [review.split() for review in reviews]    \n",
    "    vector_size = wv.vector_size    \n",
    "    num_reviews = len(reviews)\n",
    "    input_features = np.zeros((num_reviews, 10*vector_size))\n",
    "    \n",
    "    for i, words in enumerate(review_words):\n",
    "        vectors = []\n",
    "        for j in range(min(len(words), 10)):\n",
    "            word = words[j]\n",
    "            if word in wv:\n",
    "                vectors.append(wv[word])\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        num_missing_vectors = max(0, 10 - len(vectors))\n",
    "        padded_vectors = vectors + [np.zeros(vector_size)]*num_missing_vectors        \n",
    "        feature_vector = np.concatenate(padded_vectors)        \n",
    "        input_features[i,:] = feature_vector\n",
    "    \n",
    "    return input_features\n",
    "\n",
    "input_feature = generate_input_feature_vector(balanced_data['review_body'])\n",
    "train_data_concat = input_feature[:int(0.8 * len(input_feature))]\n",
    "train_label_concat = balanced_data['star_rating'][:int(0.8 * len(input_feature))]\n",
    "test_data_concat = input_feature[int(0.8 * len(input_feature)):]\n",
    "test_label_concat =  balanced_data['star_rating'][int(0.8 * len(input_feature)):]\n",
    "\n",
    "tensor_trainX_concat = torch.Tensor(train_data_concat)\n",
    "tensor_trainY_concat = torch.Tensor(train_label_0).type(torch.LongTensor)\n",
    "tensor_testX_concat = torch.Tensor(test_data_concat)\n",
    "tensor_testY_concat = torch.Tensor(test_label_0).type(torch.LongTensor)\n",
    "\n",
    "# Create Torch datasets\n",
    "train_tensorset_concat = torch.utils.data.TensorDataset(tensor_trainX_concat, tensor_trainY_concat)\n",
    "test_tensorset_concat = torch.utils.data.TensorDataset(tensor_testX_concat, tensor_testY_concat)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_tensorset_concat, batch_size=20,shuffle=False)\n",
    "# test_loader = torch.utils.data.DataLoader(test_tensorset_concat, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45bd324d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.005786\n",
      "Epoch: 2 \tTraining Loss: 0.985832\n",
      "Epoch: 3 \tTraining Loss: 0.975598\n",
      "Epoch: 4 \tTraining Loss: 0.971429\n",
      "Epoch: 5 \tTraining Loss: 0.965208\n",
      "Epoch: 6 \tTraining Loss: 0.961411\n",
      "Epoch: 7 \tTraining Loss: 0.958249\n",
      "Epoch: 8 \tTraining Loss: 0.957097\n",
      "Epoch: 9 \tTraining Loss: 0.956316\n",
      "Epoch: 10 \tTraining Loss: 0.954477\n",
      "Epoch: 11 \tTraining Loss: 0.953982\n",
      "Epoch: 12 \tTraining Loss: 0.952753\n",
      "Epoch: 13 \tTraining Loss: 0.952201\n",
      "Epoch: 14 \tTraining Loss: 0.951212\n",
      "Epoch: 15 \tTraining Loss: 0.951829\n",
      "Epoch: 16 \tTraining Loss: 0.949436\n",
      "Epoch: 17 \tTraining Loss: 0.950420\n",
      "Epoch: 18 \tTraining Loss: 0.950830\n",
      "Epoch: 19 \tTraining Loss: 0.952146\n",
      "Epoch: 20 \tTraining Loss: 0.950115\n",
      "Epoch: 21 \tTraining Loss: 0.954124\n",
      "Epoch: 22 \tTraining Loss: 0.949655\n",
      "Epoch: 23 \tTraining Loss: 0.949825\n",
      "Epoch: 24 \tTraining Loss: 0.951329\n",
      "Epoch: 25 \tTraining Loss: 0.949513\n",
      "Epoch: 26 \tTraining Loss: 0.950727\n",
      "Epoch: 27 \tTraining Loss: 0.949899\n",
      "Epoch: 28 \tTraining Loss: 0.950814\n",
      "Epoch: 29 \tTraining Loss: 0.949059\n",
      "Epoch: 30 \tTraining Loss: 0.952461\n",
      "Epoch: 31 \tTraining Loss: 0.950335\n",
      "Epoch: 32 \tTraining Loss: 0.950397\n",
      "Epoch: 33 \tTraining Loss: 0.949674\n",
      "Epoch: 34 \tTraining Loss: 0.951029\n",
      "Epoch: 35 \tTraining Loss: 0.950742\n",
      "Epoch: 36 \tTraining Loss: 0.950227\n",
      "Epoch: 37 \tTraining Loss: 0.949388\n",
      "Epoch: 38 \tTraining Loss: 0.948802\n",
      "Epoch: 39 \tTraining Loss: 0.950941\n",
      "Epoch: 40 \tTraining Loss: 0.951487\n",
      "Epoch: 41 \tTraining Loss: 0.949132\n",
      "Epoch: 42 \tTraining Loss: 0.947800\n",
      "Epoch: 43 \tTraining Loss: 0.950369\n",
      "Epoch: 44 \tTraining Loss: 0.950264\n",
      "Epoch: 45 \tTraining Loss: 0.950138\n",
      "Epoch: 46 \tTraining Loss: 0.949871\n",
      "Epoch: 47 \tTraining Loss: 0.951315\n",
      "Epoch: 48 \tTraining Loss: 0.950341\n",
      "Epoch: 49 \tTraining Loss: 0.951103\n",
      "Epoch: 50 \tTraining Loss: 0.950528\n",
      "Epoch: 51 \tTraining Loss: 0.949911\n",
      "Epoch: 52 \tTraining Loss: 0.952441\n",
      "Epoch: 53 \tTraining Loss: 0.951524\n",
      "Epoch: 54 \tTraining Loss: 0.952383\n",
      "Epoch: 55 \tTraining Loss: 0.948843\n",
      "Epoch: 56 \tTraining Loss: 0.950208\n",
      "Epoch: 57 \tTraining Loss: 0.953280\n",
      "Epoch: 58 \tTraining Loss: 0.952538\n",
      "Epoch: 59 \tTraining Loss: 0.951600\n",
      "Epoch: 60 \tTraining Loss: 0.951910\n",
      "Epoch: 61 \tTraining Loss: 0.950422\n",
      "Epoch: 62 \tTraining Loss: 0.948984\n",
      "Epoch: 63 \tTraining Loss: 0.949014\n",
      "Epoch: 64 \tTraining Loss: 0.949932\n",
      "Epoch: 65 \tTraining Loss: 0.949470\n",
      "Epoch: 66 \tTraining Loss: 0.952643\n",
      "Epoch: 67 \tTraining Loss: 0.950955\n",
      "Epoch: 68 \tTraining Loss: 0.951370\n",
      "Epoch: 69 \tTraining Loss: 0.947697\n",
      "Epoch: 70 \tTraining Loss: 0.951642\n",
      "Epoch: 71 \tTraining Loss: 0.952917\n",
      "Epoch: 72 \tTraining Loss: 0.948433\n",
      "Epoch: 73 \tTraining Loss: 0.952398\n",
      "Epoch: 74 \tTraining Loss: 0.949081\n",
      "Epoch: 75 \tTraining Loss: 0.949288\n",
      "Epoch: 76 \tTraining Loss: 0.952259\n",
      "Epoch: 77 \tTraining Loss: 0.951580\n",
      "Epoch: 78 \tTraining Loss: 0.951917\n",
      "Epoch: 79 \tTraining Loss: 0.952272\n",
      "Epoch: 80 \tTraining Loss: 0.947504\n",
      "Epoch: 81 \tTraining Loss: 0.950460\n",
      "Epoch: 82 \tTraining Loss: 0.948462\n",
      "Epoch: 83 \tTraining Loss: 0.950049\n",
      "Epoch: 84 \tTraining Loss: 0.951389\n",
      "Epoch: 85 \tTraining Loss: 0.951008\n",
      "Epoch: 86 \tTraining Loss: 0.950862\n",
      "Epoch: 87 \tTraining Loss: 0.949588\n",
      "Epoch: 88 \tTraining Loss: 0.949317\n",
      "Epoch: 89 \tTraining Loss: 0.950291\n",
      "Epoch: 90 \tTraining Loss: 0.950362\n",
      "Epoch: 91 \tTraining Loss: 0.949799\n",
      "Epoch: 92 \tTraining Loss: 0.950146\n",
      "Epoch: 93 \tTraining Loss: 0.951760\n",
      "Epoch: 94 \tTraining Loss: 0.947211\n",
      "Epoch: 95 \tTraining Loss: 0.950921\n",
      "Epoch: 96 \tTraining Loss: 0.949257\n",
      "Epoch: 97 \tTraining Loss: 0.947268\n",
      "Epoch: 98 \tTraining Loss: 0.950846\n",
      "Epoch: 99 \tTraining Loss: 0.947822\n",
      "Epoch: 100 \tTraining Loss: 0.951403\n"
     ]
    }
   ],
   "source": [
    "concatenated_model = Feedforward_MLP(3000)\n",
    "optimizer_concat = torch.optim.Adam(concatenated_model.parameters(),lr=0.005, betas=(0.9,0.999),eps=1e-08,weight_decay=5e-5)\n",
    "test_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(100):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    concatenated_model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer_concat.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = concatenated_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer_concat.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "741ca8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN Accuracy (concatenated words) in %  =  56.40833333333334\n"
     ]
    }
   ],
   "source": [
    "test_loader_concat = torch.utils.data.DataLoader(test_tensorset_concat, batch_size=1, shuffle=False)\n",
    "# concat_model = torch.load('loa.pt')\n",
    "y_pred_concat = []\n",
    "concatenated_model.eval()\n",
    "for i, (review, rating) in enumerate(test_loader_concat):\n",
    "    outputs = concatenated_model(review)\n",
    "    predicted = torch.argmax(outputs).item() \n",
    "    y_pred_concat.append(predicted)\n",
    "print(\"FNN Accuracy (concatenated words) in %  = \", findAccuracy(tensor_testY_concat.tolist(), y_pred_concat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be3bde",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "**Test Accuracy values**\n",
    "\n",
    "(a) MLP  Accuracy (avg vectors)  = 66.86%\n",
    "\n",
    "(b) MLP Accuracy (Concatenated) = 56.4%\n",
    "\n",
    "On running the multiple times, accuracy was varying between 65-67% for (a) and 54-56% for (b)\n",
    "\n",
    "MLP with averaged vectors has higher accuray compared to MLP with first 10 concatenated vectors. Possible reasons could be,\n",
    "\n",
    "* **Information loss**: The concatenation of the first 10 words as input feature may result in information loss since the first 10 words do not necessarily capture the context of the entire sentence. As a result, the model that takes this input feature may not have enough information to accurately predict the target variable. For example: Consider 'I bought this product one month ago in a sale. It is not good'. Here the actual sentiment of review is in the last part of sentence which will be discarded by model.\n",
    "\n",
    "* **Word importance**: The first 10 words of a sentence may not always be the most important for predicting the target variable. The model that takes the average of all vectors as input feature considers all words in the sentence to be equally important, which may be more accurate for certain tasks. For example: 'It did not work for first one week of usage. But worked wonders after using for few weeks'\n",
    "\n",
    "* **Overfitting**: The model that takes the concatenation of the first 10 words as input feature may be overfitting to the training data, since the input feature is specific to the first 10 words of the sentence. On the other hand, the model that takes the average of all vectors as input feature may be more generalizable since it is based on the entire sentence.\n",
    "\n",
    "In comparision to Simple Models ( SVM and Perceptron), \n",
    "* MLP with average vectors performs better than both SVM and Perceptron because of its non-linear decision boundaries and hidden layers\n",
    "* SVM performs bettern than MLP with concatenated input vectors (b), maily because of the loss of information cause by considering only 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40490833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ab1119c",
   "metadata": {},
   "source": [
    "## 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7daedf8",
   "metadata": {},
   "source": [
    "##### Workflow:\n",
    "\n",
    "I have three seperate classes for Simple RNN, Gated RNN and LSTM\n",
    "\n",
    "1. Simple RNN:\n",
    "    Classname : RNN\n",
    "    Arguments: \n",
    "        * input_size : 300\n",
    "        * output_size : 3\n",
    "        * hidden_size : 20\n",
    "        \n",
    "   Optimizer : Adam optimizer with learning rate 0.0001\n",
    "   \n",
    "2. Gated RNN:\n",
    "    Classname: GatedNet\n",
    "    Arguments:\n",
    "        * input_dim: 300\n",
    "        * hidden_dim : 20\n",
    "        * output_dim : 3\n",
    "        \n",
    "    Optimizer : Adam optimizer with learning rate 0.0001\n",
    "    \n",
    "    \n",
    "3. LSTM:\n",
    "    Classname: LSTMModel\n",
    "    Arguments:\n",
    "        * input_dim: 300\n",
    "        * hidden_dim : 20\n",
    "        * output_dim : 3\n",
    "        \n",
    "    Optimizer : Adam optimizer with learning rate 0.0001\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c103ba1d",
   "metadata": {},
   "source": [
    "##### Data generation:\n",
    "\n",
    "Generate input feature by taking maximum review length of 20. Truncat longer reviews and pad shorter reviews with a null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5af2cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_vec_20(reviews):\n",
    "    sequences =  []\n",
    "    for review in reviews:\n",
    "        text_tokens = word_tokenize(review)        \n",
    "        vectors=[]\n",
    "        for i,word in enumerate(text_tokens):\n",
    "            if word in wv:\n",
    "                vectors.append(wv[word].reshape((1,300)))\n",
    "                \n",
    "        if(len(vectors)>=20):\n",
    "            padded_vectors = vectors[:20]\n",
    "        else:\n",
    "            num_missing_vectors = 20-len(vectors)\n",
    "            padded_vectors = vectors + [np.zeros((1,300))]*num_missing_vectors\n",
    "            \n",
    "        sequences.append(padded_vectors)\n",
    "            \n",
    "    return sequences\n",
    "            \n",
    "input_feature_rnn = generate_input_vec_20(balanced_data['review_body'])\n",
    "train_data_rnn = input_feature_rnn[:int(0.8 * len(input_feature_rnn))]\n",
    "test_data_rnn = input_feature_rnn[int(0.8 * len(input_feature_rnn)):]\n",
    "\n",
    "x_train_rnn = torch.Tensor(train_data_rnn)\n",
    "y_train_rnn = torch.Tensor(train_label_0).type(torch.LongTensor)\n",
    "x_cv_rnn = torch.Tensor(test_data_rnn)\n",
    "y_cv_rnn = torch.Tensor(test_label_0).type(torch.LongTensor)\n",
    "\n",
    "# Create Torch datasets\n",
    "train_rnn = torch.utils.data.TensorDataset(x_train_rnn, y_train_rnn)\n",
    "test_rnn = torch.utils.data.TensorDataset(x_cv_rnn, y_cv_rnn)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader_rnn = torch.utils.data.DataLoader(train_rnn, batch_size=1,shuffle=False)\n",
    "test_loader_rnn = torch.utils.data.DataLoader(test_rnn, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90471919",
   "metadata": {},
   "source": [
    "### (a) Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20dc1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "rnn_model = RNN(300,20,3)\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.0001, betas=(0.9,0.999),eps=1e-08,weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9439bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0% (0m 0s) 1.1941 / 2 ✗ (0)\n",
      "10000 20% (1m 14s) 0.5954 / 0 ✓\n",
      "20000 41% (2m 33s) 2.7528 / 2 ✗ (1)\n",
      "30000 62% (3m 48s) 1.3645 / 2 ✗ (1)\n",
      "40000 83% (5m 1s) 1.9480 / 1 ✗ (0)\n",
      "0 0% (6m 2s) 1.2555 / 1 ✗ (0)\n",
      "10000 20% (7m 16s) 0.4120 / 0 ✓\n",
      "20000 41% (8m 29s) 3.3803 / 2 ✗ (1)\n",
      "30000 62% (9m 45s) 0.8490 / 2 ✗ (1)\n",
      "40000 83% (11m 1s) 1.9096 / 1 ✗ (0)\n",
      "0 0% (12m 0s) 1.4055 / 1 ✗ (0)\n",
      "10000 20% (13m 14s) 0.1714 / 0 ✓\n",
      "20000 41% (14m 27s) 3.7929 / 2 ✗ (1)\n",
      "30000 62% (15m 41s) 1.5246 / 2 ✗ (1)\n",
      "40000 83% (16m 56s) 1.0004 / 1 ✗ (0)\n",
      "0 0% (18m 3s) 0.6036 / 0 ✓\n",
      "10000 20% (19m 20s) 0.0353 / 0 ✓\n",
      "20000 41% (20m 35s) 4.1500 / 2 ✗ (1)\n",
      "30000 62% (21m 51s) 2.4909 / 2 ✗ (1)\n",
      "40000 83% (23m 7s) 0.9554 / 1 ✗ (0)\n",
      "0 0% (24m 8s) 0.2894 / 0 ✓\n",
      "10000 20% (25m 22s) 0.0804 / 0 ✓\n",
      "20000 41% (26m 38s) 4.3747 / 2 ✗ (1)\n",
      "30000 62% (27m 52s) 2.1210 / 2 ✗ (1)\n",
      "40000 83% (29m 7s) 1.1327 / 1 ✗ (0)\n",
      "0 0% (30m 7s) 0.2013 / 0 ✓\n",
      "10000 20% (31m 23s) 0.0678 / 0 ✓\n",
      "20000 41% (32m 42s) 4.4907 / 2 ✗ (1)\n",
      "30000 62% (33m 56s) 2.0769 / 2 ✗ (1)\n",
      "40000 83% (35m 11s) 1.1928 / 1 ✗ (0)\n",
      "0 0% (36m 11s) 0.1668 / 0 ✓\n",
      "10000 20% (37m 25s) 0.0577 / 0 ✓\n",
      "20000 41% (38m 39s) 4.5852 / 2 ✗ (1)\n",
      "30000 62% (39m 54s) 2.0863 / 2 ✗ (1)\n",
      "40000 83% (41m 8s) 1.2271 / 1 ✗ (0)\n",
      "0 0% (42m 7s) 0.1480 / 0 ✓\n",
      "10000 20% (43m 25s) 0.0520 / 0 ✓\n",
      "20000 41% (44m 38s) 4.6581 / 2 ✗ (1)\n",
      "30000 62% (45m 51s) 2.1229 / 2 ✗ (1)\n",
      "40000 83% (47m 6s) 1.2446 / 1 ✗ (0)\n"
     ]
    }
   ],
   "source": [
    "def train(line_tensor, category):\n",
    "    hidden = rnn_model.initHidden()\n",
    "    rnn_model.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.shape[1]):\n",
    "        inp_tensor = line_tensor[0][i]\n",
    "        output, hidden = rnn_model(inp_tensor, hidden)        \n",
    "    \n",
    "    loss = criterion(output, category)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 5)\n",
    "    optimizer.step()\n",
    "\n",
    "    for p in rnn_model.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-0.0001)\n",
    "    \n",
    "    return output, loss.item()\n",
    "\n",
    "print_every = 10000\n",
    "plot_every = 1000\n",
    "\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(8):\n",
    "    print_counter = 0\n",
    "    for i, (review, rating) in enumerate(train_loader_rnn):\n",
    "        output, loss = train(review, rating)\n",
    "        current_loss += loss\n",
    "        # Print iter number, loss, name and guess\n",
    "        if print_counter % print_every == 0:\n",
    "            guess = torch.argmax(output).item()\n",
    "            correct = '✓' if guess == rating.item() else '✗ (%s)' % rating.item()\n",
    "            print('%d %d%% (%s) %.4f / %s %s' % (print_counter, (print_counter / 48000) * 100, timeSince(start), loss, guess, correct))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if print_counter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "            \n",
    "        print_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5acc131f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple RNN Accuracy in % =  61.06666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred_rnn = []\n",
    "y_test_rnn = []\n",
    "rnn_model.eval()\n",
    "for i, (review_test_rnn, rating_test_rnn) in enumerate(test_loader_rnn):\n",
    "    hidden = rnn_model.initHidden()\n",
    "    for j in range(review_test_rnn.shape[1]):\n",
    "        inp_tensor = review_test_rnn[0][j]\n",
    "        guess, hidden = rnn_model(inp_tensor, hidden)\n",
    "    guess = torch.argmax(guess).item()\n",
    "    y_test_rnn.append(rating_test_rnn.item())\n",
    "    y_pred_rnn.append(guess)\n",
    "\n",
    "print(\"Simple RNN Accuracy in % = \", findAccuracy( y_test_rnn, y_pred_rnn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf016e",
   "metadata": {},
   "source": [
    "### (b) Gated RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af9b2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers=1, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim)\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0)\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "gru_model = GRUNet(300,20,3)\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=0.0005, betas=(0.9,0.999),eps=1e-08,weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41ad15d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0......Step: 1000/48000....... Average Loss for Epoch: 1.0269569813609123\n",
      "Epoch 0......Step: 2000/48000....... Average Loss for Epoch: 0.4375462125837803\n",
      "Epoch 1......Step: 1000/48000....... Average Loss for Epoch: 1.1419159276485442\n",
      "Epoch 1......Step: 2000/48000....... Average Loss for Epoch: 0.39694790993630885\n",
      "Epoch 2......Step: 1000/48000....... Average Loss for Epoch: 1.0919201246798038\n",
      "Epoch 2......Step: 2000/48000....... Average Loss for Epoch: 0.38405050249397754\n",
      "Epoch 3......Step: 1000/48000....... Average Loss for Epoch: 1.0658256910443307\n",
      "Epoch 3......Step: 2000/48000....... Average Loss for Epoch: 0.37548590371012686\n",
      "Epoch 4......Step: 1000/48000....... Average Loss for Epoch: 1.0467317140996457\n",
      "Epoch 4......Step: 2000/48000....... Average Loss for Epoch: 0.3687788166999817\n",
      "Epoch 5......Step: 1000/48000....... Average Loss for Epoch: 1.031130910217762\n",
      "Epoch 5......Step: 2000/48000....... Average Loss for Epoch: 0.3631645495742559\n",
      "Epoch 6......Step: 1000/48000....... Average Loss for Epoch: 1.017653210312128\n",
      "Epoch 6......Step: 2000/48000....... Average Loss for Epoch: 0.3582979297339916\n",
      "Epoch 7......Step: 1000/48000....... Average Loss for Epoch: 1.005636841893196\n",
      "Epoch 7......Step: 2000/48000....... Average Loss for Epoch: 0.3539865449219942\n",
      "Epoch 8......Step: 1000/48000....... Average Loss for Epoch: 0.994736701220274\n",
      "Epoch 8......Step: 2000/48000....... Average Loss for Epoch: 0.3501113889664412\n",
      "Epoch 9......Step: 1000/48000....... Average Loss for Epoch: 0.9847522237300873\n",
      "Epoch 9......Step: 2000/48000....... Average Loss for Epoch: 0.34659007482230664\n",
      "Epoch 10......Step: 1000/48000....... Average Loss for Epoch: 0.9755440128743649\n",
      "Epoch 10......Step: 2000/48000....... Average Loss for Epoch: 0.3433575810492039\n",
      "Epoch 11......Step: 1000/48000....... Average Loss for Epoch: 0.9669984235167504\n",
      "Epoch 11......Step: 2000/48000....... Average Loss for Epoch: 0.34036049215495584\n",
      "Epoch 12......Step: 1000/48000....... Average Loss for Epoch: 0.9590113922357559\n",
      "Epoch 12......Step: 2000/48000....... Average Loss for Epoch: 0.33755533866584303\n",
      "Epoch 13......Step: 1000/48000....... Average Loss for Epoch: 0.951484542131424\n",
      "Epoch 13......Step: 2000/48000....... Average Loss for Epoch: 0.3349073360711336\n",
      "Epoch 14......Step: 1000/48000....... Average Loss for Epoch: 0.9443299185931683\n",
      "Epoch 14......Step: 2000/48000....... Average Loss for Epoch: 0.332389088049531\n",
      "Epoch 15......Step: 1000/48000....... Average Loss for Epoch: 0.9374754522442817\n",
      "Epoch 15......Step: 2000/48000....... Average Loss for Epoch: 0.3299792493581772\n",
      "Epoch 16......Step: 1000/48000....... Average Loss for Epoch: 0.9308662414848804\n",
      "Epoch 16......Step: 2000/48000....... Average Loss for Epoch: 0.32766134448349477\n",
      "Epoch 17......Step: 1000/48000....... Average Loss for Epoch: 0.9244625816047192\n",
      "Epoch 17......Step: 2000/48000....... Average Loss for Epoch: 0.32542230424284935\n",
      "Epoch 18......Step: 1000/48000....... Average Loss for Epoch: 0.9182368298768997\n",
      "Epoch 18......Step: 2000/48000....... Average Loss for Epoch: 0.3232513963282108\n",
      "Epoch 19......Step: 1000/48000....... Average Loss for Epoch: 0.9121699329912663\n",
      "Epoch 19......Step: 2000/48000....... Average Loss for Epoch: 0.32113974380493165\n"
     ]
    }
   ],
   "source": [
    "def train_gru(line_tensor, category):\n",
    "    gru_model.zero_grad()\n",
    "    line_tensor = line_tensor.reshape(-1, 20, 300)\n",
    "    output = gru_model(line_tensor)        \n",
    "    \n",
    "    loss = criterion(output, category)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(gru_model.parameters(), 5)\n",
    "    optimizer.step()\n",
    "\n",
    "    for p in gru_model.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-0.005)\n",
    "    \n",
    "    return output, loss.item()\n",
    "# Keep track of losses for plotting\n",
    "train_loader_gru = torch.utils.data.DataLoader(train_rnn, batch_size=20,shuffle=False)\n",
    "test_loader_gru = torch.utils.data.DataLoader(test_rnn, batch_size=20, shuffle=False)\n",
    "\n",
    "current_loss = 0\n",
    "for epoch in range(20):\n",
    "    print_counter = 0\n",
    "    for i, (review, rating) in enumerate(train_loader_gru):\n",
    "        print_counter += 1\n",
    "        output, loss = train_gru(review, rating)        \n",
    "        current_loss += loss\n",
    "        if print_counter%1000 == 0:\n",
    "            print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, print_counter, len(train_loader_rnn), current_loss/print_counter))\n",
    "            current_loss = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c5c67d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gated RNN Accuracy =  65.88333333333334\n"
     ]
    }
   ],
   "source": [
    "y_pred_gru = []\n",
    "y_test_gru = []\n",
    "gru_model.eval()\n",
    "for i, (review, rating) in enumerate(test_loader_rnn):\n",
    "    line_tensor = review.reshape(-1, 20, 300)\n",
    "    output = gru_model(line_tensor) \n",
    "    output = torch.argmax(output).item()\n",
    "    y_test_gru.append(rating.item())\n",
    "    y_pred_gru.append(output)\n",
    "\n",
    "print(\"Gated RNN Accuracy = \", findAccuracy(y_test_gru, y_pred_gru))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2d2f2",
   "metadata": {},
   "source": [
    "### (c) LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "965ae8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0......Step: 1000/48000....... Average Loss for Epoch: 1.0319135826826096\n",
      "Epoch 0......Step: 2000/48000....... Average Loss for Epoch: 0.4492488602101803\n",
      "Epoch 1......Step: 1000/48000....... Average Loss for Epoch: 1.1977822415828705\n",
      "Epoch 1......Step: 2000/48000....... Average Loss for Epoch: 0.4116702942252159\n",
      "Epoch 2......Step: 1000/48000....... Average Loss for Epoch: 1.1262562407255172\n",
      "Epoch 2......Step: 2000/48000....... Average Loss for Epoch: 0.3949509975016117\n",
      "Epoch 3......Step: 1000/48000....... Average Loss for Epoch: 1.0924741402566434\n",
      "Epoch 3......Step: 2000/48000....... Average Loss for Epoch: 0.38503675700724127\n",
      "Epoch 4......Step: 1000/48000....... Average Loss for Epoch: 1.069533033490181\n",
      "Epoch 4......Step: 2000/48000....... Average Loss for Epoch: 0.3772322600930929\n",
      "Epoch 5......Step: 1000/48000....... Average Loss for Epoch: 1.0511162491440773\n",
      "Epoch 5......Step: 2000/48000....... Average Loss for Epoch: 0.37053066992759703\n",
      "Epoch 6......Step: 1000/48000....... Average Loss for Epoch: 1.0352882313728333\n",
      "Epoch 6......Step: 2000/48000....... Average Loss for Epoch: 0.36462545043230055\n",
      "Epoch 7......Step: 1000/48000....... Average Loss for Epoch: 1.0212589570879935\n",
      "Epoch 7......Step: 2000/48000....... Average Loss for Epoch: 0.35933102083206175\n",
      "Epoch 8......Step: 1000/48000....... Average Loss for Epoch: 1.008683785110712\n",
      "Epoch 8......Step: 2000/48000....... Average Loss for Epoch: 0.35458521461486814\n",
      "Epoch 9......Step: 1000/48000....... Average Loss for Epoch: 0.9972255088686943\n",
      "Epoch 9......Step: 2000/48000....... Average Loss for Epoch: 0.35026766896247863\n",
      "Epoch 10......Step: 1000/48000....... Average Loss for Epoch: 0.9866280865967274\n",
      "Epoch 10......Step: 2000/48000....... Average Loss for Epoch: 0.34629952315986157\n",
      "Epoch 11......Step: 1000/48000....... Average Loss for Epoch: 0.9766885764598846\n",
      "Epoch 11......Step: 2000/48000....... Average Loss for Epoch: 0.342590204641223\n",
      "Epoch 12......Step: 1000/48000....... Average Loss for Epoch: 0.967251287996769\n",
      "Epoch 12......Step: 2000/48000....... Average Loss for Epoch: 0.3390627297013998\n",
      "Epoch 13......Step: 1000/48000....... Average Loss for Epoch: 0.9582253035008907\n",
      "Epoch 13......Step: 2000/48000....... Average Loss for Epoch: 0.3356765458583832\n",
      "Epoch 14......Step: 1000/48000....... Average Loss for Epoch: 0.9495910190045833\n",
      "Epoch 14......Step: 2000/48000....... Average Loss for Epoch: 0.3324545042812824\n",
      "Epoch 15......Step: 1000/48000....... Average Loss for Epoch: 0.9412614594995975\n",
      "Epoch 15......Step: 2000/48000....... Average Loss for Epoch: 0.3294470566213131\n",
      "Epoch 16......Step: 1000/48000....... Average Loss for Epoch: 0.9332913633883\n",
      "Epoch 16......Step: 2000/48000....... Average Loss for Epoch: 0.32649919982254505\n",
      "Epoch 17......Step: 1000/48000....... Average Loss for Epoch: 0.9254759859740734\n",
      "Epoch 17......Step: 2000/48000....... Average Loss for Epoch: 0.32369021824002264\n",
      "Epoch 18......Step: 1000/48000....... Average Loss for Epoch: 0.9176299183964729\n",
      "Epoch 18......Step: 2000/48000....... Average Loss for Epoch: 0.3211787001267076\n",
      "Epoch 19......Step: 1000/48000....... Average Loss for Epoch: 0.9102008429765701\n",
      "Epoch 19......Step: 2000/48000....... Average Loss for Epoch: 0.31848941153287885\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layer_dim=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "\n",
    "lstm_model = LSTMModel(300, 20, 3)\n",
    "optimizer_lstm = torch.optim.Adam(lstm_model.parameters(), lr=0.0005, betas=(0.9,0.999),eps=1e-08,weight_decay=5e-5)  \n",
    "\n",
    "\n",
    "def train_lstm(line_tensor, category):\n",
    "    lstm_model.zero_grad()\n",
    "    line_tensor = line_tensor.reshape(-1, 20, 300)\n",
    "    output = lstm_model(line_tensor)        \n",
    "    \n",
    "    loss = criterion(output, category)\n",
    "    optimizer_lstm.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), 5)\n",
    "    optimizer_lstm.step()\n",
    "\n",
    "    for p in lstm_model.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-0.005)\n",
    "    \n",
    "    return output, loss.item()\n",
    "\n",
    "current_loss = 0\n",
    "for epoch in range(20):\n",
    "    print_counter = 0\n",
    "    for i, (review, rating) in enumerate(train_loader_gru):\n",
    "        print_counter += 1\n",
    "        output, loss = train_lstm(review, rating)       \n",
    "        current_loss += loss\n",
    "        if print_counter%1000 == 0:\n",
    "            print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, print_counter, len(train_loader_rnn), current_loss/print_counter))\n",
    "            current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45ad8d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Accuracy in % =  66.03333333333333\n"
     ]
    }
   ],
   "source": [
    "y_pred_lstm = []\n",
    "y_test_lstm = []\n",
    "lstm_model.eval()\n",
    "for i, (review, rating) in enumerate(test_loader_rnn):\n",
    "    line_tensor = review.reshape(-1, 20, 300)\n",
    "    output = lstm_model(line_tensor) \n",
    "    output = torch.argmax(output).item()\n",
    "    y_test_lstm.append(rating.item())\n",
    "    y_pred_lstm.append(output)\n",
    "\n",
    "print(\"LSTM Accuracy in % = \", findAccuracy(y_test_lstm, y_pred_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef84b5",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "**Test Accuracy values**\n",
    "\n",
    "Simple RNN = 61.02% (varied between 59 - 61)\n",
    "\n",
    "Gated RNN = 65.88% (varied between 65 - 67%)\n",
    "\n",
    "LSTM = 66.03% (varied between 66-67%)\n",
    "\n",
    "Gated RNN models are designed to address the vanishing gradient problem that can occur in simple RNN models, where the gradient signal becomes too small to propagate through the network during backpropagation. This can lead to difficulty in capturing long-term dependencies in the data. The vanishing gradient problem can occur when the recurrent weights in an RNN are repeatedly multiplied by small values causing the gradient signal to shrink exponentially over time.\n",
    "\n",
    "Gated RNN models, on the other hand, use gating mechanisms to selectively update the hidden state and control the flow of information through the network. The gating mechanisms allow the model to remember information over longer periods of time and avoid the vanishing gradient problem.\n",
    "\n",
    "Therefore, Gated RNN models outperform the simple RNN model, because they are better at capturing long-term dependencies in the data. \n",
    "\n",
    "LSTM has more parameters compared to a simple RNN model, which means that it has more capacity to learn complex patterns in the data. This can also contribute to the improved performance of LSTM over the simple RNN model.\n",
    "\n",
    "Also, RNN has better performance overall when compared to FFN because FFN considered the average of review vectors which might suffer probelm of outliers ( too high or too low values). In contrast, RNN considered each word seperatly allowing them to better capture patterns in sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0ead4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20c9a42e",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "* https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "* https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook\n",
    "* https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/\n",
    "* https://blog.floydhub.com/gru-with-pytorch/\n",
    "* https://pythonguides.com/adam-optimizer-pytorch/\n",
    "* https://towardsdatascience.com/building-rnn-lstm-and-gru-for-time-series-using-pytorch-a46e5b094e7b\n",
    "* https://stackoverflow.com/questions/70006954/pytorch-rnn-loss-does-not-decrease-and-validate-accuracy-remains-unchanged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
